{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U28pRChlTIhp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bveYbeBQTJmz"
   },
   "source": [
    "## **Start running RolLbl2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXy5Uq85HaYs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "\n",
    "###Start sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit,cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "### End sklearn\n",
    "\n",
    "###***Start tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.random.set_seed(1234)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "###**** End tensorflow.keras\n",
    "#sys.path.append(\"..\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "pathAug = \"/content/drive/MyDrive/MasterThesis_Files/mainCodes/augmentation/\"\n",
    "pathData =\"/content/drive/MyDrive/MasterThesis_Files/mainCodes/data/forCrossValidation/Shifted5_Rol_Lbl2/\"\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0,pathAug)\n",
    "sys.path.insert(1,pathData)\n",
    "\n",
    "import preprocessRollingLabel2_NN as aug\n",
    "\n",
    "#####End Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5KxzUHw03_x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j29MxY7LgANj"
   },
   "outputs": [],
   "source": [
    "\n",
    "############ Start Running codes\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"Main Start running time : {datestr}\")\n",
    "\n",
    "#dfActual = pd.read_csv(pathData+\"Shifted5_NoLess5_AfterRol_Lbl2_5To1_210124_163135.csv\",header=None)\n",
    "dfActual = pd.read_csv(pathData+\"Shifted5_NoLess5_AfterRol_Lbl2_210124_155122.csv\",header=None)\n",
    "\n",
    "\n",
    "accPerFold = []\n",
    "lossPerFold = []\n",
    "dfPrReF1=pd.DataFrame()\n",
    "\n",
    "#dfActual=dfActual[:5000]\n",
    "\n",
    "yX=dfActual.values\n",
    "X = yX[:, 1:]  # converts the df to a numpy array\n",
    "y = yX[:, 0]\n",
    "\n",
    "print(f\"\\n Number of Actual labeled 0: {len(y[np.where(y==0)])}\")\n",
    "print(f\"Number of Actual labeled 1: {len(y[np.where(y==1)])}\")\n",
    "print(f\"Number of Actual labeled 2: {len(y[np.where(y==2)])} \\n\")\n",
    "\n",
    "\n",
    "dataSplitPCT=.3\n",
    "dataSplitValTestPCT=.5\n",
    "\n",
    "train_test_split_Shuffle=True\n",
    "flagFitShuffle = True\n",
    "flagSeed=True\n",
    "\n",
    "p1=\"\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)#5\n",
    "model=0\n",
    "\n",
    "for foldNum, (trainIndex, testIndex) in enumerate(skf.split(X,y),start=1):\n",
    "\n",
    "    #print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n",
    "\n",
    "    yXtrain, yXtest = yX[trainIndex], yX[testIndex]\n",
    "    #ytrain, ytest = y[trainIndex], y[testIndex]\n",
    "\n",
    "    AugedNN=aug.GenerateAug_NN_Rolling(yXtrain,foldNum,flagLbl2=False,jitterNum4Lbl1=3,jitterNum4Lbl2=29)###***Generate synthetic data 38\n",
    "\n",
    "    datestrfoldNum = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"\\n Start running time Fold={foldNum}: {datestrfoldNum} ,-------------------------- \\n\")\n",
    "\n",
    "    Actual_AugedNN=np.concatenate((yXtrain,AugedNN),axis=0)\n",
    "\n",
    "    yXtrain1, yXtrain2 = train_test_split(Actual_AugedNN, shuffle=train_test_split_Shuffle,\n",
    "                                                          test_size=dataSplitPCT, random_state=42,\n",
    "                                                          stratify=Actual_AugedNN[:,0])  # stratify=input_y\n",
    "\n",
    "    yXtrain = np.concatenate((yXtrain1, yXtrain2), axis=0)\n",
    "\n",
    "    yXvalid, yXtest = train_test_split(yXtest, shuffle=train_test_split_Shuffle,\n",
    "                                          test_size=dataSplitValTestPCT, random_state=42,\n",
    "                                          stratify=yXtest[:, 0])  # stratify=input_y\n",
    "\n",
    "    print(f\"\\n Number of Final yXtrain_Fold={foldNum} labeled 0: {len(yXtrain[np.where(yXtrain[:, 0] == 0)])}\")\n",
    "    print(f\"Number of Final yXtrain_Fold={foldNum} labeled 1: {len(yXtrain[np.where(yXtrain[:, 0] == 1)])}\")\n",
    "    print(f\"Number of Final yXtrain_Fold={foldNum} labeled 2: {len(yXtrain[np.where(yXtrain[:, 0] == 2)])} \\n\")\n",
    "\n",
    "    ytrain = to_categorical(yXtrain  [:,0] )\n",
    "    yvalid = to_categorical(yXvalid  [:,0] )\n",
    "    ytest  = to_categorical(yXtest   [:,0] )\n",
    "\n",
    "    xtrain = yXtrain  [:,1:]\n",
    "    xvalid = yXvalid  [:,1:]\n",
    "    xtest  = yXtest   [:,1:]\n",
    "\n",
    "\n",
    "    neurons = xtrain.shape[1]\n",
    "\n",
    "    epochs = 100#100#0#30#30# 150  # 0  # 100#300#60#300#10#200#00#150\n",
    "    batch = 32\n",
    "    lr = 0.0001\n",
    "\n",
    "    #flagR1 = True\n",
    "    flagR1=False\n",
    "    r1 = .1\n",
    "    r2 = .3\n",
    "    d1 = .3\n",
    "\n",
    "    if foldNum==1:\n",
    "        print(\"\\n Hyperparameters:\")\n",
    "        print(f\"epochs: {epochs}, batch: {batch}, lr: {lr}, neurons: {neurons}, flagFitShuffle: {flagFitShuffle} , train_test_split_Shuffle: {train_test_split_Shuffle}, flagSeed: {flagSeed}\\n \")\n",
    "\n",
    "        print(f\"\\n xtrain: {np.shape(xtrain)}, ytrain: {np.shape(ytrain)}\")\n",
    "        print(f\"xvalid: {np.shape(xvalid)}, yvalid: {np.shape(yvalid)}\")\n",
    "        print(f\"xtest: {np.shape(xtest)}, ytest: {np.shape(ytest)} \\n\")\n",
    "\n",
    "        #p1 = os.path.join(str(pathCurrent.parent.parent), \"Results\", \"Results_001_class_oppys\", \"bestModels\", \"\")\n",
    "        # pathSavingPlotsPerRunning = pathSavingPlots + datestr #+ \"_\" + modelname\n",
    "        # if not os.path.exists(pathSavingPlotsPerRunning):\n",
    "        #     os.makedirs(pathSavingPlotsPerRunning)\n",
    "\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(177, activation='tanh', input_dim=xtrain.shape[1]\n",
    "                    #,kernel_regularizer=l1_l2(l1=r1, l2=r2)\n",
    "                    #,kernel_regularizer = l1(r1) if flagR1 else l2(r2),\n",
    "                    ))  # , input_dim=xtrain.shape[1]\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(150, activation='tanh'\n",
    "                    #,kernel_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "                    #,kernel_regularizer=l1_l2(l1=r1, l2=r2)\n",
    "                    # bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "                    # activity_regularizer=l1(r2)\n",
    "                    ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(90, activation='tanh'\n",
    "                    #,kernel_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "                    #,kernel_regularizer=l1_l2(l1=r1, l2=r2)\n",
    "                    # bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer = l1(r1) if flagR1 else l2(r2)\n",
    "                    # activity_regularizer=l2(r2)\n",
    "                    ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    # model.add(Dense(295, activation='tanh'\n",
    "    #                 #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "    # #                 # bias_regularizer=l1(r2),\n",
    "    # #                 activity_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "    # #                 # activity_regularizer=l2(r2)\n",
    "    #                 ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr)#lr\n",
    "    # cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",save_best_only=True,verbose=0)\n",
    "    # model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "    if foldNum == 1:\n",
    "        print(\"\\n Final test model.summary(): \\n\")\n",
    "        print(model.summary())\n",
    "\n",
    "        print(f\"\\n model.get_config: {str(model.get_config())}\")\n",
    "\n",
    "    # fit model\n",
    "    history1 = model.fit(xtrain, ytrain, batch_size=batch, epochs=epochs\n",
    "                         , validation_data=(xvalid, yvalid)\n",
    "                         , verbose=1, use_multiprocessing=True,\n",
    "                         shuffle=flagFitShuffle).history  # ,shuffle=True#,callbacks=[es]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history1['loss'], linewidth=2, label='Train')  # OR accuracy\n",
    "    plt.plot(history1['val_loss'], linewidth=2, label='Validation')  # OR val_accuracy\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.13, 1.13))\n",
    "    plt.title(f'Model loss Fold={foldNum}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0,.5)\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"loss&val_loss_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    yPred = model.predict(xtest, verbose=1)\n",
    "\n",
    "    print(f\"confusion_matrix_Fold={foldNum}:\\n {confusion_matrix(ytest.argmax(axis=1), yPred.argmax(axis=1))} \\n\")\n",
    "\n",
    "    # Creating multilabel confusion matrix\n",
    "    mlbConfusion = multilabel_confusion_matrix(ytest.argmax(axis=1), yPred.argmax(axis=1))\n",
    "    print(f\"multilabel_confusion_matrix_Fold={foldNum}:\\n {mlbConfusion} \\n\")\n",
    "\n",
    "    print(f\"accuracy_score_Fold={foldNum}:\\n {accuracy_score(ytest.argmax(axis=1), yPred.argmax(axis=1), normalize=False)} \\n\")\n",
    "\n",
    "    print(f\"classification_report_Fold={foldNum}:\\n {classification_report(ytest.argmax(axis=1), yPred.argmax(axis=1))} \\n\")\n",
    "\n",
    "    cr=pd.DataFrame(classification_report(ytest.argmax(axis=1), yPred.argmax(axis=1),output_dict=True))\n",
    "    dfPrReF1=dfPrReF1.append(cr.iloc[:3,:3])\n",
    "\n",
    "    # Predicting test images\n",
    "    # preds = np.where(yPred < 0.5, 0, 1)\n",
    "\n",
    "    mlbClasses = [0, 1, 2]\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for j, (label, matrix) in enumerate(zip(mlbClasses, mlbConfusion)):\n",
    "        plt.subplot(f'23{j + 1}')\n",
    "        labels = [f'Not_{label}', label]\n",
    "        sns.heatmap(matrix, annot=True, square=True, fmt='d', cbar=False, cmap='Blues',\n",
    "                    cbar_kws={'label': 'My Colorbar'},  # , fmt = 'd'\n",
    "                    xticklabels=labels, yticklabels=labels, linecolor='black', linewidth=1)\n",
    "\n",
    "        plt.ylabel('Actual class')\n",
    "        plt.xlabel(f'Predicted class_Fold={foldNum}')\n",
    "        plt.title(labels[0])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"ConfusionMatrix_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"End running time Fold={foldNum}: {datestr} ,-------------------------- \\n\")\n",
    "\n",
    "dfPrReF1=pd.DataFrame([np.round(dfPrReF1[dfPrReF1.index=='precision'].mean(),2),np.round(dfPrReF1[dfPrReF1.index=='recall'].mean(),2)\n",
    ",np.round(dfPrReF1[dfPrReF1.index=='f1-score'].mean(),2)],index=['precision','recall','f1-score'])\n",
    "\n",
    "print(f\"\\nclassification_report_AllFolds:\\n {dfPrReF1}\")\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"End running time: {datestr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuXYtlqxpsNe"
   },
   "source": [
    "# End running  RolLbl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9zEE1AkST-3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu1S5oeop9-Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiOORdHtqT-x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKUFvQagSUGD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVVZVyLVj_Zc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ9Rj0YFjfWx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX9ZZl-73Ev5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP6J0IarSgdJ"
   },
   "source": [
    "## **forAugShifted5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173919
    },
    "id": "Nz1jPyPPm6Ir",
    "outputId": "83f22a13-a2d5-45ca-f164-178e30d68946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Start running time : 210214_085547\n",
      "\n",
      " Number of Actual labeled 0: 87245\n",
      "Number of Actual labeled 1: 2870\n",
      "Number of Actual labeled 2: 0 \n",
      "\n",
      "Start running time Data Augmentation_Fold=1: 210214_085548 ,-------------------------- \n",
      "\n",
      "\n",
      " Data Augmentation Hyperparameters:\n",
      "epochs: 2000, batch: 32, lr: 0.0001, neurons1: 177, neurons2: 150, flagFitShuffle: True \n",
      " \n",
      "\n",
      " Hyperparameters:\n",
      "\n",
      " Data Augmentation model.summary(): \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 177)               10620     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               26700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                13590     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 59)                5369      \n",
      "=================================================================\n",
      "Total params: 56,279\n",
      "Trainable params: 56,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Data Augmentation MSE Label1, iter2==> mean: 1.2333758419753588e-05, min: 3.510143964658872e-06, max: 0.00023430869206242266\n",
      "End running time Data Augmentation_Fold=1: 210214_091523 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=1: 210214_091523 ,-------------------------- \n",
      "\n",
      "\n",
      " Number of Final yXtrain_Fold=1 labeled 0: 69796\n",
      "Number of Final yXtrain_Fold=1 labeled 1: 16072\n",
      "Number of Final yXtrain_Fold=1 labeled 2: 0 \n",
      "\n",
      "\n",
      " Hyperparameters:\n",
      "epochs: 1000, batch: 512, lr: 0.0001, neurons: 59, flagFitShuffle: True , train_test_split_Shuffle: True, flagSeed: True\n",
      " \n",
      "\n",
      " xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,) \n",
      "\n",
      "\n",
      " Final test model.summary(): \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 177)               10620     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               26700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                13590     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 91        \n",
      "=================================================================\n",
      "Total params: 51,001\n",
      "Trainable params: 51,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " model.get_config: {'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 59), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_input'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 59), 'dtype': 'float32', 'units': 177, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 150, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 90, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.5575 - accuracy: 0.7114 - val_loss: 0.2311 - val_accuracy: 0.9673\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8187 - val_loss: 0.2391 - val_accuracy: 0.9599\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8213 - val_loss: 0.2465 - val_accuracy: 0.9557\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.8226 - val_loss: 0.2302 - val_accuracy: 0.9543\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8256 - val_loss: 0.2419 - val_accuracy: 0.9538\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8245 - val_loss: 0.2318 - val_accuracy: 0.9534\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4157 - accuracy: 0.8246 - val_loss: 0.2138 - val_accuracy: 0.9562\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8281 - val_loss: 0.2294 - val_accuracy: 0.9521\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8296 - val_loss: 0.2356 - val_accuracy: 0.9482\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8291 - val_loss: 0.2411 - val_accuracy: 0.9493\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8305 - val_loss: 0.2188 - val_accuracy: 0.9511\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8286 - val_loss: 0.2249 - val_accuracy: 0.9484\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8317 - val_loss: 0.2213 - val_accuracy: 0.9464\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8333 - val_loss: 0.2308 - val_accuracy: 0.9433\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8340 - val_loss: 0.2112 - val_accuracy: 0.9462\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8355 - val_loss: 0.2179 - val_accuracy: 0.9444\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8369 - val_loss: 0.1953 - val_accuracy: 0.9494\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8374 - val_loss: 0.2074 - val_accuracy: 0.9451\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8364 - val_loss: 0.1879 - val_accuracy: 0.9484\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8395 - val_loss: 0.2280 - val_accuracy: 0.9370\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8409 - val_loss: 0.1907 - val_accuracy: 0.9465\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8399 - val_loss: 0.1870 - val_accuracy: 0.9481\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8415 - val_loss: 0.2032 - val_accuracy: 0.9414\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8411 - val_loss: 0.2073 - val_accuracy: 0.9349\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8429 - val_loss: 0.2087 - val_accuracy: 0.9345\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8438 - val_loss: 0.1842 - val_accuracy: 0.9430\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8458 - val_loss: 0.2218 - val_accuracy: 0.9289\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8483 - val_loss: 0.1995 - val_accuracy: 0.9390\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8505 - val_loss: 0.1769 - val_accuracy: 0.9482\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8505 - val_loss: 0.2054 - val_accuracy: 0.9332\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3131 - accuracy: 0.8515 - val_loss: 0.1840 - val_accuracy: 0.9404\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8548 - val_loss: 0.1810 - val_accuracy: 0.9397\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3028 - accuracy: 0.8569 - val_loss: 0.2116 - val_accuracy: 0.9252\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8576 - val_loss: 0.2015 - val_accuracy: 0.9314\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8613 - val_loss: 0.1716 - val_accuracy: 0.9443\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8637 - val_loss: 0.1785 - val_accuracy: 0.9402\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8644 - val_loss: 0.1606 - val_accuracy: 0.9502\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8670 - val_loss: 0.1877 - val_accuracy: 0.9334\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8694 - val_loss: 0.1900 - val_accuracy: 0.9312\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8714 - val_loss: 0.1885 - val_accuracy: 0.9298\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8742 - val_loss: 0.1649 - val_accuracy: 0.9424\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8760 - val_loss: 0.1939 - val_accuracy: 0.9293\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8791 - val_loss: 0.1578 - val_accuracy: 0.9440\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8804 - val_loss: 0.1952 - val_accuracy: 0.9229\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8826 - val_loss: 0.1688 - val_accuracy: 0.9356\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8835 - val_loss: 0.1677 - val_accuracy: 0.9372\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.8846 - val_loss: 0.1514 - val_accuracy: 0.9457\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2507 - accuracy: 0.8858 - val_loss: 0.1780 - val_accuracy: 0.9311\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.8883 - val_loss: 0.1522 - val_accuracy: 0.9452\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.8921 - val_loss: 0.1473 - val_accuracy: 0.9483\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8915 - val_loss: 0.1789 - val_accuracy: 0.9274\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.8931 - val_loss: 0.1622 - val_accuracy: 0.9372\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.8942 - val_loss: 0.1506 - val_accuracy: 0.9438\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8961 - val_loss: 0.1685 - val_accuracy: 0.9347\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8965 - val_loss: 0.1903 - val_accuracy: 0.9200\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.8988 - val_loss: 0.1453 - val_accuracy: 0.9444\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9002 - val_loss: 0.1348 - val_accuracy: 0.9492\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2206 - accuracy: 0.9015 - val_loss: 0.1301 - val_accuracy: 0.9514\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9027 - val_loss: 0.1316 - val_accuracy: 0.9514\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9061 - val_loss: 0.1517 - val_accuracy: 0.9393\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9065 - val_loss: 0.1601 - val_accuracy: 0.9349\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9080 - val_loss: 0.1603 - val_accuracy: 0.9333\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9083 - val_loss: 0.1341 - val_accuracy: 0.9495\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9081 - val_loss: 0.1450 - val_accuracy: 0.9454\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9123 - val_loss: 0.1345 - val_accuracy: 0.9486\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2006 - accuracy: 0.9114 - val_loss: 0.1378 - val_accuracy: 0.9452\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9123 - val_loss: 0.1196 - val_accuracy: 0.9536\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9153 - val_loss: 0.1479 - val_accuracy: 0.9403\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9148 - val_loss: 0.1508 - val_accuracy: 0.9359\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9165 - val_loss: 0.1459 - val_accuracy: 0.9424\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9216 - val_loss: 0.1225 - val_accuracy: 0.9522\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.9195 - val_loss: 0.1405 - val_accuracy: 0.9438\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9197 - val_loss: 0.1324 - val_accuracy: 0.9482\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9218 - val_loss: 0.1236 - val_accuracy: 0.9516\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.9243 - val_loss: 0.1177 - val_accuracy: 0.9543\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9258 - val_loss: 0.1254 - val_accuracy: 0.9504\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9270 - val_loss: 0.1166 - val_accuracy: 0.9524\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9259 - val_loss: 0.1257 - val_accuracy: 0.9498\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9280 - val_loss: 0.1032 - val_accuracy: 0.9592\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9274 - val_loss: 0.1050 - val_accuracy: 0.9579\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9299 - val_loss: 0.1018 - val_accuracy: 0.9593\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9302 - val_loss: 0.1205 - val_accuracy: 0.9515\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9318 - val_loss: 0.1119 - val_accuracy: 0.9548\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9305 - val_loss: 0.0998 - val_accuracy: 0.9606\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9329 - val_loss: 0.0930 - val_accuracy: 0.9618\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9324 - val_loss: 0.1085 - val_accuracy: 0.9559\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9345 - val_loss: 0.1137 - val_accuracy: 0.9511\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9364 - val_loss: 0.0985 - val_accuracy: 0.9602\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9357 - val_loss: 0.1332 - val_accuracy: 0.9448\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9362 - val_loss: 0.0906 - val_accuracy: 0.9634\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9352 - val_loss: 0.1019 - val_accuracy: 0.9588\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9376 - val_loss: 0.1221 - val_accuracy: 0.9488\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1509 - accuracy: 0.9386 - val_loss: 0.1077 - val_accuracy: 0.9562\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9383 - val_loss: 0.1064 - val_accuracy: 0.9566\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9391 - val_loss: 0.1254 - val_accuracy: 0.9471\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9398 - val_loss: 0.1029 - val_accuracy: 0.9568\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9411 - val_loss: 0.0902 - val_accuracy: 0.9627\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9413 - val_loss: 0.0994 - val_accuracy: 0.9581\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9403 - val_loss: 0.0988 - val_accuracy: 0.9591\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1441 - accuracy: 0.9424 - val_loss: 0.0969 - val_accuracy: 0.9595\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9437 - val_loss: 0.1043 - val_accuracy: 0.9557\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9426 - val_loss: 0.0922 - val_accuracy: 0.9617\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9444 - val_loss: 0.1046 - val_accuracy: 0.9538\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9428 - val_loss: 0.1153 - val_accuracy: 0.9515\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1405 - accuracy: 0.9445 - val_loss: 0.1134 - val_accuracy: 0.9516\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9448 - val_loss: 0.1113 - val_accuracy: 0.9525\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9467 - val_loss: 0.0968 - val_accuracy: 0.9588\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1346 - accuracy: 0.9473 - val_loss: 0.1112 - val_accuracy: 0.9528\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1323 - accuracy: 0.9487 - val_loss: 0.0809 - val_accuracy: 0.9659\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.9472 - val_loss: 0.1122 - val_accuracy: 0.9509\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9474 - val_loss: 0.0735 - val_accuracy: 0.9704\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9472 - val_loss: 0.1010 - val_accuracy: 0.9571\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1332 - accuracy: 0.9482 - val_loss: 0.1029 - val_accuracy: 0.9562\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9487 - val_loss: 0.0901 - val_accuracy: 0.9612\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.9479 - val_loss: 0.0936 - val_accuracy: 0.9594\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9489 - val_loss: 0.1131 - val_accuracy: 0.9513\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1308 - accuracy: 0.9496 - val_loss: 0.0994 - val_accuracy: 0.9582\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9506 - val_loss: 0.0927 - val_accuracy: 0.9594\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1268 - accuracy: 0.9506 - val_loss: 0.0927 - val_accuracy: 0.9607\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9501 - val_loss: 0.1168 - val_accuracy: 0.9483\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1280 - accuracy: 0.9508 - val_loss: 0.1099 - val_accuracy: 0.9524\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9510 - val_loss: 0.0847 - val_accuracy: 0.9642\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9507 - val_loss: 0.1021 - val_accuracy: 0.9556\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9519 - val_loss: 0.1204 - val_accuracy: 0.9474\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9497 - val_loss: 0.1012 - val_accuracy: 0.9564\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9517 - val_loss: 0.0920 - val_accuracy: 0.9608\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9520 - val_loss: 0.0974 - val_accuracy: 0.9581\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9528 - val_loss: 0.0865 - val_accuracy: 0.9627\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9520 - val_loss: 0.0808 - val_accuracy: 0.9654\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9549 - val_loss: 0.0895 - val_accuracy: 0.9610\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9527 - val_loss: 0.0879 - val_accuracy: 0.9610\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.9549 - val_loss: 0.0804 - val_accuracy: 0.9649\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9558 - val_loss: 0.1021 - val_accuracy: 0.9571\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9552 - val_loss: 0.0907 - val_accuracy: 0.9608\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9560 - val_loss: 0.0888 - val_accuracy: 0.9607\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9550 - val_loss: 0.0816 - val_accuracy: 0.9632\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9554 - val_loss: 0.0929 - val_accuracy: 0.9594\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9555 - val_loss: 0.0880 - val_accuracy: 0.9625\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9557 - val_loss: 0.1043 - val_accuracy: 0.9556\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9567 - val_loss: 0.0941 - val_accuracy: 0.9587\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1169 - accuracy: 0.9556 - val_loss: 0.0943 - val_accuracy: 0.9598\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9593 - val_loss: 0.0766 - val_accuracy: 0.9678\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9559 - val_loss: 0.1012 - val_accuracy: 0.9559\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9565 - val_loss: 0.0793 - val_accuracy: 0.9663\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9576 - val_loss: 0.0906 - val_accuracy: 0.9608\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9587 - val_loss: 0.0969 - val_accuracy: 0.9585\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9565 - val_loss: 0.0838 - val_accuracy: 0.9648\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9572 - val_loss: 0.0886 - val_accuracy: 0.9609\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1117 - accuracy: 0.9584 - val_loss: 0.0748 - val_accuracy: 0.9679\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1103 - accuracy: 0.9591 - val_loss: 0.0883 - val_accuracy: 0.9617\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9597 - val_loss: 0.0933 - val_accuracy: 0.9591\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.9587 - val_loss: 0.0971 - val_accuracy: 0.9579\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9590 - val_loss: 0.0689 - val_accuracy: 0.9691\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1089 - accuracy: 0.9590 - val_loss: 0.1062 - val_accuracy: 0.9559\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9586 - val_loss: 0.1122 - val_accuracy: 0.9508\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.9605 - val_loss: 0.0846 - val_accuracy: 0.9649\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.9593 - val_loss: 0.1023 - val_accuracy: 0.9585\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9564 - val_loss: 0.0747 - val_accuracy: 0.9665\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9599 - val_loss: 0.0718 - val_accuracy: 0.9686\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9597 - val_loss: 0.0861 - val_accuracy: 0.9619\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9594 - val_loss: 0.0840 - val_accuracy: 0.9642\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9612 - val_loss: 0.0866 - val_accuracy: 0.9630\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9615 - val_loss: 0.0810 - val_accuracy: 0.9639\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1076 - accuracy: 0.9593 - val_loss: 0.0754 - val_accuracy: 0.9666\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9614 - val_loss: 0.0871 - val_accuracy: 0.9619\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.9628 - val_loss: 0.0655 - val_accuracy: 0.9710\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9632 - val_loss: 0.0917 - val_accuracy: 0.9613\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9620 - val_loss: 0.0863 - val_accuracy: 0.9643\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9622 - val_loss: 0.0853 - val_accuracy: 0.9633\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.9618 - val_loss: 0.0900 - val_accuracy: 0.9615\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9630 - val_loss: 0.0627 - val_accuracy: 0.9725\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.9611 - val_loss: 0.0841 - val_accuracy: 0.9634\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9615 - val_loss: 0.0812 - val_accuracy: 0.9653\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.9629 - val_loss: 0.0793 - val_accuracy: 0.9659\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.9628 - val_loss: 0.0878 - val_accuracy: 0.9632\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.9613 - val_loss: 0.0886 - val_accuracy: 0.9617\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9639 - val_loss: 0.0844 - val_accuracy: 0.9642\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9634 - val_loss: 0.0912 - val_accuracy: 0.9605\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9631 - val_loss: 0.0846 - val_accuracy: 0.9635\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9631 - val_loss: 0.0703 - val_accuracy: 0.9701\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9631 - val_loss: 0.0819 - val_accuracy: 0.9639\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9633 - val_loss: 0.0802 - val_accuracy: 0.9664\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9633 - val_loss: 0.0774 - val_accuracy: 0.9675\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9638 - val_loss: 0.0893 - val_accuracy: 0.9635\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9633 - val_loss: 0.0799 - val_accuracy: 0.9653\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.9635 - val_loss: 0.0697 - val_accuracy: 0.9694\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0984 - accuracy: 0.9639 - val_loss: 0.0767 - val_accuracy: 0.9683\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0968 - accuracy: 0.9639 - val_loss: 0.0744 - val_accuracy: 0.9674\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0949 - accuracy: 0.9647 - val_loss: 0.0776 - val_accuracy: 0.9670\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0968 - accuracy: 0.9644 - val_loss: 0.0806 - val_accuracy: 0.9652\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9648 - val_loss: 0.0823 - val_accuracy: 0.9646\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0951 - accuracy: 0.9652 - val_loss: 0.0788 - val_accuracy: 0.9672\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0943 - accuracy: 0.9651 - val_loss: 0.0831 - val_accuracy: 0.9634\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0954 - accuracy: 0.9641 - val_loss: 0.0749 - val_accuracy: 0.9672\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0947 - accuracy: 0.9654 - val_loss: 0.0792 - val_accuracy: 0.9655\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0970 - accuracy: 0.9633 - val_loss: 0.0688 - val_accuracy: 0.9706\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0927 - accuracy: 0.9659 - val_loss: 0.0707 - val_accuracy: 0.9695\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0924 - accuracy: 0.9669 - val_loss: 0.0789 - val_accuracy: 0.9669\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9640 - val_loss: 0.0727 - val_accuracy: 0.9678\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0949 - accuracy: 0.9642 - val_loss: 0.0693 - val_accuracy: 0.9705\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0931 - accuracy: 0.9665 - val_loss: 0.0813 - val_accuracy: 0.9658\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0930 - accuracy: 0.9659 - val_loss: 0.0803 - val_accuracy: 0.9655\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0935 - accuracy: 0.9645 - val_loss: 0.0705 - val_accuracy: 0.9705\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0916 - accuracy: 0.9660 - val_loss: 0.0698 - val_accuracy: 0.9700\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0924 - accuracy: 0.9655 - val_loss: 0.0715 - val_accuracy: 0.9689\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.9664 - val_loss: 0.0673 - val_accuracy: 0.9704\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9661 - val_loss: 0.0670 - val_accuracy: 0.9699\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9662 - val_loss: 0.0648 - val_accuracy: 0.9727\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9668 - val_loss: 0.0659 - val_accuracy: 0.9701\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9658 - val_loss: 0.0625 - val_accuracy: 0.9721\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9663 - val_loss: 0.0649 - val_accuracy: 0.9720\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9656 - val_loss: 0.0696 - val_accuracy: 0.9713\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.9671 - val_loss: 0.0764 - val_accuracy: 0.9686\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.9663 - val_loss: 0.0814 - val_accuracy: 0.9676\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.9664 - val_loss: 0.0739 - val_accuracy: 0.9685\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9681 - val_loss: 0.0652 - val_accuracy: 0.9714\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9663 - val_loss: 0.0734 - val_accuracy: 0.9689\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9663 - val_loss: 0.0729 - val_accuracy: 0.9686\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9667 - val_loss: 0.0772 - val_accuracy: 0.9679\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9692 - val_loss: 0.0764 - val_accuracy: 0.9678\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9676 - val_loss: 0.0830 - val_accuracy: 0.9653\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9678 - val_loss: 0.0702 - val_accuracy: 0.9699\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.9672 - val_loss: 0.0702 - val_accuracy: 0.9700\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9681 - val_loss: 0.0748 - val_accuracy: 0.9688\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9676 - val_loss: 0.0705 - val_accuracy: 0.9696\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9684 - val_loss: 0.0710 - val_accuracy: 0.9703\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9686 - val_loss: 0.0817 - val_accuracy: 0.9660\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.9685 - val_loss: 0.0638 - val_accuracy: 0.9723\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9676 - val_loss: 0.0679 - val_accuracy: 0.9707\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9686 - val_loss: 0.0711 - val_accuracy: 0.9696\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9692 - val_loss: 0.0640 - val_accuracy: 0.9727\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9694 - val_loss: 0.0691 - val_accuracy: 0.9705\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9689 - val_loss: 0.0631 - val_accuracy: 0.9727\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9696 - val_loss: 0.0686 - val_accuracy: 0.9717\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9682 - val_loss: 0.0624 - val_accuracy: 0.9728\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9681 - val_loss: 0.0617 - val_accuracy: 0.9737\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9704 - val_loss: 0.0939 - val_accuracy: 0.9624\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9701 - val_loss: 0.0862 - val_accuracy: 0.9645\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9694 - val_loss: 0.0720 - val_accuracy: 0.9705\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9696 - val_loss: 0.0738 - val_accuracy: 0.9694\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9702 - val_loss: 0.0588 - val_accuracy: 0.9744\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9696 - val_loss: 0.0872 - val_accuracy: 0.9653\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9697 - val_loss: 0.0745 - val_accuracy: 0.9690\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9708 - val_loss: 0.0793 - val_accuracy: 0.9687\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9702 - val_loss: 0.0681 - val_accuracy: 0.9717\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9702 - val_loss: 0.0569 - val_accuracy: 0.9747\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9704 - val_loss: 0.0693 - val_accuracy: 0.9708\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9702 - val_loss: 0.0716 - val_accuracy: 0.9710\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9711 - val_loss: 0.0686 - val_accuracy: 0.9717\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0815 - accuracy: 0.9703 - val_loss: 0.0735 - val_accuracy: 0.9700\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9701 - val_loss: 0.0599 - val_accuracy: 0.9743\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.0822 - val_accuracy: 0.9669\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9698 - val_loss: 0.0642 - val_accuracy: 0.9727\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9710 - val_loss: 0.0701 - val_accuracy: 0.9711\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9711 - val_loss: 0.0890 - val_accuracy: 0.9644\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9718 - val_loss: 0.0730 - val_accuracy: 0.9691\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9712 - val_loss: 0.0576 - val_accuracy: 0.9741\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9717 - val_loss: 0.0671 - val_accuracy: 0.9724\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9702 - val_loss: 0.0824 - val_accuracy: 0.9662\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9714 - val_loss: 0.0535 - val_accuracy: 0.9754\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9703 - val_loss: 0.0684 - val_accuracy: 0.9713\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9719 - val_loss: 0.0635 - val_accuracy: 0.9737\n",
      "Epoch 263/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9710 - val_loss: 0.0547 - val_accuracy: 0.9758\n",
      "Epoch 264/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.9703 - val_loss: 0.0669 - val_accuracy: 0.9723\n",
      "Epoch 265/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9717 - val_loss: 0.0548 - val_accuracy: 0.9755\n",
      "Epoch 266/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9721 - val_loss: 0.0822 - val_accuracy: 0.9674\n",
      "Epoch 267/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9712 - val_loss: 0.0688 - val_accuracy: 0.9719\n",
      "Epoch 268/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9718 - val_loss: 0.0545 - val_accuracy: 0.9753\n",
      "Epoch 269/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9714 - val_loss: 0.0595 - val_accuracy: 0.9751\n",
      "Epoch 270/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9725 - val_loss: 0.0555 - val_accuracy: 0.9759\n",
      "Epoch 271/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9720 - val_loss: 0.0772 - val_accuracy: 0.9693\n",
      "Epoch 272/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9713 - val_loss: 0.0607 - val_accuracy: 0.9726\n",
      "Epoch 273/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9719 - val_loss: 0.0569 - val_accuracy: 0.9746\n",
      "Epoch 274/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9729 - val_loss: 0.0646 - val_accuracy: 0.9721\n",
      "Epoch 275/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9727 - val_loss: 0.0629 - val_accuracy: 0.9727\n",
      "Epoch 276/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9717 - val_loss: 0.0584 - val_accuracy: 0.9746\n",
      "Epoch 277/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9717 - val_loss: 0.0612 - val_accuracy: 0.9730\n",
      "Epoch 278/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9696 - val_loss: 0.0641 - val_accuracy: 0.9721\n",
      "Epoch 279/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9729 - val_loss: 0.0626 - val_accuracy: 0.9733\n",
      "Epoch 280/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9738 - val_loss: 0.0836 - val_accuracy: 0.9669\n",
      "Epoch 281/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9722 - val_loss: 0.0645 - val_accuracy: 0.9735\n",
      "Epoch 282/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9734 - val_loss: 0.0571 - val_accuracy: 0.9759\n",
      "Epoch 283/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9722 - val_loss: 0.0695 - val_accuracy: 0.9709\n",
      "Epoch 284/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9736 - val_loss: 0.0740 - val_accuracy: 0.9698\n",
      "Epoch 285/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9721 - val_loss: 0.0738 - val_accuracy: 0.9699\n",
      "Epoch 286/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9718 - val_loss: 0.0634 - val_accuracy: 0.9728\n",
      "Epoch 287/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9720 - val_loss: 0.0701 - val_accuracy: 0.9719\n",
      "Epoch 288/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9745 - val_loss: 0.0622 - val_accuracy: 0.9736\n",
      "Epoch 289/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.0619 - val_accuracy: 0.9748\n",
      "Epoch 290/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9725 - val_loss: 0.0531 - val_accuracy: 0.9764\n",
      "Epoch 291/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9731 - val_loss: 0.0659 - val_accuracy: 0.9730\n",
      "Epoch 292/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9738 - val_loss: 0.0576 - val_accuracy: 0.9753\n",
      "Epoch 293/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9736 - val_loss: 0.0885 - val_accuracy: 0.9648\n",
      "Epoch 294/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9731 - val_loss: 0.0629 - val_accuracy: 0.9735\n",
      "Epoch 295/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0736 - accuracy: 0.9738 - val_loss: 0.0808 - val_accuracy: 0.9672\n",
      "Epoch 296/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9734 - val_loss: 0.0695 - val_accuracy: 0.9727\n",
      "Epoch 297/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9735 - val_loss: 0.0665 - val_accuracy: 0.9725\n",
      "Epoch 298/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9737 - val_loss: 0.0559 - val_accuracy: 0.9756\n",
      "Epoch 299/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9737 - val_loss: 0.0623 - val_accuracy: 0.9741\n",
      "Epoch 300/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9745 - val_loss: 0.0673 - val_accuracy: 0.9720\n",
      "Epoch 301/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9739 - val_loss: 0.0711 - val_accuracy: 0.9714\n",
      "Epoch 302/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.0757 - val_accuracy: 0.9698\n",
      "Epoch 303/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9740 - val_loss: 0.0616 - val_accuracy: 0.9743\n",
      "Epoch 304/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9753 - val_loss: 0.0645 - val_accuracy: 0.9730\n",
      "Epoch 305/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9735 - val_loss: 0.0576 - val_accuracy: 0.9750\n",
      "Epoch 306/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9737 - val_loss: 0.0543 - val_accuracy: 0.9761\n",
      "Epoch 307/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9746 - val_loss: 0.0744 - val_accuracy: 0.9707\n",
      "Epoch 308/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9737 - val_loss: 0.0558 - val_accuracy: 0.9764\n",
      "Epoch 309/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 0.0566 - val_accuracy: 0.9756\n",
      "Epoch 310/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9733 - val_loss: 0.0657 - val_accuracy: 0.9730\n",
      "Epoch 311/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9745 - val_loss: 0.0573 - val_accuracy: 0.9759\n",
      "Epoch 312/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9744 - val_loss: 0.0629 - val_accuracy: 0.9747\n",
      "Epoch 313/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9755 - val_loss: 0.0696 - val_accuracy: 0.9721\n",
      "Epoch 314/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9743 - val_loss: 0.0591 - val_accuracy: 0.9747\n",
      "Epoch 315/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9738 - val_loss: 0.0582 - val_accuracy: 0.9765\n",
      "Epoch 316/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9738 - val_loss: 0.0741 - val_accuracy: 0.9705\n",
      "Epoch 317/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.0645 - val_accuracy: 0.9727\n",
      "Epoch 318/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.0657 - val_accuracy: 0.9727\n",
      "Epoch 319/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9743 - val_loss: 0.0559 - val_accuracy: 0.9765\n",
      "Epoch 320/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.0564 - val_accuracy: 0.9758\n",
      "Epoch 321/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9739 - val_loss: 0.0802 - val_accuracy: 0.9693\n",
      "Epoch 322/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9745 - val_loss: 0.0758 - val_accuracy: 0.9697\n",
      "Epoch 323/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9742 - val_loss: 0.0631 - val_accuracy: 0.9736\n",
      "Epoch 324/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9750 - val_loss: 0.0626 - val_accuracy: 0.9743\n",
      "Epoch 325/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9753 - val_loss: 0.0676 - val_accuracy: 0.9727\n",
      "Epoch 326/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9755 - val_loss: 0.0666 - val_accuracy: 0.9721\n",
      "Epoch 327/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9752 - val_loss: 0.0549 - val_accuracy: 0.9769\n",
      "Epoch 328/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9746 - val_loss: 0.0611 - val_accuracy: 0.9747\n",
      "Epoch 329/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.0584 - val_accuracy: 0.9760\n",
      "Epoch 330/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.0635 - val_accuracy: 0.9741\n",
      "Epoch 331/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9751 - val_loss: 0.0621 - val_accuracy: 0.9749\n",
      "Epoch 332/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9758 - val_loss: 0.0592 - val_accuracy: 0.9750\n",
      "Epoch 333/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9756 - val_loss: 0.0559 - val_accuracy: 0.9764\n",
      "Epoch 334/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9759 - val_loss: 0.0659 - val_accuracy: 0.9737\n",
      "Epoch 335/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9756 - val_loss: 0.0731 - val_accuracy: 0.9716\n",
      "Epoch 336/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9758 - val_loss: 0.0644 - val_accuracy: 0.9733\n",
      "Epoch 337/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.0605 - val_accuracy: 0.9754\n",
      "Epoch 338/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9761 - val_loss: 0.0582 - val_accuracy: 0.9763\n",
      "Epoch 339/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9750 - val_loss: 0.0642 - val_accuracy: 0.9738\n",
      "Epoch 340/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9755 - val_loss: 0.0626 - val_accuracy: 0.9754\n",
      "Epoch 341/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9748 - val_loss: 0.0719 - val_accuracy: 0.9721\n",
      "Epoch 342/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9749 - val_loss: 0.0794 - val_accuracy: 0.9700\n",
      "Epoch 343/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.0842 - val_accuracy: 0.9687\n",
      "Epoch 344/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9749 - val_loss: 0.0722 - val_accuracy: 0.9725\n",
      "Epoch 345/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9750 - val_loss: 0.0668 - val_accuracy: 0.9730\n",
      "Epoch 346/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9756 - val_loss: 0.0621 - val_accuracy: 0.9743\n",
      "Epoch 347/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9752 - val_loss: 0.0604 - val_accuracy: 0.9755\n",
      "Epoch 348/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9759 - val_loss: 0.0513 - val_accuracy: 0.9774\n",
      "Epoch 349/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9758 - val_loss: 0.0702 - val_accuracy: 0.9726\n",
      "Epoch 350/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9765 - val_loss: 0.0675 - val_accuracy: 0.9728\n",
      "Epoch 351/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0694 - accuracy: 0.9757 - val_loss: 0.0739 - val_accuracy: 0.9713\n",
      "Epoch 352/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9756 - val_loss: 0.0688 - val_accuracy: 0.9729\n",
      "Epoch 353/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.0522 - val_accuracy: 0.9758\n",
      "Epoch 354/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9764 - val_loss: 0.0632 - val_accuracy: 0.9750\n",
      "Epoch 355/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9761 - val_loss: 0.0592 - val_accuracy: 0.9756\n",
      "Epoch 356/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9759 - val_loss: 0.0595 - val_accuracy: 0.9760\n",
      "Epoch 357/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9758 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 358/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9761 - val_loss: 0.0639 - val_accuracy: 0.9740\n",
      "Epoch 359/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9766 - val_loss: 0.0670 - val_accuracy: 0.9740\n",
      "Epoch 360/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9753 - val_loss: 0.0645 - val_accuracy: 0.9740\n",
      "Epoch 361/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9752 - val_loss: 0.0652 - val_accuracy: 0.9741\n",
      "Epoch 362/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9751 - val_loss: 0.0535 - val_accuracy: 0.9770\n",
      "Epoch 363/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9757 - val_loss: 0.0770 - val_accuracy: 0.9699\n",
      "Epoch 364/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9758 - val_loss: 0.0607 - val_accuracy: 0.9749\n",
      "Epoch 365/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9756 - val_loss: 0.0658 - val_accuracy: 0.9738\n",
      "Epoch 366/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9754 - val_loss: 0.0624 - val_accuracy: 0.9749\n",
      "Epoch 367/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9773 - val_loss: 0.0547 - val_accuracy: 0.9770\n",
      "Epoch 368/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9765 - val_loss: 0.0709 - val_accuracy: 0.9729\n",
      "Epoch 369/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9760 - val_loss: 0.0774 - val_accuracy: 0.9709\n",
      "Epoch 370/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9757 - val_loss: 0.0643 - val_accuracy: 0.9747\n",
      "Epoch 371/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.0664 - val_accuracy: 0.9736\n",
      "Epoch 372/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9769 - val_loss: 0.0545 - val_accuracy: 0.9770\n",
      "Epoch 373/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9769 - val_loss: 0.0751 - val_accuracy: 0.9723\n",
      "Epoch 374/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9755 - val_loss: 0.0787 - val_accuracy: 0.9700\n",
      "Epoch 375/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9754 - val_loss: 0.0655 - val_accuracy: 0.9744\n",
      "Epoch 376/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9763 - val_loss: 0.0697 - val_accuracy: 0.9723\n",
      "Epoch 377/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9766 - val_loss: 0.0534 - val_accuracy: 0.9764\n",
      "Epoch 378/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9757 - val_loss: 0.0522 - val_accuracy: 0.9766\n",
      "Epoch 379/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9753 - val_loss: 0.0615 - val_accuracy: 0.9750\n",
      "Epoch 380/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.0530 - val_accuracy: 0.9769\n",
      "Epoch 381/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9761 - val_loss: 0.0635 - val_accuracy: 0.9749\n",
      "Epoch 382/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9759 - val_loss: 0.0635 - val_accuracy: 0.9750\n",
      "Epoch 383/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9761 - val_loss: 0.0547 - val_accuracy: 0.9768\n",
      "Epoch 384/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9763 - val_loss: 0.0730 - val_accuracy: 0.9724\n",
      "Epoch 385/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9767 - val_loss: 0.0665 - val_accuracy: 0.9738\n",
      "Epoch 386/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9757 - val_loss: 0.0676 - val_accuracy: 0.9731\n",
      "Epoch 387/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9773 - val_loss: 0.0574 - val_accuracy: 0.9773\n",
      "Epoch 388/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9775 - val_loss: 0.0686 - val_accuracy: 0.9735\n",
      "Epoch 389/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9755 - val_loss: 0.0664 - val_accuracy: 0.9743\n",
      "Epoch 390/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9765 - val_loss: 0.0606 - val_accuracy: 0.9750\n",
      "Epoch 391/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9764 - val_loss: 0.0664 - val_accuracy: 0.9744\n",
      "Epoch 392/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9760 - val_loss: 0.0607 - val_accuracy: 0.9753\n",
      "Epoch 393/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9763 - val_loss: 0.0622 - val_accuracy: 0.9743\n",
      "Epoch 394/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9764 - val_loss: 0.0607 - val_accuracy: 0.9755\n",
      "Epoch 395/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.0678 - val_accuracy: 0.9744\n",
      "Epoch 396/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9760 - val_loss: 0.0529 - val_accuracy: 0.9774\n",
      "Epoch 397/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9774 - val_loss: 0.0516 - val_accuracy: 0.9779\n",
      "Epoch 398/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9765 - val_loss: 0.0668 - val_accuracy: 0.9738\n",
      "Epoch 399/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9775 - val_loss: 0.0507 - val_accuracy: 0.9770\n",
      "Epoch 400/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9766 - val_loss: 0.0560 - val_accuracy: 0.9770\n",
      "Epoch 401/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9763 - val_loss: 0.0583 - val_accuracy: 0.9758\n",
      "Epoch 402/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9771 - val_loss: 0.0584 - val_accuracy: 0.9765\n",
      "Epoch 403/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9763 - val_loss: 0.0631 - val_accuracy: 0.9749\n",
      "Epoch 404/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9775 - val_loss: 0.0689 - val_accuracy: 0.9731\n",
      "Epoch 405/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9773 - val_loss: 0.0576 - val_accuracy: 0.9759\n",
      "Epoch 406/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9756 - val_loss: 0.0554 - val_accuracy: 0.9765\n",
      "Epoch 407/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.0684 - val_accuracy: 0.9746\n",
      "Epoch 408/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9771 - val_loss: 0.0661 - val_accuracy: 0.9740\n",
      "Epoch 409/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9773 - val_loss: 0.0632 - val_accuracy: 0.9746\n",
      "Epoch 410/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.0618 - val_accuracy: 0.9758\n",
      "Epoch 411/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.0675 - val_accuracy: 0.9738\n",
      "Epoch 412/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9781 - val_loss: 0.0683 - val_accuracy: 0.9740\n",
      "Epoch 413/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9771 - val_loss: 0.0720 - val_accuracy: 0.9730\n",
      "Epoch 414/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9785 - val_loss: 0.0511 - val_accuracy: 0.9766\n",
      "Epoch 415/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9775 - val_loss: 0.0523 - val_accuracy: 0.9780\n",
      "Epoch 416/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9777 - val_loss: 0.0538 - val_accuracy: 0.9768\n",
      "Epoch 417/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9768 - val_loss: 0.0561 - val_accuracy: 0.9769\n",
      "Epoch 418/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9770 - val_loss: 0.0538 - val_accuracy: 0.9776\n",
      "Epoch 419/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 0.0611 - val_accuracy: 0.9755\n",
      "Epoch 420/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9771 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 421/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9767 - val_loss: 0.0569 - val_accuracy: 0.9766\n",
      "Epoch 422/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9782 - val_loss: 0.0535 - val_accuracy: 0.9777\n",
      "Epoch 423/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9765 - val_loss: 0.0608 - val_accuracy: 0.9760\n",
      "Epoch 424/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9776 - val_loss: 0.0527 - val_accuracy: 0.9767\n",
      "Epoch 425/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9776 - val_loss: 0.0546 - val_accuracy: 0.9766\n",
      "Epoch 426/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9767 - val_loss: 0.0672 - val_accuracy: 0.9739\n",
      "Epoch 427/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9758 - val_loss: 0.0702 - val_accuracy: 0.9731\n",
      "Epoch 428/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 0.0756 - val_accuracy: 0.9701\n",
      "Epoch 429/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9771 - val_loss: 0.0587 - val_accuracy: 0.9758\n",
      "Epoch 430/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9777 - val_loss: 0.0537 - val_accuracy: 0.9768\n",
      "Epoch 431/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9777 - val_loss: 0.0684 - val_accuracy: 0.9740\n",
      "Epoch 432/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9766 - val_loss: 0.0652 - val_accuracy: 0.9750\n",
      "Epoch 433/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9774 - val_loss: 0.0616 - val_accuracy: 0.9754\n",
      "Epoch 434/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9772 - val_loss: 0.0647 - val_accuracy: 0.9741\n",
      "Epoch 435/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9780 - val_loss: 0.0602 - val_accuracy: 0.9761\n",
      "Epoch 436/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9772 - val_loss: 0.0698 - val_accuracy: 0.9743\n",
      "Epoch 437/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9772 - val_loss: 0.0782 - val_accuracy: 0.9717\n",
      "Epoch 438/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.0598 - val_accuracy: 0.9763\n",
      "Epoch 439/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9772 - val_loss: 0.0551 - val_accuracy: 0.9776\n",
      "Epoch 440/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9777 - val_loss: 0.0751 - val_accuracy: 0.9728\n",
      "Epoch 441/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9778 - val_loss: 0.0552 - val_accuracy: 0.9765\n",
      "Epoch 442/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9779 - val_loss: 0.0578 - val_accuracy: 0.9767\n",
      "Epoch 443/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.0647 - val_accuracy: 0.9753\n",
      "Epoch 444/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9771 - val_loss: 0.0650 - val_accuracy: 0.9749\n",
      "Epoch 445/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9784 - val_loss: 0.0670 - val_accuracy: 0.9747\n",
      "Epoch 446/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9784 - val_loss: 0.0621 - val_accuracy: 0.9763\n",
      "Epoch 447/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 0.0586 - val_accuracy: 0.9765\n",
      "Epoch 448/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9776 - val_loss: 0.0647 - val_accuracy: 0.9750\n",
      "Epoch 449/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9781 - val_loss: 0.0630 - val_accuracy: 0.9755\n",
      "Epoch 450/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9773 - val_loss: 0.0534 - val_accuracy: 0.9776\n",
      "Epoch 451/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9788 - val_loss: 0.0574 - val_accuracy: 0.9770\n",
      "Epoch 452/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0614 - accuracy: 0.9777 - val_loss: 0.0515 - val_accuracy: 0.9775\n",
      "Epoch 453/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9776 - val_loss: 0.0481 - val_accuracy: 0.9784\n",
      "Epoch 454/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9766 - val_loss: 0.0556 - val_accuracy: 0.9767\n",
      "Epoch 455/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9783 - val_loss: 0.0597 - val_accuracy: 0.9774\n",
      "Epoch 456/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9778 - val_loss: 0.0630 - val_accuracy: 0.9749\n",
      "Epoch 457/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9776 - val_loss: 0.0575 - val_accuracy: 0.9760\n",
      "Epoch 458/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9780 - val_loss: 0.0563 - val_accuracy: 0.9776\n",
      "Epoch 459/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9770 - val_loss: 0.0578 - val_accuracy: 0.9763\n",
      "Epoch 460/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9785 - val_loss: 0.0530 - val_accuracy: 0.9764\n",
      "Epoch 461/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9783 - val_loss: 0.0551 - val_accuracy: 0.9776\n",
      "Epoch 462/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9781 - val_loss: 0.0579 - val_accuracy: 0.9775\n",
      "Epoch 463/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9785 - val_loss: 0.0641 - val_accuracy: 0.9751\n",
      "Epoch 464/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9784 - val_loss: 0.0539 - val_accuracy: 0.9784\n",
      "Epoch 465/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9777 - val_loss: 0.0590 - val_accuracy: 0.9773\n",
      "Epoch 466/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9784 - val_loss: 0.0577 - val_accuracy: 0.9766\n",
      "Epoch 467/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9776 - val_loss: 0.0680 - val_accuracy: 0.9738\n",
      "Epoch 468/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9788 - val_loss: 0.0553 - val_accuracy: 0.9770\n",
      "Epoch 469/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9783 - val_loss: 0.0565 - val_accuracy: 0.9769\n",
      "Epoch 470/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9775 - val_loss: 0.0567 - val_accuracy: 0.9769\n",
      "Epoch 471/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9777 - val_loss: 0.0553 - val_accuracy: 0.9773\n",
      "Epoch 472/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9774 - val_loss: 0.0719 - val_accuracy: 0.9728\n",
      "Epoch 473/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9782 - val_loss: 0.0604 - val_accuracy: 0.9760\n",
      "Epoch 474/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9776 - val_loss: 0.0554 - val_accuracy: 0.9776\n",
      "Epoch 475/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9788 - val_loss: 0.0557 - val_accuracy: 0.9776\n",
      "Epoch 476/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9777 - val_loss: 0.0654 - val_accuracy: 0.9749\n",
      "Epoch 477/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9779 - val_loss: 0.0648 - val_accuracy: 0.9754\n",
      "Epoch 478/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9776 - val_loss: 0.0573 - val_accuracy: 0.9770\n",
      "Epoch 479/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9784 - val_loss: 0.0537 - val_accuracy: 0.9778\n",
      "Epoch 480/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9777 - val_loss: 0.0573 - val_accuracy: 0.9769\n",
      "Epoch 481/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9776 - val_loss: 0.0670 - val_accuracy: 0.9748\n",
      "Epoch 482/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9779 - val_loss: 0.0608 - val_accuracy: 0.9765\n",
      "Epoch 483/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9793 - val_loss: 0.0664 - val_accuracy: 0.9751\n",
      "Epoch 484/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9788 - val_loss: 0.0625 - val_accuracy: 0.9754\n",
      "Epoch 485/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9773 - val_loss: 0.0615 - val_accuracy: 0.9756\n",
      "Epoch 486/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9779 - val_loss: 0.0546 - val_accuracy: 0.9773\n",
      "Epoch 487/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9781 - val_loss: 0.0613 - val_accuracy: 0.9764\n",
      "Epoch 488/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9782 - val_loss: 0.0642 - val_accuracy: 0.9753\n",
      "Epoch 489/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9783 - val_loss: 0.0506 - val_accuracy: 0.9791\n",
      "Epoch 490/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9786 - val_loss: 0.0628 - val_accuracy: 0.9760\n",
      "Epoch 491/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9781 - val_loss: 0.0507 - val_accuracy: 0.9767\n",
      "Epoch 492/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9772 - val_loss: 0.0612 - val_accuracy: 0.9759\n",
      "Epoch 493/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9780 - val_loss: 0.0592 - val_accuracy: 0.9763\n",
      "Epoch 494/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.0578 - val_accuracy: 0.9773\n",
      "Epoch 495/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 0.0552 - val_accuracy: 0.9769\n",
      "Epoch 496/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9781 - val_loss: 0.0577 - val_accuracy: 0.9773\n",
      "Epoch 497/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0604 - accuracy: 0.9787 - val_loss: 0.0549 - val_accuracy: 0.9774\n",
      "Epoch 498/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9779 - val_loss: 0.0641 - val_accuracy: 0.9756\n",
      "Epoch 499/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.0613 - val_accuracy: 0.9755\n",
      "Epoch 500/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9789 - val_loss: 0.0517 - val_accuracy: 0.9779\n",
      "Epoch 501/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 0.0515 - val_accuracy: 0.9779\n",
      "Epoch 502/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9785 - val_loss: 0.0543 - val_accuracy: 0.9784\n",
      "Epoch 503/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9782 - val_loss: 0.0557 - val_accuracy: 0.9771\n",
      "Epoch 504/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9779 - val_loss: 0.0685 - val_accuracy: 0.9743\n",
      "Epoch 505/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9785 - val_loss: 0.0659 - val_accuracy: 0.9744\n",
      "Epoch 506/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9792 - val_loss: 0.0523 - val_accuracy: 0.9782\n",
      "Epoch 507/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9788 - val_loss: 0.0691 - val_accuracy: 0.9749\n",
      "Epoch 508/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9785 - val_loss: 0.0575 - val_accuracy: 0.9764\n",
      "Epoch 509/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9771 - val_loss: 0.0525 - val_accuracy: 0.9774\n",
      "Epoch 510/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9792 - val_loss: 0.0631 - val_accuracy: 0.9761\n",
      "Epoch 511/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9783 - val_loss: 0.0596 - val_accuracy: 0.9751\n",
      "Epoch 512/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9785 - val_loss: 0.0644 - val_accuracy: 0.9754\n",
      "Epoch 513/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9783 - val_loss: 0.0598 - val_accuracy: 0.9761\n",
      "Epoch 514/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9786 - val_loss: 0.0601 - val_accuracy: 0.9756\n",
      "Epoch 515/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9789 - val_loss: 0.0542 - val_accuracy: 0.9774\n",
      "Epoch 516/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9778 - val_loss: 0.0561 - val_accuracy: 0.9771\n",
      "Epoch 517/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9788 - val_loss: 0.0573 - val_accuracy: 0.9777\n",
      "Epoch 518/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9792 - val_loss: 0.0596 - val_accuracy: 0.9766\n",
      "Epoch 519/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.0553 - val_accuracy: 0.9766\n",
      "Epoch 520/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9789 - val_loss: 0.0618 - val_accuracy: 0.9757\n",
      "Epoch 521/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9794 - val_loss: 0.0576 - val_accuracy: 0.9782\n",
      "Epoch 522/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9796 - val_loss: 0.0508 - val_accuracy: 0.9790\n",
      "Epoch 523/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9795 - val_loss: 0.0643 - val_accuracy: 0.9756\n",
      "Epoch 524/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9791 - val_loss: 0.0562 - val_accuracy: 0.9781\n",
      "Epoch 525/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9793 - val_loss: 0.0551 - val_accuracy: 0.9777\n",
      "Epoch 526/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.0606 - val_accuracy: 0.9756\n",
      "Epoch 527/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.0708 - val_accuracy: 0.9740\n",
      "Epoch 528/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9797 - val_loss: 0.0523 - val_accuracy: 0.9775\n",
      "Epoch 529/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9795 - val_loss: 0.0590 - val_accuracy: 0.9761\n",
      "Epoch 530/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9791 - val_loss: 0.0572 - val_accuracy: 0.9765\n",
      "Epoch 531/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9783 - val_loss: 0.0511 - val_accuracy: 0.9775\n",
      "Epoch 532/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9787 - val_loss: 0.0528 - val_accuracy: 0.9779\n",
      "Epoch 533/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9781 - val_loss: 0.0613 - val_accuracy: 0.9766\n",
      "Epoch 534/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 0.0486 - val_accuracy: 0.9778\n",
      "Epoch 535/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9778 - val_loss: 0.0586 - val_accuracy: 0.9760\n",
      "Epoch 536/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9785 - val_loss: 0.0628 - val_accuracy: 0.9756\n",
      "Epoch 537/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9800 - val_loss: 0.0543 - val_accuracy: 0.9771\n",
      "Epoch 538/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9777 - val_loss: 0.0582 - val_accuracy: 0.9769\n",
      "Epoch 539/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.0515 - val_accuracy: 0.9784\n",
      "Epoch 540/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9779 - val_loss: 0.0554 - val_accuracy: 0.9771\n",
      "Epoch 541/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9791 - val_loss: 0.0555 - val_accuracy: 0.9773\n",
      "Epoch 542/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.0621 - val_accuracy: 0.9757\n",
      "Epoch 543/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9792 - val_loss: 0.0525 - val_accuracy: 0.9774\n",
      "Epoch 544/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9787 - val_loss: 0.0510 - val_accuracy: 0.9775\n",
      "Epoch 545/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9793 - val_loss: 0.0618 - val_accuracy: 0.9760\n",
      "Epoch 546/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.0588 - val_accuracy: 0.9774\n",
      "Epoch 547/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9790 - val_loss: 0.0599 - val_accuracy: 0.9759\n",
      "Epoch 548/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9789 - val_loss: 0.0532 - val_accuracy: 0.9780\n",
      "Epoch 549/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9795 - val_loss: 0.0589 - val_accuracy: 0.9767\n",
      "Epoch 550/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.0520 - val_accuracy: 0.9775\n",
      "Epoch 551/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9782 - val_loss: 0.0544 - val_accuracy: 0.9770\n",
      "Epoch 552/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9790 - val_loss: 0.0637 - val_accuracy: 0.9757\n",
      "Epoch 553/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 0.0573 - val_accuracy: 0.9766\n",
      "Epoch 554/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9799 - val_loss: 0.0580 - val_accuracy: 0.9765\n",
      "Epoch 555/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9794 - val_loss: 0.0538 - val_accuracy: 0.9769\n",
      "Epoch 556/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9786 - val_loss: 0.0529 - val_accuracy: 0.9774\n",
      "Epoch 557/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 0.0562 - val_accuracy: 0.9775\n",
      "Epoch 558/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9794 - val_loss: 0.0657 - val_accuracy: 0.9750\n",
      "Epoch 559/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9792 - val_loss: 0.0584 - val_accuracy: 0.9766\n",
      "Epoch 560/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9791 - val_loss: 0.0536 - val_accuracy: 0.9775\n",
      "Epoch 561/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9785 - val_loss: 0.0539 - val_accuracy: 0.9768\n",
      "Epoch 562/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9796 - val_loss: 0.0462 - val_accuracy: 0.9790\n",
      "Epoch 563/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9797 - val_loss: 0.0558 - val_accuracy: 0.9766\n",
      "Epoch 564/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9794 - val_loss: 0.0539 - val_accuracy: 0.9782\n",
      "Epoch 565/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9782 - val_loss: 0.0652 - val_accuracy: 0.9758\n",
      "Epoch 566/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9795 - val_loss: 0.0620 - val_accuracy: 0.9758\n",
      "Epoch 567/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9794 - val_loss: 0.0558 - val_accuracy: 0.9770\n",
      "Epoch 568/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9794 - val_loss: 0.0629 - val_accuracy: 0.9750\n",
      "Epoch 569/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9795 - val_loss: 0.0541 - val_accuracy: 0.9769\n",
      "Epoch 570/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9800 - val_loss: 0.0539 - val_accuracy: 0.9771\n",
      "Epoch 571/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9785 - val_loss: 0.0620 - val_accuracy: 0.9757\n",
      "Epoch 572/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9791 - val_loss: 0.0565 - val_accuracy: 0.9765\n",
      "Epoch 573/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9798 - val_loss: 0.0502 - val_accuracy: 0.9774\n",
      "Epoch 574/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9789 - val_loss: 0.0667 - val_accuracy: 0.9747\n",
      "Epoch 575/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9794 - val_loss: 0.0608 - val_accuracy: 0.9769\n",
      "Epoch 576/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.0593 - val_accuracy: 0.9773\n",
      "Epoch 577/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9788 - val_loss: 0.0611 - val_accuracy: 0.9764\n",
      "Epoch 578/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9783 - val_loss: 0.0536 - val_accuracy: 0.9761\n",
      "Epoch 579/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9795 - val_loss: 0.0563 - val_accuracy: 0.9765\n",
      "Epoch 580/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9783 - val_loss: 0.0556 - val_accuracy: 0.9767\n",
      "Epoch 581/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9803 - val_loss: 0.0628 - val_accuracy: 0.9758\n",
      "Epoch 582/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.0781 - val_accuracy: 0.9736\n",
      "Epoch 583/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 0.0702 - val_accuracy: 0.9740\n",
      "Epoch 584/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9799 - val_loss: 0.0592 - val_accuracy: 0.9763\n",
      "Epoch 585/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9791 - val_loss: 0.0502 - val_accuracy: 0.9786\n",
      "Epoch 586/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9799 - val_loss: 0.0529 - val_accuracy: 0.9768\n",
      "Epoch 587/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9796 - val_loss: 0.0665 - val_accuracy: 0.9757\n",
      "Epoch 588/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.0576 - val_accuracy: 0.9769\n",
      "Epoch 589/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9796 - val_loss: 0.0497 - val_accuracy: 0.9773\n",
      "Epoch 590/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9796 - val_loss: 0.0537 - val_accuracy: 0.9776\n",
      "Epoch 591/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9790 - val_loss: 0.0518 - val_accuracy: 0.9774\n",
      "Epoch 592/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9790 - val_loss: 0.0496 - val_accuracy: 0.9781\n",
      "Epoch 593/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9802 - val_loss: 0.0591 - val_accuracy: 0.9768\n",
      "Epoch 594/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9790 - val_loss: 0.0621 - val_accuracy: 0.9763\n",
      "Epoch 595/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9799 - val_loss: 0.0544 - val_accuracy: 0.9776\n",
      "Epoch 596/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9800 - val_loss: 0.0611 - val_accuracy: 0.9763\n",
      "Epoch 597/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9795 - val_loss: 0.0610 - val_accuracy: 0.9764\n",
      "Epoch 598/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0564 - accuracy: 0.9796 - val_loss: 0.0513 - val_accuracy: 0.9765\n",
      "Epoch 599/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9795 - val_loss: 0.0512 - val_accuracy: 0.9782\n",
      "Epoch 600/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9795 - val_loss: 0.0645 - val_accuracy: 0.9761\n",
      "Epoch 601/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9802 - val_loss: 0.0604 - val_accuracy: 0.9764\n",
      "Epoch 602/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9795 - val_loss: 0.0509 - val_accuracy: 0.9776\n",
      "Epoch 603/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9808 - val_loss: 0.0609 - val_accuracy: 0.9763\n",
      "Epoch 604/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9815 - val_loss: 0.0623 - val_accuracy: 0.9754\n",
      "Epoch 605/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9800 - val_loss: 0.0548 - val_accuracy: 0.9774\n",
      "Epoch 606/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9799 - val_loss: 0.0518 - val_accuracy: 0.9785\n",
      "Epoch 607/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9796 - val_loss: 0.0549 - val_accuracy: 0.9771\n",
      "Epoch 608/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9794 - val_loss: 0.0579 - val_accuracy: 0.9777\n",
      "Epoch 609/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.0562 - val_accuracy: 0.9781\n",
      "Epoch 610/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9794 - val_loss: 0.0563 - val_accuracy: 0.9773\n",
      "Epoch 611/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9796 - val_loss: 0.0498 - val_accuracy: 0.9779\n",
      "Epoch 612/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9794 - val_loss: 0.0521 - val_accuracy: 0.9778\n",
      "Epoch 613/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9801 - val_loss: 0.0571 - val_accuracy: 0.9777\n",
      "Epoch 614/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9789 - val_loss: 0.0542 - val_accuracy: 0.9768\n",
      "Epoch 615/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9804 - val_loss: 0.0506 - val_accuracy: 0.9777\n",
      "Epoch 616/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9795 - val_loss: 0.0533 - val_accuracy: 0.9776\n",
      "Epoch 617/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9805 - val_loss: 0.0630 - val_accuracy: 0.9758\n",
      "Epoch 618/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9802 - val_loss: 0.0592 - val_accuracy: 0.9770\n",
      "Epoch 619/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.0623 - val_accuracy: 0.9758\n",
      "Epoch 620/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9796 - val_loss: 0.0568 - val_accuracy: 0.9775\n",
      "Epoch 621/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9800 - val_loss: 0.0490 - val_accuracy: 0.9769\n",
      "Epoch 622/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9797 - val_loss: 0.0625 - val_accuracy: 0.9759\n",
      "Epoch 623/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9791 - val_loss: 0.0618 - val_accuracy: 0.9756\n",
      "Epoch 624/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.0588 - val_accuracy: 0.9777\n",
      "Epoch 625/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.0611 - val_accuracy: 0.9757\n",
      "Epoch 626/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9809 - val_loss: 0.0611 - val_accuracy: 0.9768\n",
      "Epoch 627/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9803 - val_loss: 0.0565 - val_accuracy: 0.9773\n",
      "Epoch 628/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9792 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 629/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9803 - val_loss: 0.0560 - val_accuracy: 0.9767\n",
      "Epoch 630/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9788 - val_loss: 0.0561 - val_accuracy: 0.9755\n",
      "Epoch 631/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9793 - val_loss: 0.0689 - val_accuracy: 0.9753\n",
      "Epoch 632/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9800 - val_loss: 0.0538 - val_accuracy: 0.9778\n",
      "Epoch 633/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9793 - val_loss: 0.0555 - val_accuracy: 0.9771\n",
      "Epoch 634/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9798 - val_loss: 0.0616 - val_accuracy: 0.9755\n",
      "Epoch 635/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9797 - val_loss: 0.0643 - val_accuracy: 0.9757\n",
      "Epoch 636/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9807 - val_loss: 0.0572 - val_accuracy: 0.9776\n",
      "Epoch 637/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9798 - val_loss: 0.0587 - val_accuracy: 0.9776\n",
      "Epoch 638/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9793 - val_loss: 0.0548 - val_accuracy: 0.9777\n",
      "Epoch 639/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.0612 - val_accuracy: 0.9764\n",
      "Epoch 640/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9788 - val_loss: 0.0613 - val_accuracy: 0.9758\n",
      "Epoch 641/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9808 - val_loss: 0.0512 - val_accuracy: 0.9780\n",
      "Epoch 642/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9799 - val_loss: 0.0530 - val_accuracy: 0.9778\n",
      "Epoch 643/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9799 - val_loss: 0.0492 - val_accuracy: 0.9771\n",
      "Epoch 644/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9803 - val_loss: 0.0639 - val_accuracy: 0.9757\n",
      "Epoch 645/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9795 - val_loss: 0.0568 - val_accuracy: 0.9776\n",
      "Epoch 646/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9811 - val_loss: 0.0508 - val_accuracy: 0.9758\n",
      "Epoch 647/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9795 - val_loss: 0.0568 - val_accuracy: 0.9770\n",
      "Epoch 648/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9802 - val_loss: 0.0593 - val_accuracy: 0.9768\n",
      "Epoch 649/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9807 - val_loss: 0.0627 - val_accuracy: 0.9760\n",
      "Epoch 650/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0666 - val_accuracy: 0.9753\n",
      "Epoch 651/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9790 - val_loss: 0.0531 - val_accuracy: 0.9779\n",
      "Epoch 652/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9799 - val_loss: 0.0559 - val_accuracy: 0.9773\n",
      "Epoch 653/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9809 - val_loss: 0.0566 - val_accuracy: 0.9777\n",
      "Epoch 654/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9805 - val_loss: 0.0688 - val_accuracy: 0.9748\n",
      "Epoch 655/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9802 - val_loss: 0.0559 - val_accuracy: 0.9768\n",
      "Epoch 656/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9806 - val_loss: 0.0529 - val_accuracy: 0.9775\n",
      "Epoch 657/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9803 - val_loss: 0.0607 - val_accuracy: 0.9765\n",
      "Epoch 658/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9811 - val_loss: 0.0638 - val_accuracy: 0.9751\n",
      "Epoch 659/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.0520 - val_accuracy: 0.9781\n",
      "Epoch 660/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9799 - val_loss: 0.0538 - val_accuracy: 0.9774\n",
      "Epoch 661/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.0616 - val_accuracy: 0.9763\n",
      "Epoch 662/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9805 - val_loss: 0.0681 - val_accuracy: 0.9756\n",
      "Epoch 663/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9799 - val_loss: 0.0626 - val_accuracy: 0.9754\n",
      "Epoch 664/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9803 - val_loss: 0.0653 - val_accuracy: 0.9761\n",
      "Epoch 665/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9819 - val_loss: 0.0612 - val_accuracy: 0.9760\n",
      "Epoch 666/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.0553 - val_accuracy: 0.9771\n",
      "Epoch 667/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9800 - val_loss: 0.0542 - val_accuracy: 0.9775\n",
      "Epoch 668/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9793 - val_loss: 0.0534 - val_accuracy: 0.9781\n",
      "Epoch 669/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.0480 - val_accuracy: 0.9791\n",
      "Epoch 670/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9799 - val_loss: 0.0559 - val_accuracy: 0.9778\n",
      "Epoch 671/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.0571 - val_accuracy: 0.9769\n",
      "Epoch 672/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9800 - val_loss: 0.0620 - val_accuracy: 0.9756\n",
      "Epoch 673/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9796 - val_loss: 0.0506 - val_accuracy: 0.9791\n",
      "Epoch 674/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0519 - val_accuracy: 0.9785\n",
      "Epoch 675/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.0560 - val_accuracy: 0.9769\n",
      "Epoch 676/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9800 - val_loss: 0.0528 - val_accuracy: 0.9770\n",
      "Epoch 677/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9799 - val_loss: 0.0545 - val_accuracy: 0.9775\n",
      "Epoch 678/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.0549 - val_accuracy: 0.9768\n",
      "Epoch 679/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9806 - val_loss: 0.0632 - val_accuracy: 0.9758\n",
      "Epoch 680/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9803 - val_loss: 0.0653 - val_accuracy: 0.9751\n",
      "Epoch 681/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9795 - val_loss: 0.0502 - val_accuracy: 0.9773\n",
      "Epoch 682/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9804 - val_loss: 0.0611 - val_accuracy: 0.9767\n",
      "Epoch 683/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9788 - val_loss: 0.0550 - val_accuracy: 0.9767\n",
      "Epoch 684/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0573 - val_accuracy: 0.9767\n",
      "Epoch 685/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9803 - val_loss: 0.0544 - val_accuracy: 0.9779\n",
      "Epoch 686/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9808 - val_loss: 0.0574 - val_accuracy: 0.9769\n",
      "Epoch 687/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9810 - val_loss: 0.0544 - val_accuracy: 0.9780\n",
      "Epoch 688/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9803 - val_loss: 0.0574 - val_accuracy: 0.9765\n",
      "Epoch 689/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9808 - val_loss: 0.0573 - val_accuracy: 0.9769\n",
      "Epoch 690/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9797 - val_loss: 0.0612 - val_accuracy: 0.9763\n",
      "Epoch 691/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9809 - val_loss: 0.0527 - val_accuracy: 0.9767\n",
      "Epoch 692/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9800 - val_loss: 0.0541 - val_accuracy: 0.9784\n",
      "Epoch 693/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9803 - val_loss: 0.0504 - val_accuracy: 0.9786\n",
      "Epoch 694/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9798 - val_loss: 0.0695 - val_accuracy: 0.9753\n",
      "Epoch 695/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9806 - val_loss: 0.0707 - val_accuracy: 0.9746\n",
      "Epoch 696/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9805 - val_loss: 0.0624 - val_accuracy: 0.9759\n",
      "Epoch 697/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9803 - val_loss: 0.0577 - val_accuracy: 0.9778\n",
      "Epoch 698/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9798 - val_loss: 0.0629 - val_accuracy: 0.9765\n",
      "Epoch 699/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 0.0716 - val_accuracy: 0.9751\n",
      "Epoch 700/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9800 - val_loss: 0.0678 - val_accuracy: 0.9757\n",
      "Epoch 701/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9810 - val_loss: 0.0560 - val_accuracy: 0.9774\n",
      "Epoch 702/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9812 - val_loss: 0.0587 - val_accuracy: 0.9763\n",
      "Epoch 703/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9803 - val_loss: 0.0492 - val_accuracy: 0.9780\n",
      "Epoch 704/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9811 - val_loss: 0.0583 - val_accuracy: 0.9777\n",
      "Epoch 705/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9812 - val_loss: 0.0549 - val_accuracy: 0.9775\n",
      "Epoch 706/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9804 - val_loss: 0.0556 - val_accuracy: 0.9775\n",
      "Epoch 707/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9799 - val_loss: 0.0578 - val_accuracy: 0.9771\n",
      "Epoch 708/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9815 - val_loss: 0.0663 - val_accuracy: 0.9753\n",
      "Epoch 709/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9795 - val_loss: 0.0563 - val_accuracy: 0.9780\n",
      "Epoch 710/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.0577 - val_accuracy: 0.9775\n",
      "Epoch 711/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9813 - val_loss: 0.0733 - val_accuracy: 0.9751\n",
      "Epoch 712/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9789 - val_loss: 0.0518 - val_accuracy: 0.9782\n",
      "Epoch 713/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9815 - val_loss: 0.0568 - val_accuracy: 0.9765\n",
      "Epoch 714/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9805 - val_loss: 0.0609 - val_accuracy: 0.9771\n",
      "Epoch 715/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9801 - val_loss: 0.0557 - val_accuracy: 0.9786\n",
      "Epoch 716/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9808 - val_loss: 0.0531 - val_accuracy: 0.9782\n",
      "Epoch 717/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9818 - val_loss: 0.0481 - val_accuracy: 0.9773\n",
      "Epoch 718/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9799 - val_loss: 0.0537 - val_accuracy: 0.9775\n",
      "Epoch 719/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9808 - val_loss: 0.0512 - val_accuracy: 0.9775\n",
      "Epoch 720/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9805 - val_loss: 0.0624 - val_accuracy: 0.9770\n",
      "Epoch 721/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9802 - val_loss: 0.0651 - val_accuracy: 0.9764\n",
      "Epoch 722/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.0520 - val_accuracy: 0.9768\n",
      "Epoch 723/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9806 - val_loss: 0.0624 - val_accuracy: 0.9766\n",
      "Epoch 724/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9796 - val_loss: 0.0550 - val_accuracy: 0.9773\n",
      "Epoch 725/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9800 - val_loss: 0.0572 - val_accuracy: 0.9777\n",
      "Epoch 726/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 0.0693 - val_accuracy: 0.9750\n",
      "Epoch 727/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9806 - val_loss: 0.0525 - val_accuracy: 0.9774\n",
      "Epoch 728/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9805 - val_loss: 0.0614 - val_accuracy: 0.9763\n",
      "Epoch 729/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.0495 - val_accuracy: 0.9781\n",
      "Epoch 730/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9822 - val_loss: 0.0557 - val_accuracy: 0.9776\n",
      "Epoch 731/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9809 - val_loss: 0.0535 - val_accuracy: 0.9779\n",
      "Epoch 732/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9806 - val_loss: 0.0625 - val_accuracy: 0.9764\n",
      "Epoch 733/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9804 - val_loss: 0.0570 - val_accuracy: 0.9777\n",
      "Epoch 734/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9806 - val_loss: 0.0686 - val_accuracy: 0.9755\n",
      "Epoch 735/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9810 - val_loss: 0.0532 - val_accuracy: 0.9784\n",
      "Epoch 736/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9813 - val_loss: 0.0542 - val_accuracy: 0.9770\n",
      "Epoch 737/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9811 - val_loss: 0.0665 - val_accuracy: 0.9755\n",
      "Epoch 738/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9809 - val_loss: 0.0555 - val_accuracy: 0.9776\n",
      "Epoch 739/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9805 - val_loss: 0.0548 - val_accuracy: 0.9771\n",
      "Epoch 740/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.0551 - val_accuracy: 0.9779\n",
      "Epoch 741/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9811 - val_loss: 0.0574 - val_accuracy: 0.9766\n",
      "Epoch 742/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9809 - val_loss: 0.0518 - val_accuracy: 0.9787\n",
      "Epoch 743/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9818 - val_loss: 0.0692 - val_accuracy: 0.9753\n",
      "Epoch 744/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9812 - val_loss: 0.0637 - val_accuracy: 0.9761\n",
      "Epoch 745/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9808 - val_loss: 0.0526 - val_accuracy: 0.9782\n",
      "Epoch 746/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.0601 - val_accuracy: 0.9770\n",
      "Epoch 747/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9809 - val_loss: 0.0564 - val_accuracy: 0.9784\n",
      "Epoch 748/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9807 - val_loss: 0.0552 - val_accuracy: 0.9773\n",
      "Epoch 749/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9811 - val_loss: 0.0566 - val_accuracy: 0.9779\n",
      "Epoch 750/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.0584 - val_accuracy: 0.9776\n",
      "Epoch 751/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9807 - val_loss: 0.0539 - val_accuracy: 0.9779\n",
      "Epoch 752/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9814 - val_loss: 0.0753 - val_accuracy: 0.9747\n",
      "Epoch 753/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9803 - val_loss: 0.0589 - val_accuracy: 0.9773\n",
      "Epoch 754/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9802 - val_loss: 0.0650 - val_accuracy: 0.9758\n",
      "Epoch 755/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0549 - val_accuracy: 0.9773\n",
      "Epoch 756/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9804 - val_loss: 0.0532 - val_accuracy: 0.9778\n",
      "Epoch 757/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9819 - val_loss: 0.0632 - val_accuracy: 0.9770\n",
      "Epoch 758/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9825 - val_loss: 0.0514 - val_accuracy: 0.9779\n",
      "Epoch 759/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9816 - val_loss: 0.0514 - val_accuracy: 0.9785\n",
      "Epoch 760/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.0607 - val_accuracy: 0.9766\n",
      "Epoch 761/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9801 - val_loss: 0.0537 - val_accuracy: 0.9774\n",
      "Epoch 762/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9809 - val_loss: 0.0535 - val_accuracy: 0.9777\n",
      "Epoch 763/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9811 - val_loss: 0.0519 - val_accuracy: 0.9763\n",
      "Epoch 764/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9808 - val_loss: 0.0548 - val_accuracy: 0.9771\n",
      "Epoch 765/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9813 - val_loss: 0.0581 - val_accuracy: 0.9769\n",
      "Epoch 766/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9814 - val_loss: 0.0596 - val_accuracy: 0.9769\n",
      "Epoch 767/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9812 - val_loss: 0.0524 - val_accuracy: 0.9779\n",
      "Epoch 768/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9819 - val_loss: 0.0590 - val_accuracy: 0.9773\n",
      "Epoch 769/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9811 - val_loss: 0.0511 - val_accuracy: 0.9782\n",
      "Epoch 770/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9805 - val_loss: 0.0547 - val_accuracy: 0.9768\n",
      "Epoch 771/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.0538 - val_accuracy: 0.9779\n",
      "Epoch 772/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9807 - val_loss: 0.0551 - val_accuracy: 0.9775\n",
      "Epoch 773/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9808 - val_loss: 0.0623 - val_accuracy: 0.9769\n",
      "Epoch 774/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0495 - val_accuracy: 0.9778\n",
      "Epoch 775/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9816 - val_loss: 0.0576 - val_accuracy: 0.9778\n",
      "Epoch 776/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9813 - val_loss: 0.0592 - val_accuracy: 0.9775\n",
      "Epoch 777/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.0567 - val_accuracy: 0.9771\n",
      "Epoch 778/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9824 - val_loss: 0.0614 - val_accuracy: 0.9760\n",
      "Epoch 779/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9802 - val_loss: 0.0581 - val_accuracy: 0.9771\n",
      "Epoch 780/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.0550 - val_accuracy: 0.9768\n",
      "Epoch 781/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0494 - val_accuracy: 0.9782\n",
      "Epoch 782/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9810 - val_loss: 0.0546 - val_accuracy: 0.9777\n",
      "Epoch 783/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9824 - val_loss: 0.0561 - val_accuracy: 0.9765\n",
      "Epoch 784/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9811 - val_loss: 0.0594 - val_accuracy: 0.9768\n",
      "Epoch 785/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.0671 - val_accuracy: 0.9751\n",
      "Epoch 786/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.0564 - val_accuracy: 0.9769\n",
      "Epoch 787/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9820 - val_loss: 0.0516 - val_accuracy: 0.9781\n",
      "Epoch 788/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9818 - val_loss: 0.0545 - val_accuracy: 0.9781\n",
      "Epoch 789/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.0526 - val_accuracy: 0.9776\n",
      "Epoch 790/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0595 - val_accuracy: 0.9765\n",
      "Epoch 791/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9807 - val_loss: 0.0555 - val_accuracy: 0.9777\n",
      "Epoch 792/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 0.0624 - val_accuracy: 0.9770\n",
      "Epoch 793/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9812 - val_loss: 0.0588 - val_accuracy: 0.9777\n",
      "Epoch 794/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9818 - val_loss: 0.0559 - val_accuracy: 0.9763\n",
      "Epoch 795/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9813 - val_loss: 0.0554 - val_accuracy: 0.9770\n",
      "Epoch 796/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9819 - val_loss: 0.0678 - val_accuracy: 0.9748\n",
      "Epoch 797/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9813 - val_loss: 0.0563 - val_accuracy: 0.9773\n",
      "Epoch 798/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9813 - val_loss: 0.0618 - val_accuracy: 0.9767\n",
      "Epoch 799/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9827 - val_loss: 0.0645 - val_accuracy: 0.9768\n",
      "Epoch 800/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 0.0541 - val_accuracy: 0.9764\n",
      "Epoch 801/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9814 - val_loss: 0.0663 - val_accuracy: 0.9765\n",
      "Epoch 802/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9819 - val_loss: 0.0538 - val_accuracy: 0.9784\n",
      "Epoch 803/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9820 - val_loss: 0.0634 - val_accuracy: 0.9768\n",
      "Epoch 804/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.0592 - val_accuracy: 0.9770\n",
      "Epoch 805/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9817 - val_loss: 0.0588 - val_accuracy: 0.9771\n",
      "Epoch 806/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0535 - val_accuracy: 0.9776\n",
      "Epoch 807/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9817 - val_loss: 0.0555 - val_accuracy: 0.9780\n",
      "Epoch 808/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9814 - val_loss: 0.0532 - val_accuracy: 0.9768\n",
      "Epoch 809/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9816 - val_loss: 0.0557 - val_accuracy: 0.9776\n",
      "Epoch 810/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9813 - val_loss: 0.0593 - val_accuracy: 0.9768\n",
      "Epoch 811/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9825 - val_loss: 0.0561 - val_accuracy: 0.9776\n",
      "Epoch 812/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9801 - val_loss: 0.0549 - val_accuracy: 0.9764\n",
      "Epoch 813/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9812 - val_loss: 0.0573 - val_accuracy: 0.9773\n",
      "Epoch 814/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.0516 - val_accuracy: 0.9780\n",
      "Epoch 815/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9813 - val_loss: 0.0633 - val_accuracy: 0.9766\n",
      "Epoch 816/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9810 - val_loss: 0.0610 - val_accuracy: 0.9768\n",
      "Epoch 817/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9811 - val_loss: 0.0553 - val_accuracy: 0.9776\n",
      "Epoch 818/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9825 - val_loss: 0.0525 - val_accuracy: 0.9778\n",
      "Epoch 819/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9817 - val_loss: 0.0552 - val_accuracy: 0.9781\n",
      "Epoch 820/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9828 - val_loss: 0.0544 - val_accuracy: 0.9763\n",
      "Epoch 821/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9817 - val_loss: 0.0695 - val_accuracy: 0.9760\n",
      "Epoch 822/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 0.0660 - val_accuracy: 0.9765\n",
      "Epoch 823/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9822 - val_loss: 0.0558 - val_accuracy: 0.9769\n",
      "Epoch 824/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9824 - val_loss: 0.0561 - val_accuracy: 0.9768\n",
      "Epoch 825/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9813 - val_loss: 0.0589 - val_accuracy: 0.9777\n",
      "Epoch 826/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9810 - val_loss: 0.0553 - val_accuracy: 0.9773\n",
      "Epoch 827/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9817 - val_loss: 0.0589 - val_accuracy: 0.9776\n",
      "Epoch 828/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9807 - val_loss: 0.0587 - val_accuracy: 0.9782\n",
      "Epoch 829/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9823 - val_loss: 0.0522 - val_accuracy: 0.9788\n",
      "Epoch 830/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.0546 - val_accuracy: 0.9774\n",
      "Epoch 831/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9813 - val_loss: 0.0603 - val_accuracy: 0.9765\n",
      "Epoch 832/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9834 - val_loss: 0.0559 - val_accuracy: 0.9778\n",
      "Epoch 833/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0545 - val_accuracy: 0.9781\n",
      "Epoch 834/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9807 - val_loss: 0.0607 - val_accuracy: 0.9764\n",
      "Epoch 835/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.0654 - val_accuracy: 0.9768\n",
      "Epoch 836/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9820 - val_loss: 0.0600 - val_accuracy: 0.9774\n",
      "Epoch 837/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9821 - val_loss: 0.0517 - val_accuracy: 0.9790\n",
      "Epoch 838/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9814 - val_loss: 0.0554 - val_accuracy: 0.9777\n",
      "Epoch 839/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9809 - val_loss: 0.0546 - val_accuracy: 0.9778\n",
      "Epoch 840/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0613 - val_accuracy: 0.9773\n",
      "Epoch 841/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9827 - val_loss: 0.0510 - val_accuracy: 0.9778\n",
      "Epoch 842/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9814 - val_loss: 0.0536 - val_accuracy: 0.9777\n",
      "Epoch 843/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9811 - val_loss: 0.0620 - val_accuracy: 0.9765\n",
      "Epoch 844/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9814 - val_loss: 0.0622 - val_accuracy: 0.9771\n",
      "Epoch 845/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9812 - val_loss: 0.0494 - val_accuracy: 0.9780\n",
      "Epoch 846/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.0625 - val_accuracy: 0.9764\n",
      "Epoch 847/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9811 - val_loss: 0.0593 - val_accuracy: 0.9771\n",
      "Epoch 848/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9812 - val_loss: 0.0543 - val_accuracy: 0.9774\n",
      "Epoch 849/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9812 - val_loss: 0.0570 - val_accuracy: 0.9778\n",
      "Epoch 850/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9811 - val_loss: 0.0608 - val_accuracy: 0.9767\n",
      "Epoch 851/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.0505 - val_accuracy: 0.9777\n",
      "Epoch 852/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9815 - val_loss: 0.0590 - val_accuracy: 0.9779\n",
      "Epoch 853/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0543 - val_accuracy: 0.9779\n",
      "Epoch 854/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.0507 - val_accuracy: 0.9785\n",
      "Epoch 855/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9810 - val_loss: 0.0534 - val_accuracy: 0.9766\n",
      "Epoch 856/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0508 - accuracy: 0.9809 - val_loss: 0.0590 - val_accuracy: 0.9759\n",
      "Epoch 857/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9813 - val_loss: 0.0548 - val_accuracy: 0.9781\n",
      "Epoch 858/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9823 - val_loss: 0.0527 - val_accuracy: 0.9761\n",
      "Epoch 859/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9814 - val_loss: 0.0570 - val_accuracy: 0.9776\n",
      "Epoch 860/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0544 - val_accuracy: 0.9777\n",
      "Epoch 861/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9817 - val_loss: 0.0631 - val_accuracy: 0.9766\n",
      "Epoch 862/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.0581 - val_accuracy: 0.9785\n",
      "Epoch 863/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0474 - accuracy: 0.9825 - val_loss: 0.0611 - val_accuracy: 0.9768\n",
      "Epoch 864/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0474 - accuracy: 0.9829 - val_loss: 0.0567 - val_accuracy: 0.9771\n",
      "Epoch 865/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9818 - val_loss: 0.0530 - val_accuracy: 0.9788\n",
      "Epoch 866/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9821 - val_loss: 0.0683 - val_accuracy: 0.9761\n",
      "Epoch 867/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9815 - val_loss: 0.0527 - val_accuracy: 0.9777\n",
      "Epoch 868/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0491 - accuracy: 0.9820 - val_loss: 0.0517 - val_accuracy: 0.9777\n",
      "Epoch 869/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 0.0576 - val_accuracy: 0.9787\n",
      "Epoch 870/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9810 - val_loss: 0.0616 - val_accuracy: 0.9776\n",
      "Epoch 871/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9808 - val_loss: 0.0664 - val_accuracy: 0.9751\n",
      "Epoch 872/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0491 - accuracy: 0.9824 - val_loss: 0.0557 - val_accuracy: 0.9776\n",
      "Epoch 873/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 0.0606 - val_accuracy: 0.9773\n",
      "Epoch 874/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0537 - val_accuracy: 0.9778\n",
      "Epoch 875/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0602 - val_accuracy: 0.9768\n",
      "Epoch 876/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0509 - accuracy: 0.9814 - val_loss: 0.0602 - val_accuracy: 0.9775\n",
      "Epoch 877/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0503 - accuracy: 0.9811 - val_loss: 0.0578 - val_accuracy: 0.9777\n",
      "Epoch 878/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0503 - accuracy: 0.9819 - val_loss: 0.0531 - val_accuracy: 0.9782\n",
      "Epoch 879/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0498 - accuracy: 0.9810 - val_loss: 0.0626 - val_accuracy: 0.9767\n",
      "Epoch 880/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0488 - accuracy: 0.9824 - val_loss: 0.0579 - val_accuracy: 0.9787\n",
      "Epoch 881/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0483 - accuracy: 0.9819 - val_loss: 0.0580 - val_accuracy: 0.9778\n",
      "Epoch 882/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9817 - val_loss: 0.0583 - val_accuracy: 0.9774\n",
      "Epoch 883/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.0653 - val_accuracy: 0.9763\n",
      "Epoch 884/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9807 - val_loss: 0.0497 - val_accuracy: 0.9784\n",
      "Epoch 885/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 0.0522 - val_accuracy: 0.9775\n",
      "Epoch 886/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0595 - val_accuracy: 0.9769\n",
      "Epoch 887/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0625 - val_accuracy: 0.9768\n",
      "Epoch 888/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9812 - val_loss: 0.0542 - val_accuracy: 0.9774\n",
      "Epoch 889/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9824 - val_loss: 0.0630 - val_accuracy: 0.9764\n",
      "Epoch 890/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9812 - val_loss: 0.0541 - val_accuracy: 0.9786\n",
      "Epoch 891/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9823 - val_loss: 0.0542 - val_accuracy: 0.9786\n",
      "Epoch 892/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.0539 - val_accuracy: 0.9777\n",
      "Epoch 893/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0527 - val_accuracy: 0.9773\n",
      "Epoch 894/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.0530 - val_accuracy: 0.9776\n",
      "Epoch 895/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9814 - val_loss: 0.0597 - val_accuracy: 0.9768\n",
      "Epoch 896/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9831 - val_loss: 0.0589 - val_accuracy: 0.9770\n",
      "Epoch 897/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9816 - val_loss: 0.0629 - val_accuracy: 0.9770\n",
      "Epoch 898/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9824 - val_loss: 0.0574 - val_accuracy: 0.9784\n",
      "Epoch 899/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9826 - val_loss: 0.0690 - val_accuracy: 0.9749\n",
      "Epoch 900/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9819 - val_loss: 0.0569 - val_accuracy: 0.9776\n",
      "Epoch 901/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0475 - accuracy: 0.9826 - val_loss: 0.0660 - val_accuracy: 0.9767\n",
      "Epoch 902/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9814 - val_loss: 0.0618 - val_accuracy: 0.9767\n",
      "Epoch 903/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9813 - val_loss: 0.0568 - val_accuracy: 0.9778\n",
      "Epoch 904/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9818 - val_loss: 0.0558 - val_accuracy: 0.9778\n",
      "Epoch 905/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9824 - val_loss: 0.0562 - val_accuracy: 0.9778\n",
      "Epoch 906/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9822 - val_loss: 0.0634 - val_accuracy: 0.9769\n",
      "Epoch 907/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.0696 - val_accuracy: 0.9766\n",
      "Epoch 908/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9826 - val_loss: 0.0596 - val_accuracy: 0.9776\n",
      "Epoch 909/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9818 - val_loss: 0.0530 - val_accuracy: 0.9788\n",
      "Epoch 910/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9826 - val_loss: 0.0607 - val_accuracy: 0.9760\n",
      "Epoch 911/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9815 - val_loss: 0.0562 - val_accuracy: 0.9775\n",
      "Epoch 912/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9821 - val_loss: 0.0522 - val_accuracy: 0.9780\n",
      "Epoch 913/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 0.0573 - val_accuracy: 0.9773\n",
      "Epoch 914/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9825 - val_loss: 0.0557 - val_accuracy: 0.9770\n",
      "Epoch 915/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9818 - val_loss: 0.0543 - val_accuracy: 0.9785\n",
      "Epoch 916/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 0.0570 - val_accuracy: 0.9786\n",
      "Epoch 917/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9828 - val_loss: 0.0536 - val_accuracy: 0.9771\n",
      "Epoch 918/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0546 - val_accuracy: 0.9769\n",
      "Epoch 919/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9815 - val_loss: 0.0580 - val_accuracy: 0.9779\n",
      "Epoch 920/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 0.0571 - val_accuracy: 0.9773\n",
      "Epoch 921/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9825 - val_loss: 0.0560 - val_accuracy: 0.9781\n",
      "Epoch 922/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9822 - val_loss: 0.0603 - val_accuracy: 0.9767\n",
      "Epoch 923/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9825 - val_loss: 0.0556 - val_accuracy: 0.9779\n",
      "Epoch 924/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9811 - val_loss: 0.0584 - val_accuracy: 0.9775\n",
      "Epoch 925/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9826 - val_loss: 0.0620 - val_accuracy: 0.9770\n",
      "Epoch 926/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.0556 - val_accuracy: 0.9774\n",
      "Epoch 927/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9816 - val_loss: 0.0583 - val_accuracy: 0.9771\n",
      "Epoch 928/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0550 - val_accuracy: 0.9767\n",
      "Epoch 929/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9825 - val_loss: 0.0558 - val_accuracy: 0.9777\n",
      "Epoch 930/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 0.0610 - val_accuracy: 0.9767\n",
      "Epoch 931/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9822 - val_loss: 0.0601 - val_accuracy: 0.9756\n",
      "Epoch 932/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9824 - val_loss: 0.0523 - val_accuracy: 0.9781\n",
      "Epoch 933/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9816 - val_loss: 0.0591 - val_accuracy: 0.9770\n",
      "Epoch 934/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9827 - val_loss: 0.0632 - val_accuracy: 0.9759\n",
      "Epoch 935/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.0582 - val_accuracy: 0.9776\n",
      "Epoch 936/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9820 - val_loss: 0.0496 - val_accuracy: 0.9784\n",
      "Epoch 937/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9810 - val_loss: 0.0576 - val_accuracy: 0.9769\n",
      "Epoch 938/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9821 - val_loss: 0.0566 - val_accuracy: 0.9778\n",
      "Epoch 939/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9825 - val_loss: 0.0530 - val_accuracy: 0.9779\n",
      "Epoch 940/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9828 - val_loss: 0.0656 - val_accuracy: 0.9759\n",
      "Epoch 941/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0687 - val_accuracy: 0.9760\n",
      "Epoch 942/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9822 - val_loss: 0.0670 - val_accuracy: 0.9756\n",
      "Epoch 943/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9822 - val_loss: 0.0670 - val_accuracy: 0.9760\n",
      "Epoch 944/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9817 - val_loss: 0.0506 - val_accuracy: 0.9778\n",
      "Epoch 945/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9811 - val_loss: 0.0645 - val_accuracy: 0.9753\n",
      "Epoch 946/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9822 - val_loss: 0.0565 - val_accuracy: 0.9775\n",
      "Epoch 947/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9825 - val_loss: 0.0543 - val_accuracy: 0.9777\n",
      "Epoch 948/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9823 - val_loss: 0.0512 - val_accuracy: 0.9776\n",
      "Epoch 949/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9828 - val_loss: 0.0595 - val_accuracy: 0.9778\n",
      "Epoch 950/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9808 - val_loss: 0.0586 - val_accuracy: 0.9780\n",
      "Epoch 951/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.0583 - val_accuracy: 0.9771\n",
      "Epoch 952/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9818 - val_loss: 0.0562 - val_accuracy: 0.9767\n",
      "Epoch 953/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.9832 - val_loss: 0.0569 - val_accuracy: 0.9773\n",
      "Epoch 954/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9825 - val_loss: 0.0624 - val_accuracy: 0.9773\n",
      "Epoch 955/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9828 - val_loss: 0.0529 - val_accuracy: 0.9775\n",
      "Epoch 956/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0567 - val_accuracy: 0.9771\n",
      "Epoch 957/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.0638 - val_accuracy: 0.9768\n",
      "Epoch 958/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9816 - val_loss: 0.0578 - val_accuracy: 0.9764\n",
      "Epoch 959/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9828 - val_loss: 0.0652 - val_accuracy: 0.9761\n",
      "Epoch 960/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9826 - val_loss: 0.0561 - val_accuracy: 0.9779\n",
      "Epoch 961/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9822 - val_loss: 0.0585 - val_accuracy: 0.9780\n",
      "Epoch 962/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9822 - val_loss: 0.0496 - val_accuracy: 0.9787\n",
      "Epoch 963/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0467 - accuracy: 0.9826 - val_loss: 0.0549 - val_accuracy: 0.9776\n",
      "Epoch 964/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9824 - val_loss: 0.0552 - val_accuracy: 0.9777\n",
      "Epoch 965/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9822 - val_loss: 0.0548 - val_accuracy: 0.9771\n",
      "Epoch 966/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9824 - val_loss: 0.0583 - val_accuracy: 0.9771\n",
      "Epoch 967/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9824 - val_loss: 0.0579 - val_accuracy: 0.9773\n",
      "Epoch 968/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0532 - val_accuracy: 0.9778\n",
      "Epoch 969/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9822 - val_loss: 0.0588 - val_accuracy: 0.9763\n",
      "Epoch 970/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9822 - val_loss: 0.0596 - val_accuracy: 0.9774\n",
      "Epoch 971/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9815 - val_loss: 0.0631 - val_accuracy: 0.9765\n",
      "Epoch 972/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9814 - val_loss: 0.0596 - val_accuracy: 0.9781\n",
      "Epoch 973/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: 0.0498 - val_accuracy: 0.9791\n",
      "Epoch 974/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9817 - val_loss: 0.0538 - val_accuracy: 0.9768\n",
      "Epoch 975/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9809 - val_loss: 0.0537 - val_accuracy: 0.9779\n",
      "Epoch 976/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9822 - val_loss: 0.0703 - val_accuracy: 0.9753\n",
      "Epoch 977/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9814 - val_loss: 0.0596 - val_accuracy: 0.9768\n",
      "Epoch 978/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9824 - val_loss: 0.0542 - val_accuracy: 0.9777\n",
      "Epoch 979/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0702 - val_accuracy: 0.9761\n",
      "Epoch 980/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9825 - val_loss: 0.0563 - val_accuracy: 0.9769\n",
      "Epoch 981/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9818 - val_loss: 0.0522 - val_accuracy: 0.9788\n",
      "Epoch 982/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9824 - val_loss: 0.0551 - val_accuracy: 0.9771\n",
      "Epoch 983/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9825 - val_loss: 0.0527 - val_accuracy: 0.9780\n",
      "Epoch 984/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9824 - val_loss: 0.0553 - val_accuracy: 0.9771\n",
      "Epoch 985/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9824 - val_loss: 0.0689 - val_accuracy: 0.9755\n",
      "Epoch 986/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9826 - val_loss: 0.0558 - val_accuracy: 0.9778\n",
      "Epoch 987/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9821 - val_loss: 0.0523 - val_accuracy: 0.9782\n",
      "Epoch 988/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9826 - val_loss: 0.0551 - val_accuracy: 0.9770\n",
      "Epoch 989/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9827 - val_loss: 0.0590 - val_accuracy: 0.9765\n",
      "Epoch 990/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9819 - val_loss: 0.0607 - val_accuracy: 0.9775\n",
      "Epoch 991/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 0.0580 - val_accuracy: 0.9776\n",
      "Epoch 992/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9827 - val_loss: 0.0576 - val_accuracy: 0.9777\n",
      "Epoch 993/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.9830 - val_loss: 0.0504 - val_accuracy: 0.9770\n",
      "Epoch 994/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9825 - val_loss: 0.0569 - val_accuracy: 0.9778\n",
      "Epoch 995/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9818 - val_loss: 0.0570 - val_accuracy: 0.9765\n",
      "Epoch 996/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9825 - val_loss: 0.0535 - val_accuracy: 0.9781\n",
      "Epoch 997/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9824 - val_loss: 0.0571 - val_accuracy: 0.9775\n",
      "Epoch 998/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9821 - val_loss: 0.0596 - val_accuracy: 0.9759\n",
      "Epoch 999/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9819 - val_loss: 0.0522 - val_accuracy: 0.9775\n",
      "Epoch 1000/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9815 - val_loss: 0.0608 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 940us/step\n",
      "xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,)\n",
      "\n",
      "classification_report_Fold=1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       1.00      0.98      0.99      8725\n",
      " Anomalous 1       0.59      0.87      0.71       287\n",
      "\n",
      "    accuracy                           0.98      9012\n",
      "   macro avg       0.79      0.93      0.85      9012\n",
      "weighted avg       0.98      0.98      0.98      9012\n",
      "\n",
      "confusion_matrix_Fold=1:\n",
      "\n",
      "True Negatives:  8553\n",
      "False Positives:  172\n",
      "False Negatives:  36\n",
      "True Positives:  251\n",
      "accuracy_score_Fold=1:\n",
      " 8804 \n",
      "\n",
      "End running time Fold=1: 210214_092303 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=2: 210214_092303 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 1.110064921240261e-05, min: 3.897988961917731e-06, max: 0.0002312187141438627\n",
      "End running time Data Augmentation_Fold=2: 210214_094233 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=2: 210214_094233 ,-------------------------- \n",
      "\n",
      "\n",
      " Number of Final yXtrain_Fold=2 labeled 0: 69796\n",
      "Number of Final yXtrain_Fold=2 labeled 1: 16072\n",
      "Number of Final yXtrain_Fold=2 labeled 2: 0 \n",
      "\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.4919 - accuracy: 0.7971 - val_loss: 0.2338 - val_accuracy: 0.9665\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8210 - val_loss: 0.2295 - val_accuracy: 0.9627\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.8232 - val_loss: 0.2419 - val_accuracy: 0.9544\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4181 - accuracy: 0.8252 - val_loss: 0.2117 - val_accuracy: 0.9558\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4100 - accuracy: 0.8273 - val_loss: 0.2176 - val_accuracy: 0.9553\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8266 - val_loss: 0.2194 - val_accuracy: 0.9534\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8293 - val_loss: 0.2060 - val_accuracy: 0.9551\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8323 - val_loss: 0.2041 - val_accuracy: 0.9523\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8335 - val_loss: 0.2275 - val_accuracy: 0.9442\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8335 - val_loss: 0.2457 - val_accuracy: 0.9444\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8345 - val_loss: 0.2024 - val_accuracy: 0.9534\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8334 - val_loss: 0.2116 - val_accuracy: 0.9481\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8359 - val_loss: 0.2213 - val_accuracy: 0.9455\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8367 - val_loss: 0.2235 - val_accuracy: 0.9426\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8399 - val_loss: 0.2112 - val_accuracy: 0.9452\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8386 - val_loss: 0.2135 - val_accuracy: 0.9445\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3576 - accuracy: 0.8417 - val_loss: 0.1870 - val_accuracy: 0.9512\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3543 - accuracy: 0.8415 - val_loss: 0.1953 - val_accuracy: 0.9481\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3519 - accuracy: 0.8420 - val_loss: 0.1792 - val_accuracy: 0.9534\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8454 - val_loss: 0.2110 - val_accuracy: 0.9396\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3403 - accuracy: 0.8476 - val_loss: 0.1772 - val_accuracy: 0.9528\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3354 - accuracy: 0.8480 - val_loss: 0.1790 - val_accuracy: 0.9509\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3311 - accuracy: 0.8486 - val_loss: 0.1943 - val_accuracy: 0.9440\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3292 - accuracy: 0.8507 - val_loss: 0.1974 - val_accuracy: 0.9389\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3241 - accuracy: 0.8519 - val_loss: 0.2061 - val_accuracy: 0.9367\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3221 - accuracy: 0.8535 - val_loss: 0.1841 - val_accuracy: 0.9454\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3161 - accuracy: 0.8536 - val_loss: 0.2130 - val_accuracy: 0.9296\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3113 - accuracy: 0.8580 - val_loss: 0.1900 - val_accuracy: 0.9400\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3089 - accuracy: 0.8581 - val_loss: 0.1767 - val_accuracy: 0.9464\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3044 - accuracy: 0.8606 - val_loss: 0.1942 - val_accuracy: 0.9357\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.3022 - accuracy: 0.8612 - val_loss: 0.1827 - val_accuracy: 0.9395\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2955 - accuracy: 0.8649 - val_loss: 0.1742 - val_accuracy: 0.9446\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2903 - accuracy: 0.8675 - val_loss: 0.1908 - val_accuracy: 0.9354\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2878 - accuracy: 0.8676 - val_loss: 0.1888 - val_accuracy: 0.9344\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2840 - accuracy: 0.8713 - val_loss: 0.1597 - val_accuracy: 0.9484\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8706 - val_loss: 0.1661 - val_accuracy: 0.9454\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8743 - val_loss: 0.1519 - val_accuracy: 0.9495\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8731 - val_loss: 0.1963 - val_accuracy: 0.9241\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8764 - val_loss: 0.1729 - val_accuracy: 0.9384\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8791 - val_loss: 0.1760 - val_accuracy: 0.9349\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8820 - val_loss: 0.1568 - val_accuracy: 0.9475\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8829 - val_loss: 0.1898 - val_accuracy: 0.9274\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8831 - val_loss: 0.1547 - val_accuracy: 0.9473\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8843 - val_loss: 0.1834 - val_accuracy: 0.9286\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2515 - accuracy: 0.8879 - val_loss: 0.1489 - val_accuracy: 0.9496\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8881 - val_loss: 0.1578 - val_accuracy: 0.9425\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8915 - val_loss: 0.1612 - val_accuracy: 0.9387\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2424 - accuracy: 0.8933 - val_loss: 0.1869 - val_accuracy: 0.9213\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8965 - val_loss: 0.1395 - val_accuracy: 0.9512\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.8966 - val_loss: 0.1360 - val_accuracy: 0.9527\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9006 - val_loss: 0.1572 - val_accuracy: 0.9401\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8999 - val_loss: 0.1580 - val_accuracy: 0.9383\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.9009 - val_loss: 0.1467 - val_accuracy: 0.9440\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9025 - val_loss: 0.1548 - val_accuracy: 0.9416\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9057 - val_loss: 0.1730 - val_accuracy: 0.9260\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9054 - val_loss: 0.1345 - val_accuracy: 0.9504\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9058 - val_loss: 0.1322 - val_accuracy: 0.9516\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9103 - val_loss: 0.1380 - val_accuracy: 0.9466\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9069 - val_loss: 0.1329 - val_accuracy: 0.9509\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9133 - val_loss: 0.1369 - val_accuracy: 0.9484\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9116 - val_loss: 0.1498 - val_accuracy: 0.9387\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9146 - val_loss: 0.1402 - val_accuracy: 0.9444\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9120 - val_loss: 0.1360 - val_accuracy: 0.9450\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9134 - val_loss: 0.1579 - val_accuracy: 0.9329\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9159 - val_loss: 0.1260 - val_accuracy: 0.9516\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9170 - val_loss: 0.1278 - val_accuracy: 0.9498\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9165 - val_loss: 0.1229 - val_accuracy: 0.9497\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1940 - accuracy: 0.9173 - val_loss: 0.1562 - val_accuracy: 0.9350\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9171 - val_loss: 0.1382 - val_accuracy: 0.9436\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9197 - val_loss: 0.1389 - val_accuracy: 0.9418\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9201 - val_loss: 0.1249 - val_accuracy: 0.9487\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9203 - val_loss: 0.1462 - val_accuracy: 0.9397\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9219 - val_loss: 0.1166 - val_accuracy: 0.9528\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9228 - val_loss: 0.1350 - val_accuracy: 0.9448\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9238 - val_loss: 0.1151 - val_accuracy: 0.9525\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9253 - val_loss: 0.1041 - val_accuracy: 0.9574\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9246 - val_loss: 0.1240 - val_accuracy: 0.9471\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.9247 - val_loss: 0.1199 - val_accuracy: 0.9492\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9272 - val_loss: 0.1072 - val_accuracy: 0.9557\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.9275 - val_loss: 0.1405 - val_accuracy: 0.9426\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9282 - val_loss: 0.1095 - val_accuracy: 0.9542\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9286 - val_loss: 0.1219 - val_accuracy: 0.9476\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9308 - val_loss: 0.0997 - val_accuracy: 0.9595\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9292 - val_loss: 0.0977 - val_accuracy: 0.9592\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.9285 - val_loss: 0.1062 - val_accuracy: 0.9547\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9317 - val_loss: 0.1079 - val_accuracy: 0.9538\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9321 - val_loss: 0.1201 - val_accuracy: 0.9475\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9350 - val_loss: 0.0994 - val_accuracy: 0.9564\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9336 - val_loss: 0.1296 - val_accuracy: 0.9447\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1601 - accuracy: 0.9333 - val_loss: 0.1059 - val_accuracy: 0.9562\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9352 - val_loss: 0.0999 - val_accuracy: 0.9559\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9365 - val_loss: 0.1205 - val_accuracy: 0.9493\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1541 - accuracy: 0.9378 - val_loss: 0.1158 - val_accuracy: 0.9508\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9360 - val_loss: 0.1180 - val_accuracy: 0.9486\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9373 - val_loss: 0.0954 - val_accuracy: 0.9607\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1538 - accuracy: 0.9359 - val_loss: 0.1238 - val_accuracy: 0.9460\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9393 - val_loss: 0.1092 - val_accuracy: 0.9534\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9397 - val_loss: 0.1094 - val_accuracy: 0.9527\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9398 - val_loss: 0.1067 - val_accuracy: 0.9525\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9402 - val_loss: 0.1122 - val_accuracy: 0.9519\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.9422 - val_loss: 0.1304 - val_accuracy: 0.9447\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9412 - val_loss: 0.0816 - val_accuracy: 0.9652\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1445 - accuracy: 0.9415 - val_loss: 0.1094 - val_accuracy: 0.9524\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.9416 - val_loss: 0.1053 - val_accuracy: 0.9542\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9437 - val_loss: 0.0940 - val_accuracy: 0.9594\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1417 - accuracy: 0.9423 - val_loss: 0.1098 - val_accuracy: 0.9521\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9438 - val_loss: 0.1109 - val_accuracy: 0.9516\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.9464 - val_loss: 0.1004 - val_accuracy: 0.9546\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9447 - val_loss: 0.0966 - val_accuracy: 0.9565\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9458 - val_loss: 0.0964 - val_accuracy: 0.9572\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9448 - val_loss: 0.0827 - val_accuracy: 0.9640\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9454 - val_loss: 0.0944 - val_accuracy: 0.9585\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9468 - val_loss: 0.1020 - val_accuracy: 0.9556\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9472 - val_loss: 0.0996 - val_accuracy: 0.9583\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1346 - accuracy: 0.9459 - val_loss: 0.0899 - val_accuracy: 0.9603\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9475 - val_loss: 0.1280 - val_accuracy: 0.9458\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1322 - accuracy: 0.9474 - val_loss: 0.0944 - val_accuracy: 0.9593\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9480 - val_loss: 0.0919 - val_accuracy: 0.9592\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1297 - accuracy: 0.9489 - val_loss: 0.0935 - val_accuracy: 0.9574\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9477 - val_loss: 0.1073 - val_accuracy: 0.9525\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.9497 - val_loss: 0.1036 - val_accuracy: 0.9541\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9498 - val_loss: 0.0797 - val_accuracy: 0.9634\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9493 - val_loss: 0.0835 - val_accuracy: 0.9635\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.9501 - val_loss: 0.1064 - val_accuracy: 0.9534\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1254 - accuracy: 0.9512 - val_loss: 0.1036 - val_accuracy: 0.9536\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1254 - accuracy: 0.9512 - val_loss: 0.0916 - val_accuracy: 0.9594\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1248 - accuracy: 0.9512 - val_loss: 0.1067 - val_accuracy: 0.9544\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9503 - val_loss: 0.0913 - val_accuracy: 0.9595\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9521 - val_loss: 0.1068 - val_accuracy: 0.9535\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9523 - val_loss: 0.0975 - val_accuracy: 0.9558\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1218 - accuracy: 0.9522 - val_loss: 0.0815 - val_accuracy: 0.9633\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9539 - val_loss: 0.0873 - val_accuracy: 0.9599\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1162 - accuracy: 0.9550 - val_loss: 0.1113 - val_accuracy: 0.9527\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9523 - val_loss: 0.0869 - val_accuracy: 0.9597\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.9537 - val_loss: 0.0921 - val_accuracy: 0.9592\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9525 - val_loss: 0.0900 - val_accuracy: 0.9592\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1195 - accuracy: 0.9537 - val_loss: 0.0956 - val_accuracy: 0.9575\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9549 - val_loss: 0.0849 - val_accuracy: 0.9609\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9554 - val_loss: 0.0982 - val_accuracy: 0.9581\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9555 - val_loss: 0.0939 - val_accuracy: 0.9587\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9534 - val_loss: 0.0958 - val_accuracy: 0.9577\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.9557 - val_loss: 0.0736 - val_accuracy: 0.9673\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9545 - val_loss: 0.1029 - val_accuracy: 0.9544\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9550 - val_loss: 0.0772 - val_accuracy: 0.9647\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9562 - val_loss: 0.0955 - val_accuracy: 0.9576\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9547 - val_loss: 0.1073 - val_accuracy: 0.9528\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9567 - val_loss: 0.0832 - val_accuracy: 0.9634\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.9568 - val_loss: 0.0905 - val_accuracy: 0.9608\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1124 - accuracy: 0.9558 - val_loss: 0.0850 - val_accuracy: 0.9617\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1098 - accuracy: 0.9581 - val_loss: 0.0745 - val_accuracy: 0.9668\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9579 - val_loss: 0.1071 - val_accuracy: 0.9549\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1102 - accuracy: 0.9568 - val_loss: 0.0784 - val_accuracy: 0.9632\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.9577 - val_loss: 0.0802 - val_accuracy: 0.9638\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9572 - val_loss: 0.1047 - val_accuracy: 0.9548\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9565 - val_loss: 0.1146 - val_accuracy: 0.9521\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9587 - val_loss: 0.0788 - val_accuracy: 0.9659\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1084 - accuracy: 0.9574 - val_loss: 0.0933 - val_accuracy: 0.9578\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9597 - val_loss: 0.0690 - val_accuracy: 0.9716\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.9598 - val_loss: 0.0830 - val_accuracy: 0.9637\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1059 - accuracy: 0.9599 - val_loss: 0.0709 - val_accuracy: 0.9690\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.9578 - val_loss: 0.0866 - val_accuracy: 0.9615\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9601 - val_loss: 0.0981 - val_accuracy: 0.9587\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9607 - val_loss: 0.0780 - val_accuracy: 0.9655\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9591 - val_loss: 0.0804 - val_accuracy: 0.9658\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9589 - val_loss: 0.0917 - val_accuracy: 0.9612\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9609 - val_loss: 0.0875 - val_accuracy: 0.9622\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1019 - accuracy: 0.9618 - val_loss: 0.1055 - val_accuracy: 0.9548\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9587 - val_loss: 0.0918 - val_accuracy: 0.9603\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.9615 - val_loss: 0.0985 - val_accuracy: 0.9578\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.9621 - val_loss: 0.0921 - val_accuracy: 0.9609\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9608 - val_loss: 0.0837 - val_accuracy: 0.9623\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9611 - val_loss: 0.0717 - val_accuracy: 0.9688\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9617 - val_loss: 0.0758 - val_accuracy: 0.9676\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9615 - val_loss: 0.0909 - val_accuracy: 0.9597\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9617 - val_loss: 0.1032 - val_accuracy: 0.9553\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9608 - val_loss: 0.0964 - val_accuracy: 0.9604\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9629 - val_loss: 0.0864 - val_accuracy: 0.9642\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0993 - accuracy: 0.9635 - val_loss: 0.0821 - val_accuracy: 0.9627\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9630 - val_loss: 0.0826 - val_accuracy: 0.9648\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9627 - val_loss: 0.0859 - val_accuracy: 0.9620\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9621 - val_loss: 0.0784 - val_accuracy: 0.9673\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9621 - val_loss: 0.0659 - val_accuracy: 0.9719\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9629 - val_loss: 0.0679 - val_accuracy: 0.9710\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9637 - val_loss: 0.0715 - val_accuracy: 0.9693\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0959 - accuracy: 0.9645 - val_loss: 0.0770 - val_accuracy: 0.9665\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9638 - val_loss: 0.0786 - val_accuracy: 0.9663\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9635 - val_loss: 0.0779 - val_accuracy: 0.9657\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9648 - val_loss: 0.0859 - val_accuracy: 0.9634\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9635 - val_loss: 0.0761 - val_accuracy: 0.9670\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.9639 - val_loss: 0.0858 - val_accuracy: 0.9629\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.9652 - val_loss: 0.0870 - val_accuracy: 0.9634\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9650 - val_loss: 0.0837 - val_accuracy: 0.9650\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9628 - val_loss: 0.0833 - val_accuracy: 0.9650\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0939 - accuracy: 0.9646 - val_loss: 0.0730 - val_accuracy: 0.9677\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9654 - val_loss: 0.0803 - val_accuracy: 0.9665\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9647 - val_loss: 0.0820 - val_accuracy: 0.9663\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.9648 - val_loss: 0.0753 - val_accuracy: 0.9674\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0941 - accuracy: 0.9644 - val_loss: 0.1114 - val_accuracy: 0.9552\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9657 - val_loss: 0.0889 - val_accuracy: 0.9629\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9660 - val_loss: 0.0778 - val_accuracy: 0.9669\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9647 - val_loss: 0.0731 - val_accuracy: 0.9687\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9663 - val_loss: 0.0789 - val_accuracy: 0.9665\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9658 - val_loss: 0.0712 - val_accuracy: 0.9703\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9673 - val_loss: 0.0689 - val_accuracy: 0.9710\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9665 - val_loss: 0.0956 - val_accuracy: 0.9605\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.9636 - val_loss: 0.0853 - val_accuracy: 0.9636\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9666 - val_loss: 0.0664 - val_accuracy: 0.9720\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9663 - val_loss: 0.0696 - val_accuracy: 0.9710\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0890 - accuracy: 0.9681 - val_loss: 0.0823 - val_accuracy: 0.9653\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9688 - val_loss: 0.0714 - val_accuracy: 0.9686\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.9660 - val_loss: 0.0706 - val_accuracy: 0.9695\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9673 - val_loss: 0.0754 - val_accuracy: 0.9676\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9672 - val_loss: 0.0754 - val_accuracy: 0.9673\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9666 - val_loss: 0.0924 - val_accuracy: 0.9622\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9673 - val_loss: 0.0681 - val_accuracy: 0.9709\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.9682 - val_loss: 0.0741 - val_accuracy: 0.9693\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9677 - val_loss: 0.0664 - val_accuracy: 0.9721\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9675 - val_loss: 0.0750 - val_accuracy: 0.9691\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9681 - val_loss: 0.0836 - val_accuracy: 0.9656\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9690 - val_loss: 0.0727 - val_accuracy: 0.9686\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0871 - accuracy: 0.9685 - val_loss: 0.0668 - val_accuracy: 0.9730\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9677 - val_loss: 0.0738 - val_accuracy: 0.9699\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0890 - accuracy: 0.9671 - val_loss: 0.0706 - val_accuracy: 0.9698\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9680 - val_loss: 0.0770 - val_accuracy: 0.9690\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9680 - val_loss: 0.0778 - val_accuracy: 0.9675\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9693 - val_loss: 0.0761 - val_accuracy: 0.9689\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9688 - val_loss: 0.0902 - val_accuracy: 0.9628\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9688 - val_loss: 0.0856 - val_accuracy: 0.9647\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9690 - val_loss: 0.0762 - val_accuracy: 0.9683\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9696 - val_loss: 0.0787 - val_accuracy: 0.9664\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9681 - val_loss: 0.0815 - val_accuracy: 0.9665\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9683 - val_loss: 0.0699 - val_accuracy: 0.9701\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9681 - val_loss: 0.0690 - val_accuracy: 0.9713\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9702 - val_loss: 0.0764 - val_accuracy: 0.9688\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.9684 - val_loss: 0.0605 - val_accuracy: 0.9731\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9683 - val_loss: 0.0582 - val_accuracy: 0.9747\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9685 - val_loss: 0.1002 - val_accuracy: 0.9596\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9703 - val_loss: 0.0753 - val_accuracy: 0.9690\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9692 - val_loss: 0.0696 - val_accuracy: 0.9715\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9696 - val_loss: 0.0705 - val_accuracy: 0.9696\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9708 - val_loss: 0.0710 - val_accuracy: 0.9701\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9700 - val_loss: 0.0906 - val_accuracy: 0.9626\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9705 - val_loss: 0.0769 - val_accuracy: 0.9682\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9703 - val_loss: 0.0769 - val_accuracy: 0.9683\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9692 - val_loss: 0.0770 - val_accuracy: 0.9673\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9706 - val_loss: 0.0617 - val_accuracy: 0.9725\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9696 - val_loss: 0.0657 - val_accuracy: 0.9726\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9696 - val_loss: 0.0734 - val_accuracy: 0.9704\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9699 - val_loss: 0.0597 - val_accuracy: 0.9748\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0803 - accuracy: 0.9705 - val_loss: 0.0767 - val_accuracy: 0.9686\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9713 - val_loss: 0.0619 - val_accuracy: 0.9747\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9700 - val_loss: 0.0761 - val_accuracy: 0.9678\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9703 - val_loss: 0.0847 - val_accuracy: 0.9660\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9700 - val_loss: 0.0906 - val_accuracy: 0.9640\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9704 - val_loss: 0.0716 - val_accuracy: 0.9708\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9715 - val_loss: 0.0677 - val_accuracy: 0.9724\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9699 - val_loss: 0.0701 - val_accuracy: 0.9723\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9719 - val_loss: 0.0633 - val_accuracy: 0.9727\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9707 - val_loss: 0.0612 - val_accuracy: 0.9741\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9706 - val_loss: 0.0687 - val_accuracy: 0.9713\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9703 - val_loss: 0.0764 - val_accuracy: 0.9679\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9715 - val_loss: 0.0696 - val_accuracy: 0.9716\n",
      "Epoch 263/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9715 - val_loss: 0.0676 - val_accuracy: 0.9703\n",
      "Epoch 264/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9711 - val_loss: 0.0714 - val_accuracy: 0.9714\n",
      "Epoch 265/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9719 - val_loss: 0.0808 - val_accuracy: 0.9668\n",
      "Epoch 266/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9710 - val_loss: 0.0740 - val_accuracy: 0.9696\n",
      "Epoch 267/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9717 - val_loss: 0.0665 - val_accuracy: 0.9714\n",
      "Epoch 268/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9731 - val_loss: 0.0625 - val_accuracy: 0.9741\n",
      "Epoch 269/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.9712 - val_loss: 0.0651 - val_accuracy: 0.9730\n",
      "Epoch 270/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9723 - val_loss: 0.0617 - val_accuracy: 0.9733\n",
      "Epoch 271/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9728 - val_loss: 0.0801 - val_accuracy: 0.9683\n",
      "Epoch 272/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9717 - val_loss: 0.0649 - val_accuracy: 0.9731\n",
      "Epoch 273/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9729 - val_loss: 0.0664 - val_accuracy: 0.9723\n",
      "Epoch 274/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9723 - val_loss: 0.0734 - val_accuracy: 0.9710\n",
      "Epoch 275/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9723 - val_loss: 0.0672 - val_accuracy: 0.9733\n",
      "Epoch 276/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9709 - val_loss: 0.0776 - val_accuracy: 0.9680\n",
      "Epoch 277/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9716 - val_loss: 0.0565 - val_accuracy: 0.9764\n",
      "Epoch 278/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9713 - val_loss: 0.0648 - val_accuracy: 0.9727\n",
      "Epoch 279/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9721 - val_loss: 0.0709 - val_accuracy: 0.9714\n",
      "Epoch 280/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.9732 - val_loss: 0.0898 - val_accuracy: 0.9648\n",
      "Epoch 281/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9714 - val_loss: 0.0651 - val_accuracy: 0.9733\n",
      "Epoch 282/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9730 - val_loss: 0.0783 - val_accuracy: 0.9688\n",
      "Epoch 283/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9736 - val_loss: 0.0704 - val_accuracy: 0.9726\n",
      "Epoch 284/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9731 - val_loss: 0.0779 - val_accuracy: 0.9683\n",
      "Epoch 285/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9732 - val_loss: 0.0889 - val_accuracy: 0.9640\n",
      "Epoch 286/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9721 - val_loss: 0.0708 - val_accuracy: 0.9707\n",
      "Epoch 287/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9733 - val_loss: 0.0669 - val_accuracy: 0.9707\n",
      "Epoch 288/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9730 - val_loss: 0.0791 - val_accuracy: 0.9690\n",
      "Epoch 289/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9735 - val_loss: 0.0655 - val_accuracy: 0.9728\n",
      "Epoch 290/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9723 - val_loss: 0.0699 - val_accuracy: 0.9723\n",
      "Epoch 291/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9734 - val_loss: 0.0785 - val_accuracy: 0.9682\n",
      "Epoch 292/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9729 - val_loss: 0.0649 - val_accuracy: 0.9737\n",
      "Epoch 293/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9734 - val_loss: 0.0819 - val_accuracy: 0.9677\n",
      "Epoch 294/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9722 - val_loss: 0.0594 - val_accuracy: 0.9745\n",
      "Epoch 295/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0738 - accuracy: 0.9738 - val_loss: 0.0716 - val_accuracy: 0.9706\n",
      "Epoch 296/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.0754 - val_accuracy: 0.9698\n",
      "Epoch 297/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9744 - val_loss: 0.0669 - val_accuracy: 0.9725\n",
      "Epoch 298/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9731 - val_loss: 0.0704 - val_accuracy: 0.9709\n",
      "Epoch 299/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9729 - val_loss: 0.0602 - val_accuracy: 0.9746\n",
      "Epoch 300/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9745 - val_loss: 0.0806 - val_accuracy: 0.9676\n",
      "Epoch 301/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9723 - val_loss: 0.0802 - val_accuracy: 0.9686\n",
      "Epoch 302/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9727 - val_loss: 0.0760 - val_accuracy: 0.9684\n",
      "Epoch 303/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9731 - val_loss: 0.0669 - val_accuracy: 0.9725\n",
      "Epoch 304/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9745 - val_loss: 0.0673 - val_accuracy: 0.9727\n",
      "Epoch 305/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9732 - val_loss: 0.0706 - val_accuracy: 0.9716\n",
      "Epoch 306/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9735 - val_loss: 0.0732 - val_accuracy: 0.9704\n",
      "Epoch 307/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9739 - val_loss: 0.0676 - val_accuracy: 0.9734\n",
      "Epoch 308/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.0757 - val_accuracy: 0.9694\n",
      "Epoch 309/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 0.0638 - val_accuracy: 0.9740\n",
      "Epoch 310/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9728 - val_loss: 0.0650 - val_accuracy: 0.9744\n",
      "Epoch 311/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9754 - val_loss: 0.0722 - val_accuracy: 0.9704\n",
      "Epoch 312/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9759 - val_loss: 0.0738 - val_accuracy: 0.9696\n",
      "Epoch 313/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9741 - val_loss: 0.0790 - val_accuracy: 0.9690\n",
      "Epoch 314/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9724 - val_loss: 0.0559 - val_accuracy: 0.9766\n",
      "Epoch 315/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9731 - val_loss: 0.0598 - val_accuracy: 0.9753\n",
      "Epoch 316/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9737 - val_loss: 0.0622 - val_accuracy: 0.9737\n",
      "Epoch 317/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9740 - val_loss: 0.0623 - val_accuracy: 0.9739\n",
      "Epoch 318/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9754 - val_loss: 0.0732 - val_accuracy: 0.9700\n",
      "Epoch 319/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.9748 - val_loss: 0.0632 - val_accuracy: 0.9741\n",
      "Epoch 320/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9742 - val_loss: 0.0725 - val_accuracy: 0.9710\n",
      "Epoch 321/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9745 - val_loss: 0.0696 - val_accuracy: 0.9727\n",
      "Epoch 322/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9753 - val_loss: 0.0733 - val_accuracy: 0.9694\n",
      "Epoch 323/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.9742 - val_loss: 0.0634 - val_accuracy: 0.9746\n",
      "Epoch 324/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9754 - val_loss: 0.0660 - val_accuracy: 0.9734\n",
      "Epoch 325/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9759 - val_loss: 0.0714 - val_accuracy: 0.9715\n",
      "Epoch 326/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9742 - val_loss: 0.0638 - val_accuracy: 0.9731\n",
      "Epoch 327/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9743 - val_loss: 0.0690 - val_accuracy: 0.9725\n",
      "Epoch 328/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9738 - val_loss: 0.0633 - val_accuracy: 0.9729\n",
      "Epoch 329/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9740 - val_loss: 0.0704 - val_accuracy: 0.9729\n",
      "Epoch 330/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9756 - val_loss: 0.0583 - val_accuracy: 0.9764\n",
      "Epoch 331/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9756 - val_loss: 0.0617 - val_accuracy: 0.9759\n",
      "Epoch 332/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9746 - val_loss: 0.0692 - val_accuracy: 0.9728\n",
      "Epoch 333/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9743 - val_loss: 0.0577 - val_accuracy: 0.9769\n",
      "Epoch 334/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9755 - val_loss: 0.0681 - val_accuracy: 0.9728\n",
      "Epoch 335/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9748 - val_loss: 0.0639 - val_accuracy: 0.9740\n",
      "Epoch 336/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9753 - val_loss: 0.0567 - val_accuracy: 0.9757\n",
      "Epoch 337/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9757 - val_loss: 0.0664 - val_accuracy: 0.9731\n",
      "Epoch 338/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9753 - val_loss: 0.0698 - val_accuracy: 0.9723\n",
      "Epoch 339/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9758 - val_loss: 0.0721 - val_accuracy: 0.9708\n",
      "Epoch 340/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9744 - val_loss: 0.0620 - val_accuracy: 0.9743\n",
      "Epoch 341/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9754 - val_loss: 0.0729 - val_accuracy: 0.9707\n",
      "Epoch 342/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9741 - val_loss: 0.0721 - val_accuracy: 0.9720\n",
      "Epoch 343/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9763 - val_loss: 0.0620 - val_accuracy: 0.9746\n",
      "Epoch 344/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9759 - val_loss: 0.0607 - val_accuracy: 0.9737\n",
      "Epoch 345/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9758 - val_loss: 0.0682 - val_accuracy: 0.9729\n",
      "Epoch 346/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9755 - val_loss: 0.0708 - val_accuracy: 0.9724\n",
      "Epoch 347/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9764 - val_loss: 0.0839 - val_accuracy: 0.9686\n",
      "Epoch 348/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9745 - val_loss: 0.0594 - val_accuracy: 0.9763\n",
      "Epoch 349/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9770 - val_loss: 0.0678 - val_accuracy: 0.9734\n",
      "Epoch 350/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9760 - val_loss: 0.0570 - val_accuracy: 0.9761\n",
      "Epoch 351/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.0614 - val_accuracy: 0.9741\n",
      "Epoch 352/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9757 - val_loss: 0.0631 - val_accuracy: 0.9747\n",
      "Epoch 353/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9764 - val_loss: 0.0629 - val_accuracy: 0.9739\n",
      "Epoch 354/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9753 - val_loss: 0.0692 - val_accuracy: 0.9724\n",
      "Epoch 355/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0702 - accuracy: 0.9751 - val_loss: 0.0682 - val_accuracy: 0.9734\n",
      "Epoch 356/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9751 - val_loss: 0.0682 - val_accuracy: 0.9720\n",
      "Epoch 357/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9770 - val_loss: 0.0685 - val_accuracy: 0.9726\n",
      "Epoch 358/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9773 - val_loss: 0.0663 - val_accuracy: 0.9736\n",
      "Epoch 359/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9756 - val_loss: 0.0589 - val_accuracy: 0.9758\n",
      "Epoch 360/1000\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9752 - val_loss: 0.0652 - val_accuracy: 0.9730\n",
      "Epoch 361/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9748 - val_loss: 0.0630 - val_accuracy: 0.9751\n",
      "Epoch 362/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9759 - val_loss: 0.0582 - val_accuracy: 0.9750\n",
      "Epoch 363/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9743 - val_loss: 0.0692 - val_accuracy: 0.9708\n",
      "Epoch 364/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9763 - val_loss: 0.0694 - val_accuracy: 0.9726\n",
      "Epoch 365/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9771 - val_loss: 0.0578 - val_accuracy: 0.9764\n",
      "Epoch 366/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9759 - val_loss: 0.0697 - val_accuracy: 0.9714\n",
      "Epoch 367/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.0556 - val_accuracy: 0.9771\n",
      "Epoch 368/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9758 - val_loss: 0.0596 - val_accuracy: 0.9755\n",
      "Epoch 369/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9765 - val_loss: 0.0576 - val_accuracy: 0.9751\n",
      "Epoch 370/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9761 - val_loss: 0.0644 - val_accuracy: 0.9740\n",
      "Epoch 371/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9757 - val_loss: 0.0633 - val_accuracy: 0.9743\n",
      "Epoch 372/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 0.0692 - val_accuracy: 0.9729\n",
      "Epoch 373/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9762 - val_loss: 0.0755 - val_accuracy: 0.9705\n",
      "Epoch 374/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9767 - val_loss: 0.0584 - val_accuracy: 0.9766\n",
      "Epoch 375/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9777 - val_loss: 0.0636 - val_accuracy: 0.9746\n",
      "Epoch 376/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9767 - val_loss: 0.0548 - val_accuracy: 0.9773\n",
      "Epoch 377/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9777 - val_loss: 0.0598 - val_accuracy: 0.9761\n",
      "Epoch 378/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9765 - val_loss: 0.0730 - val_accuracy: 0.9707\n",
      "Epoch 379/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9761 - val_loss: 0.0646 - val_accuracy: 0.9747\n",
      "Epoch 380/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9767 - val_loss: 0.0626 - val_accuracy: 0.9743\n",
      "Epoch 381/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9769 - val_loss: 0.0758 - val_accuracy: 0.9699\n",
      "Epoch 382/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9757 - val_loss: 0.0590 - val_accuracy: 0.9749\n",
      "Epoch 383/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9778 - val_loss: 0.0669 - val_accuracy: 0.9729\n",
      "Epoch 384/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.0837 - val_accuracy: 0.9684\n",
      "Epoch 385/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9767 - val_loss: 0.0792 - val_accuracy: 0.9693\n",
      "Epoch 386/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9772 - val_loss: 0.0677 - val_accuracy: 0.9738\n",
      "Epoch 387/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9775 - val_loss: 0.0532 - val_accuracy: 0.9782\n",
      "Epoch 388/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9758 - val_loss: 0.0670 - val_accuracy: 0.9740\n",
      "Epoch 389/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9774 - val_loss: 0.0646 - val_accuracy: 0.9745\n",
      "Epoch 390/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9772 - val_loss: 0.0592 - val_accuracy: 0.9756\n",
      "Epoch 391/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9769 - val_loss: 0.0651 - val_accuracy: 0.9736\n",
      "Epoch 392/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9771 - val_loss: 0.0629 - val_accuracy: 0.9749\n",
      "Epoch 393/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9773 - val_loss: 0.0583 - val_accuracy: 0.9765\n",
      "Epoch 394/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9772 - val_loss: 0.0586 - val_accuracy: 0.9760\n",
      "Epoch 395/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9773 - val_loss: 0.0689 - val_accuracy: 0.9727\n",
      "Epoch 396/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0648 - accuracy: 0.9768 - val_loss: 0.0560 - val_accuracy: 0.9773\n",
      "Epoch 397/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9777 - val_loss: 0.0576 - val_accuracy: 0.9768\n",
      "Epoch 398/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9778 - val_loss: 0.0597 - val_accuracy: 0.9753\n",
      "Epoch 399/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9776 - val_loss: 0.0540 - val_accuracy: 0.9767\n",
      "Epoch 400/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9776 - val_loss: 0.0571 - val_accuracy: 0.9764\n",
      "Epoch 401/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9765 - val_loss: 0.0641 - val_accuracy: 0.9754\n",
      "Epoch 402/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9766 - val_loss: 0.0584 - val_accuracy: 0.9766\n",
      "Epoch 403/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9767 - val_loss: 0.0768 - val_accuracy: 0.9713\n",
      "Epoch 404/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9757 - val_loss: 0.0706 - val_accuracy: 0.9731\n",
      "Epoch 405/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9778 - val_loss: 0.0583 - val_accuracy: 0.9756\n",
      "Epoch 406/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9773 - val_loss: 0.0691 - val_accuracy: 0.9724\n",
      "Epoch 407/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9770 - val_loss: 0.0654 - val_accuracy: 0.9740\n",
      "Epoch 408/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9773 - val_loss: 0.0682 - val_accuracy: 0.9741\n",
      "Epoch 409/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.0594 - val_accuracy: 0.9759\n",
      "Epoch 410/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9783 - val_loss: 0.0649 - val_accuracy: 0.9744\n",
      "Epoch 411/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9767 - val_loss: 0.0721 - val_accuracy: 0.9725\n",
      "Epoch 412/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9770 - val_loss: 0.0569 - val_accuracy: 0.9775\n",
      "Epoch 413/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9768 - val_loss: 0.0703 - val_accuracy: 0.9730\n",
      "Epoch 414/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9775 - val_loss: 0.0563 - val_accuracy: 0.9767\n",
      "Epoch 415/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9779 - val_loss: 0.0567 - val_accuracy: 0.9765\n",
      "Epoch 416/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9779 - val_loss: 0.0673 - val_accuracy: 0.9741\n",
      "Epoch 417/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9782 - val_loss: 0.0588 - val_accuracy: 0.9764\n",
      "Epoch 418/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9771 - val_loss: 0.0574 - val_accuracy: 0.9769\n",
      "Epoch 419/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9767 - val_loss: 0.0595 - val_accuracy: 0.9756\n",
      "Epoch 420/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9779 - val_loss: 0.0606 - val_accuracy: 0.9767\n",
      "Epoch 421/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9782 - val_loss: 0.0794 - val_accuracy: 0.9695\n",
      "Epoch 422/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9775 - val_loss: 0.0597 - val_accuracy: 0.9759\n",
      "Epoch 423/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.0709 - val_accuracy: 0.9730\n",
      "Epoch 424/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.0682 - val_accuracy: 0.9737\n",
      "Epoch 425/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9772 - val_loss: 0.0561 - val_accuracy: 0.9768\n",
      "Epoch 426/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9790 - val_loss: 0.0626 - val_accuracy: 0.9748\n",
      "Epoch 427/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9787 - val_loss: 0.0627 - val_accuracy: 0.9751\n",
      "Epoch 428/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9777 - val_loss: 0.0690 - val_accuracy: 0.9729\n",
      "Epoch 429/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9778 - val_loss: 0.0620 - val_accuracy: 0.9753\n",
      "Epoch 430/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9772 - val_loss: 0.0607 - val_accuracy: 0.9751\n",
      "Epoch 431/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9785 - val_loss: 0.0654 - val_accuracy: 0.9739\n",
      "Epoch 432/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9772 - val_loss: 0.0534 - val_accuracy: 0.9778\n",
      "Epoch 433/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9779 - val_loss: 0.0642 - val_accuracy: 0.9757\n",
      "Epoch 434/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.0533 - val_accuracy: 0.9777\n",
      "Epoch 435/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9789 - val_loss: 0.0569 - val_accuracy: 0.9770\n",
      "Epoch 436/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9775 - val_loss: 0.0602 - val_accuracy: 0.9750\n",
      "Epoch 437/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9781 - val_loss: 0.0648 - val_accuracy: 0.9750\n",
      "Epoch 438/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9790 - val_loss: 0.0642 - val_accuracy: 0.9748\n",
      "Epoch 439/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9783 - val_loss: 0.0660 - val_accuracy: 0.9750\n",
      "Epoch 440/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.0696 - val_accuracy: 0.9731\n",
      "Epoch 441/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.0692 - val_accuracy: 0.9737\n",
      "Epoch 442/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9772 - val_loss: 0.0550 - val_accuracy: 0.9776\n",
      "Epoch 443/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9783 - val_loss: 0.0702 - val_accuracy: 0.9734\n",
      "Epoch 444/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9784 - val_loss: 0.0505 - val_accuracy: 0.9791\n",
      "Epoch 445/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9765 - val_loss: 0.0604 - val_accuracy: 0.9757\n",
      "Epoch 446/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9787 - val_loss: 0.0666 - val_accuracy: 0.9743\n",
      "Epoch 447/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9784 - val_loss: 0.0631 - val_accuracy: 0.9759\n",
      "Epoch 448/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9779 - val_loss: 0.0564 - val_accuracy: 0.9767\n",
      "Epoch 449/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9773 - val_loss: 0.0556 - val_accuracy: 0.9769\n",
      "Epoch 450/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9784 - val_loss: 0.0569 - val_accuracy: 0.9763\n",
      "Epoch 451/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9785 - val_loss: 0.0646 - val_accuracy: 0.9746\n",
      "Epoch 452/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.0691 - val_accuracy: 0.9731\n",
      "Epoch 453/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9782 - val_loss: 0.0623 - val_accuracy: 0.9755\n",
      "Epoch 454/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9782 - val_loss: 0.0599 - val_accuracy: 0.9760\n",
      "Epoch 455/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9791 - val_loss: 0.0644 - val_accuracy: 0.9756\n",
      "Epoch 456/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.0653 - val_accuracy: 0.9748\n",
      "Epoch 457/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9789 - val_loss: 0.0518 - val_accuracy: 0.9782\n",
      "Epoch 458/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9770 - val_loss: 0.0602 - val_accuracy: 0.9768\n",
      "Epoch 459/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9781 - val_loss: 0.0607 - val_accuracy: 0.9766\n",
      "Epoch 460/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9784 - val_loss: 0.0598 - val_accuracy: 0.9774\n",
      "Epoch 461/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9789 - val_loss: 0.0590 - val_accuracy: 0.9758\n",
      "Epoch 462/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9776 - val_loss: 0.0559 - val_accuracy: 0.9769\n",
      "Epoch 463/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9781 - val_loss: 0.0626 - val_accuracy: 0.9751\n",
      "Epoch 464/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9779 - val_loss: 0.0517 - val_accuracy: 0.9778\n",
      "Epoch 465/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0690 - val_accuracy: 0.9729\n",
      "Epoch 466/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9795 - val_loss: 0.0551 - val_accuracy: 0.9773\n",
      "Epoch 467/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9771 - val_loss: 0.0779 - val_accuracy: 0.9715\n",
      "Epoch 468/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9785 - val_loss: 0.0636 - val_accuracy: 0.9761\n",
      "Epoch 469/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9782 - val_loss: 0.0691 - val_accuracy: 0.9734\n",
      "Epoch 470/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9775 - val_loss: 0.0713 - val_accuracy: 0.9726\n",
      "Epoch 471/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9790 - val_loss: 0.0588 - val_accuracy: 0.9764\n",
      "Epoch 472/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9793 - val_loss: 0.0619 - val_accuracy: 0.9755\n",
      "Epoch 473/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9792 - val_loss: 0.0567 - val_accuracy: 0.9767\n",
      "Epoch 474/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9791 - val_loss: 0.0772 - val_accuracy: 0.9714\n",
      "Epoch 475/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9791 - val_loss: 0.0608 - val_accuracy: 0.9760\n",
      "Epoch 476/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9772 - val_loss: 0.0579 - val_accuracy: 0.9767\n",
      "Epoch 477/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9788 - val_loss: 0.0589 - val_accuracy: 0.9756\n",
      "Epoch 478/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9782 - val_loss: 0.0583 - val_accuracy: 0.9767\n",
      "Epoch 479/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9797 - val_loss: 0.0562 - val_accuracy: 0.9774\n",
      "Epoch 480/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9783 - val_loss: 0.0626 - val_accuracy: 0.9766\n",
      "Epoch 481/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9788 - val_loss: 0.0717 - val_accuracy: 0.9735\n",
      "Epoch 482/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9779 - val_loss: 0.0561 - val_accuracy: 0.9759\n",
      "Epoch 483/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9797 - val_loss: 0.0590 - val_accuracy: 0.9774\n",
      "Epoch 484/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9776 - val_loss: 0.0639 - val_accuracy: 0.9758\n",
      "Epoch 485/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9792 - val_loss: 0.0544 - val_accuracy: 0.9787\n",
      "Epoch 486/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9784 - val_loss: 0.0606 - val_accuracy: 0.9758\n",
      "Epoch 487/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9788 - val_loss: 0.0580 - val_accuracy: 0.9767\n",
      "Epoch 488/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9795 - val_loss: 0.0601 - val_accuracy: 0.9763\n",
      "Epoch 489/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9786 - val_loss: 0.0581 - val_accuracy: 0.9765\n",
      "Epoch 490/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9785 - val_loss: 0.0640 - val_accuracy: 0.9749\n",
      "Epoch 491/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9788 - val_loss: 0.0592 - val_accuracy: 0.9754\n",
      "Epoch 492/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9789 - val_loss: 0.0655 - val_accuracy: 0.9748\n",
      "Epoch 493/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9784 - val_loss: 0.0590 - val_accuracy: 0.9761\n",
      "Epoch 494/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9793 - val_loss: 0.0580 - val_accuracy: 0.9775\n",
      "Epoch 495/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9786 - val_loss: 0.0595 - val_accuracy: 0.9760\n",
      "Epoch 496/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9784 - val_loss: 0.0609 - val_accuracy: 0.9765\n",
      "Epoch 497/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0608 - accuracy: 0.9786 - val_loss: 0.0594 - val_accuracy: 0.9763\n",
      "Epoch 498/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9794 - val_loss: 0.0593 - val_accuracy: 0.9776\n",
      "Epoch 499/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9777 - val_loss: 0.0806 - val_accuracy: 0.9708\n",
      "Epoch 500/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.0522 - val_accuracy: 0.9784\n",
      "Epoch 501/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.0604 - val_accuracy: 0.9758\n",
      "Epoch 502/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9792 - val_loss: 0.0594 - val_accuracy: 0.9759\n",
      "Epoch 503/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9795 - val_loss: 0.0548 - val_accuracy: 0.9777\n",
      "Epoch 504/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.0629 - val_accuracy: 0.9756\n",
      "Epoch 505/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 0.0507 - val_accuracy: 0.9789\n",
      "Epoch 506/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9801 - val_loss: 0.0597 - val_accuracy: 0.9757\n",
      "Epoch 507/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.0700 - val_accuracy: 0.9738\n",
      "Epoch 508/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9789 - val_loss: 0.0525 - val_accuracy: 0.9784\n",
      "Epoch 509/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9793 - val_loss: 0.0612 - val_accuracy: 0.9761\n",
      "Epoch 510/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9798 - val_loss: 0.0659 - val_accuracy: 0.9750\n",
      "Epoch 511/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9793 - val_loss: 0.0573 - val_accuracy: 0.9758\n",
      "Epoch 512/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9783 - val_loss: 0.0590 - val_accuracy: 0.9771\n",
      "Epoch 513/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9796 - val_loss: 0.0670 - val_accuracy: 0.9750\n",
      "Epoch 514/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9794 - val_loss: 0.0530 - val_accuracy: 0.9775\n",
      "Epoch 515/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9792 - val_loss: 0.0552 - val_accuracy: 0.9781\n",
      "Epoch 516/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9791 - val_loss: 0.0601 - val_accuracy: 0.9771\n",
      "Epoch 517/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9786 - val_loss: 0.0656 - val_accuracy: 0.9746\n",
      "Epoch 518/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9790 - val_loss: 0.0605 - val_accuracy: 0.9756\n",
      "Epoch 519/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9785 - val_loss: 0.0598 - val_accuracy: 0.9763\n",
      "Epoch 520/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9794 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 521/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9799 - val_loss: 0.0687 - val_accuracy: 0.9747\n",
      "Epoch 522/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9794 - val_loss: 0.0668 - val_accuracy: 0.9749\n",
      "Epoch 523/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9804 - val_loss: 0.0644 - val_accuracy: 0.9748\n",
      "Epoch 524/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 0.0637 - val_accuracy: 0.9759\n",
      "Epoch 525/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9798 - val_loss: 0.0750 - val_accuracy: 0.9727\n",
      "Epoch 526/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9786 - val_loss: 0.0583 - val_accuracy: 0.9760\n",
      "Epoch 527/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9795 - val_loss: 0.0689 - val_accuracy: 0.9749\n",
      "Epoch 528/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9782 - val_loss: 0.0578 - val_accuracy: 0.9769\n",
      "Epoch 529/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9799 - val_loss: 0.0600 - val_accuracy: 0.9760\n",
      "Epoch 530/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9791 - val_loss: 0.0588 - val_accuracy: 0.9769\n",
      "Epoch 531/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0558 - val_accuracy: 0.9765\n",
      "Epoch 532/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9796 - val_loss: 0.0712 - val_accuracy: 0.9738\n",
      "Epoch 533/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9803 - val_loss: 0.0536 - val_accuracy: 0.9779\n",
      "Epoch 534/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9792 - val_loss: 0.0576 - val_accuracy: 0.9769\n",
      "Epoch 535/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.0556 - val_accuracy: 0.9780\n",
      "Epoch 536/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9789 - val_loss: 0.0632 - val_accuracy: 0.9757\n",
      "Epoch 537/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9788 - val_loss: 0.0630 - val_accuracy: 0.9748\n",
      "Epoch 538/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9804 - val_loss: 0.0622 - val_accuracy: 0.9759\n",
      "Epoch 539/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9794 - val_loss: 0.0613 - val_accuracy: 0.9761\n",
      "Epoch 540/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9794 - val_loss: 0.0553 - val_accuracy: 0.9769\n",
      "Epoch 541/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.0621 - val_accuracy: 0.9757\n",
      "Epoch 542/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9802 - val_loss: 0.0569 - val_accuracy: 0.9778\n",
      "Epoch 543/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9790 - val_loss: 0.0570 - val_accuracy: 0.9766\n",
      "Epoch 544/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9794 - val_loss: 0.0696 - val_accuracy: 0.9748\n",
      "Epoch 545/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.0609 - val_accuracy: 0.9760\n",
      "Epoch 546/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9790 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 547/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 0.0621 - val_accuracy: 0.9755\n",
      "Epoch 548/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9806 - val_loss: 0.0619 - val_accuracy: 0.9745\n",
      "Epoch 549/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9791 - val_loss: 0.0692 - val_accuracy: 0.9743\n",
      "Epoch 550/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9799 - val_loss: 0.0780 - val_accuracy: 0.9716\n",
      "Epoch 551/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9803 - val_loss: 0.0601 - val_accuracy: 0.9765\n",
      "Epoch 552/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9796 - val_loss: 0.0567 - val_accuracy: 0.9775\n",
      "Epoch 553/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0558 - accuracy: 0.9802 - val_loss: 0.0574 - val_accuracy: 0.9776\n",
      "Epoch 554/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 0.0566 - val_accuracy: 0.9765\n",
      "Epoch 555/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.0601 - val_accuracy: 0.9763\n",
      "Epoch 556/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9797 - val_loss: 0.0632 - val_accuracy: 0.9764\n",
      "Epoch 557/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9793 - val_loss: 0.0687 - val_accuracy: 0.9746\n",
      "Epoch 558/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9796 - val_loss: 0.0727 - val_accuracy: 0.9736\n",
      "Epoch 559/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9789 - val_loss: 0.0682 - val_accuracy: 0.9749\n",
      "Epoch 560/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9796 - val_loss: 0.0651 - val_accuracy: 0.9750\n",
      "Epoch 561/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9801 - val_loss: 0.0650 - val_accuracy: 0.9753\n",
      "Epoch 562/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9802 - val_loss: 0.0505 - val_accuracy: 0.9782\n",
      "Epoch 563/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9795 - val_loss: 0.0626 - val_accuracy: 0.9763\n",
      "Epoch 564/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.0680 - val_accuracy: 0.9753\n",
      "Epoch 565/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9799 - val_loss: 0.0590 - val_accuracy: 0.9769\n",
      "Epoch 566/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9795 - val_loss: 0.0575 - val_accuracy: 0.9766\n",
      "Epoch 567/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9809 - val_loss: 0.0732 - val_accuracy: 0.9736\n",
      "Epoch 568/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9807 - val_loss: 0.0623 - val_accuracy: 0.9761\n",
      "Epoch 569/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9801 - val_loss: 0.0632 - val_accuracy: 0.9751\n",
      "Epoch 570/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9795 - val_loss: 0.0598 - val_accuracy: 0.9771\n",
      "Epoch 571/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9797 - val_loss: 0.0545 - val_accuracy: 0.9773\n",
      "Epoch 572/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9797 - val_loss: 0.0673 - val_accuracy: 0.9750\n",
      "Epoch 573/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9791 - val_loss: 0.0574 - val_accuracy: 0.9769\n",
      "Epoch 574/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9797 - val_loss: 0.0729 - val_accuracy: 0.9730\n",
      "Epoch 575/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.0723 - val_accuracy: 0.9729\n",
      "Epoch 576/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9799 - val_loss: 0.0544 - val_accuracy: 0.9771\n",
      "Epoch 577/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9810 - val_loss: 0.0675 - val_accuracy: 0.9753\n",
      "Epoch 578/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.0717 - val_accuracy: 0.9745\n",
      "Epoch 579/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9803 - val_loss: 0.0596 - val_accuracy: 0.9763\n",
      "Epoch 580/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 0.0731 - val_accuracy: 0.9737\n",
      "Epoch 581/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9803 - val_loss: 0.0610 - val_accuracy: 0.9763\n",
      "Epoch 582/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9792 - val_loss: 0.0558 - val_accuracy: 0.9774\n",
      "Epoch 583/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9795 - val_loss: 0.0592 - val_accuracy: 0.9768\n",
      "Epoch 584/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9788 - val_loss: 0.0607 - val_accuracy: 0.9763\n",
      "Epoch 585/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9808 - val_loss: 0.0756 - val_accuracy: 0.9718\n",
      "Epoch 586/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9796 - val_loss: 0.0639 - val_accuracy: 0.9756\n",
      "Epoch 587/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0594 - val_accuracy: 0.9778\n",
      "Epoch 588/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0618 - val_accuracy: 0.9766\n",
      "Epoch 589/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9806 - val_loss: 0.0585 - val_accuracy: 0.9764\n",
      "Epoch 590/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9780 - val_loss: 0.0571 - val_accuracy: 0.9769\n",
      "Epoch 591/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9791 - val_loss: 0.0679 - val_accuracy: 0.9750\n",
      "Epoch 592/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9799 - val_loss: 0.0663 - val_accuracy: 0.9755\n",
      "Epoch 593/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9795 - val_loss: 0.0562 - val_accuracy: 0.9771\n",
      "Epoch 594/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9795 - val_loss: 0.0676 - val_accuracy: 0.9757\n",
      "Epoch 595/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9791 - val_loss: 0.0568 - val_accuracy: 0.9775\n",
      "Epoch 596/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9795 - val_loss: 0.0590 - val_accuracy: 0.9765\n",
      "Epoch 597/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.0708 - val_accuracy: 0.9741\n",
      "Epoch 598/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0566 - accuracy: 0.9795 - val_loss: 0.0586 - val_accuracy: 0.9757\n",
      "Epoch 599/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 0.0498 - val_accuracy: 0.9780\n",
      "Epoch 600/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9799 - val_loss: 0.0675 - val_accuracy: 0.9756\n",
      "Epoch 601/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9797 - val_loss: 0.0529 - val_accuracy: 0.9776\n",
      "Epoch 602/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9796 - val_loss: 0.0517 - val_accuracy: 0.9788\n",
      "Epoch 603/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 0.0592 - val_accuracy: 0.9767\n",
      "Epoch 604/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9761\n",
      "Epoch 605/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9803 - val_loss: 0.0675 - val_accuracy: 0.9757\n",
      "Epoch 606/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9806 - val_loss: 0.0653 - val_accuracy: 0.9759\n",
      "Epoch 607/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9807 - val_loss: 0.0594 - val_accuracy: 0.9776\n",
      "Epoch 608/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9806 - val_loss: 0.0535 - val_accuracy: 0.9774\n",
      "Epoch 609/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9801 - val_loss: 0.0620 - val_accuracy: 0.9761\n",
      "Epoch 610/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9796 - val_loss: 0.0663 - val_accuracy: 0.9770\n",
      "Epoch 611/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9797 - val_loss: 0.0774 - val_accuracy: 0.9734\n",
      "Epoch 612/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9803 - val_loss: 0.0585 - val_accuracy: 0.9770\n",
      "Epoch 613/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.0582 - val_accuracy: 0.9770\n",
      "Epoch 614/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9798 - val_loss: 0.0570 - val_accuracy: 0.9784\n",
      "Epoch 615/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9804 - val_loss: 0.0555 - val_accuracy: 0.9776\n",
      "Epoch 616/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9805 - val_loss: 0.0663 - val_accuracy: 0.9755\n",
      "Epoch 617/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9814 - val_loss: 0.0652 - val_accuracy: 0.9756\n",
      "Epoch 618/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.0634 - val_accuracy: 0.9765\n",
      "Epoch 619/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9802 - val_loss: 0.0554 - val_accuracy: 0.9784\n",
      "Epoch 620/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9804 - val_loss: 0.0541 - val_accuracy: 0.9784\n",
      "Epoch 621/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9803 - val_loss: 0.0762 - val_accuracy: 0.9726\n",
      "Epoch 622/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9796 - val_loss: 0.0645 - val_accuracy: 0.9764\n",
      "Epoch 623/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9801 - val_loss: 0.0612 - val_accuracy: 0.9757\n",
      "Epoch 624/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.0596 - val_accuracy: 0.9775\n",
      "Epoch 625/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9808 - val_loss: 0.0802 - val_accuracy: 0.9713\n",
      "Epoch 626/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.0638 - val_accuracy: 0.9759\n",
      "Epoch 627/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9813 - val_loss: 0.0598 - val_accuracy: 0.9767\n",
      "Epoch 628/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9797 - val_loss: 0.0598 - val_accuracy: 0.9775\n",
      "Epoch 629/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9799 - val_loss: 0.0538 - val_accuracy: 0.9788\n",
      "Epoch 630/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9796 - val_loss: 0.0584 - val_accuracy: 0.9767\n",
      "Epoch 631/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9821 - val_loss: 0.0649 - val_accuracy: 0.9763\n",
      "Epoch 632/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9803 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 633/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9806 - val_loss: 0.0638 - val_accuracy: 0.9768\n",
      "Epoch 634/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0536 - val_accuracy: 0.9769\n",
      "Epoch 635/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9811 - val_loss: 0.0550 - val_accuracy: 0.9774\n",
      "Epoch 636/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9810 - val_loss: 0.0527 - val_accuracy: 0.9781\n",
      "Epoch 637/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9802 - val_loss: 0.0576 - val_accuracy: 0.9785\n",
      "Epoch 638/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9802 - val_loss: 0.0563 - val_accuracy: 0.9777\n",
      "Epoch 639/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9809 - val_loss: 0.0544 - val_accuracy: 0.9778\n",
      "Epoch 640/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9807 - val_loss: 0.0550 - val_accuracy: 0.9779\n",
      "Epoch 641/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9809 - val_loss: 0.0497 - val_accuracy: 0.9781\n",
      "Epoch 642/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9797 - val_loss: 0.0684 - val_accuracy: 0.9753\n",
      "Epoch 643/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.0637 - val_accuracy: 0.9761\n",
      "Epoch 644/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.0573 - val_accuracy: 0.9773\n",
      "Epoch 645/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.0565 - val_accuracy: 0.9780\n",
      "Epoch 646/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.0537 - val_accuracy: 0.9782\n",
      "Epoch 647/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9808 - val_loss: 0.0535 - val_accuracy: 0.9787\n",
      "Epoch 648/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9805 - val_loss: 0.0683 - val_accuracy: 0.9754\n",
      "Epoch 649/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9801 - val_loss: 0.0575 - val_accuracy: 0.9770\n",
      "Epoch 650/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9809 - val_loss: 0.0819 - val_accuracy: 0.9721\n",
      "Epoch 651/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 0.0586 - val_accuracy: 0.9770\n",
      "Epoch 652/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9804 - val_loss: 0.0613 - val_accuracy: 0.9766\n",
      "Epoch 653/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.0560 - val_accuracy: 0.9779\n",
      "Epoch 654/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.0562 - val_accuracy: 0.9765\n",
      "Epoch 655/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9800 - val_loss: 0.0648 - val_accuracy: 0.9764\n",
      "Epoch 656/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9817 - val_loss: 0.0604 - val_accuracy: 0.9761\n",
      "Epoch 657/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9805 - val_loss: 0.0702 - val_accuracy: 0.9753\n",
      "Epoch 658/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9798 - val_loss: 0.0700 - val_accuracy: 0.9757\n",
      "Epoch 659/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9804 - val_loss: 0.0536 - val_accuracy: 0.9776\n",
      "Epoch 660/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9811 - val_loss: 0.0675 - val_accuracy: 0.9750\n",
      "Epoch 661/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.0550 - val_accuracy: 0.9774\n",
      "Epoch 662/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9808 - val_loss: 0.0552 - val_accuracy: 0.9780\n",
      "Epoch 663/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 0.0594 - val_accuracy: 0.9766\n",
      "Epoch 664/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9800 - val_loss: 0.0609 - val_accuracy: 0.9775\n",
      "Epoch 665/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.0599 - val_accuracy: 0.9773\n",
      "Epoch 666/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9812 - val_loss: 0.0590 - val_accuracy: 0.9768\n",
      "Epoch 667/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9807 - val_loss: 0.0572 - val_accuracy: 0.9771\n",
      "Epoch 668/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0627 - val_accuracy: 0.9769\n",
      "Epoch 669/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9810 - val_loss: 0.0570 - val_accuracy: 0.9771\n",
      "Epoch 670/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9798 - val_loss: 0.0648 - val_accuracy: 0.9766\n",
      "Epoch 671/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9815 - val_loss: 0.0782 - val_accuracy: 0.9726\n",
      "Epoch 672/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9789 - val_loss: 0.0636 - val_accuracy: 0.9768\n",
      "Epoch 673/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9813 - val_loss: 0.0599 - val_accuracy: 0.9773\n",
      "Epoch 674/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.0646 - val_accuracy: 0.9755\n",
      "Epoch 675/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9799 - val_loss: 0.0563 - val_accuracy: 0.9777\n",
      "Epoch 676/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9811 - val_loss: 0.0561 - val_accuracy: 0.9775\n",
      "Epoch 677/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9814 - val_loss: 0.0602 - val_accuracy: 0.9760\n",
      "Epoch 678/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0668 - val_accuracy: 0.9754\n",
      "Epoch 679/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9803 - val_loss: 0.0531 - val_accuracy: 0.9780\n",
      "Epoch 680/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9809 - val_loss: 0.0617 - val_accuracy: 0.9770\n",
      "Epoch 681/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9819 - val_loss: 0.0542 - val_accuracy: 0.9785\n",
      "Epoch 682/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9808 - val_loss: 0.0583 - val_accuracy: 0.9779\n",
      "Epoch 683/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9811 - val_loss: 0.0583 - val_accuracy: 0.9780\n",
      "Epoch 684/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.0763 - val_accuracy: 0.9726\n",
      "Epoch 685/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.0577 - val_accuracy: 0.9782\n",
      "Epoch 686/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9802 - val_loss: 0.0584 - val_accuracy: 0.9773\n",
      "Epoch 687/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9805 - val_loss: 0.0608 - val_accuracy: 0.9781\n",
      "Epoch 688/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9807 - val_loss: 0.0536 - val_accuracy: 0.9785\n",
      "Epoch 689/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9808 - val_loss: 0.0598 - val_accuracy: 0.9763\n",
      "Epoch 690/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9809 - val_loss: 0.0587 - val_accuracy: 0.9767\n",
      "Epoch 691/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9815 - val_loss: 0.0599 - val_accuracy: 0.9773\n",
      "Epoch 692/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9806 - val_loss: 0.0596 - val_accuracy: 0.9768\n",
      "Epoch 693/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0529 - val_accuracy: 0.9785\n",
      "Epoch 694/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 0.0683 - val_accuracy: 0.9757\n",
      "Epoch 695/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.0569 - val_accuracy: 0.9775\n",
      "Epoch 696/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0755 - val_accuracy: 0.9744\n",
      "Epoch 697/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9808 - val_loss: 0.0596 - val_accuracy: 0.9778\n",
      "Epoch 698/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.0708 - val_accuracy: 0.9747\n",
      "Epoch 699/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 0.0576 - val_accuracy: 0.9765\n",
      "Epoch 700/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9818 - val_loss: 0.0628 - val_accuracy: 0.9769\n",
      "Epoch 701/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9809 - val_loss: 0.0606 - val_accuracy: 0.9770\n",
      "Epoch 702/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.0645 - val_accuracy: 0.9764\n",
      "Epoch 703/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9820 - val_loss: 0.0551 - val_accuracy: 0.9790\n",
      "Epoch 704/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9807 - val_loss: 0.0665 - val_accuracy: 0.9759\n",
      "Epoch 705/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0504 - accuracy: 0.9818 - val_loss: 0.0671 - val_accuracy: 0.9754\n",
      "Epoch 706/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9806 - val_loss: 0.0669 - val_accuracy: 0.9750\n",
      "Epoch 707/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0555 - accuracy: 0.9794 - val_loss: 0.0588 - val_accuracy: 0.9767\n",
      "Epoch 708/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9804 - val_loss: 0.0665 - val_accuracy: 0.9756\n",
      "Epoch 709/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0533 - accuracy: 0.9800 - val_loss: 0.0589 - val_accuracy: 0.9777\n",
      "Epoch 710/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0521 - accuracy: 0.9809 - val_loss: 0.0630 - val_accuracy: 0.9763\n",
      "Epoch 711/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0553 - accuracy: 0.9792 - val_loss: 0.0549 - val_accuracy: 0.9781\n",
      "Epoch 712/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0526 - accuracy: 0.9806 - val_loss: 0.0623 - val_accuracy: 0.9769\n",
      "Epoch 713/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9814 - val_loss: 0.0544 - val_accuracy: 0.9776\n",
      "Epoch 714/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0515 - accuracy: 0.9809 - val_loss: 0.0671 - val_accuracy: 0.9760\n",
      "Epoch 715/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0639 - val_accuracy: 0.9767\n",
      "Epoch 716/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9813 - val_loss: 0.0674 - val_accuracy: 0.9756\n",
      "Epoch 717/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9812 - val_loss: 0.0542 - val_accuracy: 0.9778\n",
      "Epoch 718/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9822 - val_loss: 0.0599 - val_accuracy: 0.9777\n",
      "Epoch 719/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0501 - accuracy: 0.9821 - val_loss: 0.0588 - val_accuracy: 0.9782\n",
      "Epoch 720/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.0542 - val_accuracy: 0.9785\n",
      "Epoch 721/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0527 - accuracy: 0.9803 - val_loss: 0.0600 - val_accuracy: 0.9773\n",
      "Epoch 722/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0523 - accuracy: 0.9810 - val_loss: 0.0569 - val_accuracy: 0.9769\n",
      "Epoch 723/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9819 - val_loss: 0.0610 - val_accuracy: 0.9775\n",
      "Epoch 724/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9807 - val_loss: 0.0547 - val_accuracy: 0.9785\n",
      "Epoch 725/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9812 - val_loss: 0.0711 - val_accuracy: 0.9746\n",
      "Epoch 726/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 0.0558 - val_accuracy: 0.9782\n",
      "Epoch 727/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9815 - val_loss: 0.0608 - val_accuracy: 0.9775\n",
      "Epoch 728/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9805 - val_loss: 0.0587 - val_accuracy: 0.9769\n",
      "Epoch 729/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.0648 - val_accuracy: 0.9758\n",
      "Epoch 730/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9819 - val_loss: 0.0578 - val_accuracy: 0.9777\n",
      "Epoch 731/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9810 - val_loss: 0.0546 - val_accuracy: 0.9776\n",
      "Epoch 732/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9810 - val_loss: 0.0546 - val_accuracy: 0.9774\n",
      "Epoch 733/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9806 - val_loss: 0.0523 - val_accuracy: 0.9768\n",
      "Epoch 734/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0616 - val_accuracy: 0.9771\n",
      "Epoch 735/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9799 - val_loss: 0.0619 - val_accuracy: 0.9778\n",
      "Epoch 736/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.0625 - val_accuracy: 0.9773\n",
      "Epoch 737/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9811 - val_loss: 0.0575 - val_accuracy: 0.9778\n",
      "Epoch 738/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.0630 - val_accuracy: 0.9769\n",
      "Epoch 739/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9813 - val_loss: 0.0681 - val_accuracy: 0.9751\n",
      "Epoch 740/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9817 - val_loss: 0.0641 - val_accuracy: 0.9770\n",
      "Epoch 741/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 0.0608 - val_accuracy: 0.9773\n",
      "Epoch 742/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9814 - val_loss: 0.0581 - val_accuracy: 0.9777\n",
      "Epoch 743/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9810 - val_loss: 0.0666 - val_accuracy: 0.9759\n",
      "Epoch 744/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9820 - val_loss: 0.0580 - val_accuracy: 0.9781\n",
      "Epoch 745/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.0629 - val_accuracy: 0.9763\n",
      "Epoch 746/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9818 - val_loss: 0.0581 - val_accuracy: 0.9784\n",
      "Epoch 747/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9815 - val_loss: 0.0618 - val_accuracy: 0.9769\n",
      "Epoch 748/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9811 - val_loss: 0.0683 - val_accuracy: 0.9766\n",
      "Epoch 749/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0569 - val_accuracy: 0.9778\n",
      "Epoch 750/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.0661 - val_accuracy: 0.9758\n",
      "Epoch 751/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9805 - val_loss: 0.0649 - val_accuracy: 0.9760\n",
      "Epoch 752/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9819 - val_loss: 0.0642 - val_accuracy: 0.9766\n",
      "Epoch 753/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9818 - val_loss: 0.0619 - val_accuracy: 0.9769\n",
      "Epoch 754/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9807 - val_loss: 0.0675 - val_accuracy: 0.9760\n",
      "Epoch 755/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9815 - val_loss: 0.0657 - val_accuracy: 0.9757\n",
      "Epoch 756/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9806 - val_loss: 0.0649 - val_accuracy: 0.9765\n",
      "Epoch 757/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9815 - val_loss: 0.0648 - val_accuracy: 0.9769\n",
      "Epoch 758/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9810 - val_loss: 0.0571 - val_accuracy: 0.9770\n",
      "Epoch 759/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9805 - val_loss: 0.0571 - val_accuracy: 0.9776\n",
      "Epoch 760/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9810 - val_loss: 0.0603 - val_accuracy: 0.9778\n",
      "Epoch 761/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 0.0562 - val_accuracy: 0.9781\n",
      "Epoch 762/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9812 - val_loss: 0.0564 - val_accuracy: 0.9774\n",
      "Epoch 763/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9813 - val_loss: 0.0615 - val_accuracy: 0.9773\n",
      "Epoch 764/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9815 - val_loss: 0.0525 - val_accuracy: 0.9788\n",
      "Epoch 765/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9819 - val_loss: 0.0635 - val_accuracy: 0.9767\n",
      "Epoch 766/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9804 - val_loss: 0.0528 - val_accuracy: 0.9773\n",
      "Epoch 767/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.0535 - val_accuracy: 0.9777\n",
      "Epoch 768/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9823 - val_loss: 0.0613 - val_accuracy: 0.9775\n",
      "Epoch 769/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9815 - val_loss: 0.0637 - val_accuracy: 0.9767\n",
      "Epoch 770/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9813 - val_loss: 0.0601 - val_accuracy: 0.9765\n",
      "Epoch 771/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9814 - val_loss: 0.0663 - val_accuracy: 0.9770\n",
      "Epoch 772/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0557 - val_accuracy: 0.9777\n",
      "Epoch 773/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9807 - val_loss: 0.0613 - val_accuracy: 0.9761\n",
      "Epoch 774/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9816 - val_loss: 0.0536 - val_accuracy: 0.9780\n",
      "Epoch 775/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.0619 - val_accuracy: 0.9773\n",
      "Epoch 776/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.0548 - val_accuracy: 0.9790\n",
      "Epoch 777/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9808 - val_loss: 0.0574 - val_accuracy: 0.9781\n",
      "Epoch 778/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9809 - val_loss: 0.0622 - val_accuracy: 0.9763\n",
      "Epoch 779/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9815 - val_loss: 0.0653 - val_accuracy: 0.9763\n",
      "Epoch 780/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9818 - val_loss: 0.0574 - val_accuracy: 0.9769\n",
      "Epoch 781/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0609 - val_accuracy: 0.9761\n",
      "Epoch 782/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9815 - val_loss: 0.0601 - val_accuracy: 0.9774\n",
      "Epoch 783/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9812 - val_loss: 0.0615 - val_accuracy: 0.9771\n",
      "Epoch 784/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9823 - val_loss: 0.0605 - val_accuracy: 0.9774\n",
      "Epoch 785/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 0.0775 - val_accuracy: 0.9741\n",
      "Epoch 786/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9813 - val_loss: 0.0558 - val_accuracy: 0.9778\n",
      "Epoch 787/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9809 - val_loss: 0.0583 - val_accuracy: 0.9765\n",
      "Epoch 788/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9813 - val_loss: 0.0622 - val_accuracy: 0.9763\n",
      "Epoch 789/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9817 - val_loss: 0.0541 - val_accuracy: 0.9790\n",
      "Epoch 790/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9827 - val_loss: 0.0670 - val_accuracy: 0.9747\n",
      "Epoch 791/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9808 - val_loss: 0.0587 - val_accuracy: 0.9771\n",
      "Epoch 792/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 0.0750 - val_accuracy: 0.9747\n",
      "Epoch 793/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9803 - val_loss: 0.0609 - val_accuracy: 0.9770\n",
      "Epoch 794/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9816 - val_loss: 0.0567 - val_accuracy: 0.9771\n",
      "Epoch 795/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.0583 - val_accuracy: 0.9776\n",
      "Epoch 796/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9827 - val_loss: 0.0531 - val_accuracy: 0.9786\n",
      "Epoch 797/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9812 - val_loss: 0.0634 - val_accuracy: 0.9763\n",
      "Epoch 798/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9814 - val_loss: 0.0585 - val_accuracy: 0.9774\n",
      "Epoch 799/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9814 - val_loss: 0.0551 - val_accuracy: 0.9775\n",
      "Epoch 800/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9813 - val_loss: 0.0596 - val_accuracy: 0.9769\n",
      "Epoch 801/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9810 - val_loss: 0.0588 - val_accuracy: 0.9778\n",
      "Epoch 802/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9827 - val_loss: 0.0542 - val_accuracy: 0.9781\n",
      "Epoch 803/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9828 - val_loss: 0.0556 - val_accuracy: 0.9788\n",
      "Epoch 804/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9809 - val_loss: 0.0652 - val_accuracy: 0.9763\n",
      "Epoch 805/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9818 - val_loss: 0.0620 - val_accuracy: 0.9770\n",
      "Epoch 806/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9821 - val_loss: 0.0558 - val_accuracy: 0.9776\n",
      "Epoch 807/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0598 - val_accuracy: 0.9774\n",
      "Epoch 808/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0666 - val_accuracy: 0.9769\n",
      "Epoch 809/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9806 - val_loss: 0.0581 - val_accuracy: 0.9781\n",
      "Epoch 810/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9818 - val_loss: 0.0633 - val_accuracy: 0.9767\n",
      "Epoch 811/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9812 - val_loss: 0.0585 - val_accuracy: 0.9773\n",
      "Epoch 812/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9810 - val_loss: 0.0593 - val_accuracy: 0.9785\n",
      "Epoch 813/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9815 - val_loss: 0.0549 - val_accuracy: 0.9778\n",
      "Epoch 814/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9819 - val_loss: 0.0563 - val_accuracy: 0.9777\n",
      "Epoch 815/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9822 - val_loss: 0.0563 - val_accuracy: 0.9785\n",
      "Epoch 816/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0571 - val_accuracy: 0.9778\n",
      "Epoch 817/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9823 - val_loss: 0.0554 - val_accuracy: 0.9787\n",
      "Epoch 818/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9818 - val_loss: 0.0760 - val_accuracy: 0.9755\n",
      "Epoch 819/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9811 - val_loss: 0.0650 - val_accuracy: 0.9769\n",
      "Epoch 820/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9814 - val_loss: 0.0546 - val_accuracy: 0.9781\n",
      "Epoch 821/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9817 - val_loss: 0.0641 - val_accuracy: 0.9775\n",
      "Epoch 822/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0545 - val_accuracy: 0.9787\n",
      "Epoch 823/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9819 - val_loss: 0.0582 - val_accuracy: 0.9780\n",
      "Epoch 824/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9822 - val_loss: 0.0583 - val_accuracy: 0.9777\n",
      "Epoch 825/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9822 - val_loss: 0.0517 - val_accuracy: 0.9777\n",
      "Epoch 826/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.0597 - val_accuracy: 0.9780\n",
      "Epoch 827/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9818 - val_loss: 0.0598 - val_accuracy: 0.9774\n",
      "Epoch 828/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9819 - val_loss: 0.0554 - val_accuracy: 0.9778\n",
      "Epoch 829/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9813 - val_loss: 0.0573 - val_accuracy: 0.9785\n",
      "Epoch 830/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9821 - val_loss: 0.0546 - val_accuracy: 0.9778\n",
      "Epoch 831/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0562 - val_accuracy: 0.9775\n",
      "Epoch 832/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9815 - val_loss: 0.0570 - val_accuracy: 0.9775\n",
      "Epoch 833/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9818 - val_loss: 0.0581 - val_accuracy: 0.9775\n",
      "Epoch 834/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9814 - val_loss: 0.0666 - val_accuracy: 0.9751\n",
      "Epoch 835/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9808 - val_loss: 0.0605 - val_accuracy: 0.9766\n",
      "Epoch 836/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9825 - val_loss: 0.0688 - val_accuracy: 0.9760\n",
      "Epoch 837/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9826 - val_loss: 0.0601 - val_accuracy: 0.9774\n",
      "Epoch 838/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9816 - val_loss: 0.0602 - val_accuracy: 0.9776\n",
      "Epoch 839/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9820 - val_loss: 0.0662 - val_accuracy: 0.9766\n",
      "Epoch 840/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9812 - val_loss: 0.0593 - val_accuracy: 0.9768\n",
      "Epoch 841/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9830 - val_loss: 0.0522 - val_accuracy: 0.9791\n",
      "Epoch 842/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9828 - val_loss: 0.0575 - val_accuracy: 0.9779\n",
      "Epoch 843/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9825 - val_loss: 0.0627 - val_accuracy: 0.9777\n",
      "Epoch 844/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0614 - val_accuracy: 0.9770\n",
      "Epoch 845/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9819 - val_loss: 0.0561 - val_accuracy: 0.9778\n",
      "Epoch 846/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0678 - val_accuracy: 0.9759\n",
      "Epoch 847/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.0601 - val_accuracy: 0.9787\n",
      "Epoch 848/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0614 - val_accuracy: 0.9768\n",
      "Epoch 849/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9811 - val_loss: 0.0677 - val_accuracy: 0.9756\n",
      "Epoch 850/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9817 - val_loss: 0.0621 - val_accuracy: 0.9767\n",
      "Epoch 851/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9815 - val_loss: 0.0586 - val_accuracy: 0.9781\n",
      "Epoch 852/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9820 - val_loss: 0.0572 - val_accuracy: 0.9761\n",
      "Epoch 853/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9827 - val_loss: 0.0549 - val_accuracy: 0.9763\n",
      "Epoch 854/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.0602 - val_accuracy: 0.9777\n",
      "Epoch 855/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9818 - val_loss: 0.0663 - val_accuracy: 0.9770\n",
      "Epoch 856/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0505 - accuracy: 0.9814 - val_loss: 0.0559 - val_accuracy: 0.9786\n",
      "Epoch 857/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9813 - val_loss: 0.0603 - val_accuracy: 0.9774\n",
      "Epoch 858/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9827 - val_loss: 0.0613 - val_accuracy: 0.9771\n",
      "Epoch 859/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9814 - val_loss: 0.0569 - val_accuracy: 0.9765\n",
      "Epoch 860/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9828 - val_loss: 0.0604 - val_accuracy: 0.9771\n",
      "Epoch 861/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9825 - val_loss: 0.0620 - val_accuracy: 0.9774\n",
      "Epoch 862/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9823 - val_loss: 0.0547 - val_accuracy: 0.9774\n",
      "Epoch 863/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9818 - val_loss: 0.0613 - val_accuracy: 0.9773\n",
      "Epoch 864/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9811 - val_loss: 0.0675 - val_accuracy: 0.9761\n",
      "Epoch 865/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9805 - val_loss: 0.0611 - val_accuracy: 0.9776\n",
      "Epoch 866/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9811 - val_loss: 0.0625 - val_accuracy: 0.9776\n",
      "Epoch 867/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 0.0624 - val_accuracy: 0.9775\n",
      "Epoch 868/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9819 - val_loss: 0.0642 - val_accuracy: 0.9764\n",
      "Epoch 869/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9818 - val_loss: 0.0534 - val_accuracy: 0.9782\n",
      "Epoch 870/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 0.0611 - val_accuracy: 0.9774\n",
      "Epoch 871/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9831 - val_loss: 0.0625 - val_accuracy: 0.9780\n",
      "Epoch 872/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9814 - val_loss: 0.0645 - val_accuracy: 0.9764\n",
      "Epoch 873/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 0.0694 - val_accuracy: 0.9761\n",
      "Epoch 874/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9821 - val_loss: 0.0697 - val_accuracy: 0.9766\n",
      "Epoch 875/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9824 - val_loss: 0.0668 - val_accuracy: 0.9756\n",
      "Epoch 876/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9814 - val_loss: 0.0616 - val_accuracy: 0.9779\n",
      "Epoch 877/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9828 - val_loss: 0.0615 - val_accuracy: 0.9766\n",
      "Epoch 878/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 0.0671 - val_accuracy: 0.9759\n",
      "Epoch 879/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0631 - val_accuracy: 0.9779\n",
      "Epoch 880/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9825 - val_loss: 0.0547 - val_accuracy: 0.9782\n",
      "Epoch 881/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9812 - val_loss: 0.0592 - val_accuracy: 0.9786\n",
      "Epoch 882/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9830 - val_loss: 0.0602 - val_accuracy: 0.9773\n",
      "Epoch 883/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9826 - val_loss: 0.0544 - val_accuracy: 0.9773\n",
      "Epoch 884/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9822 - val_loss: 0.0586 - val_accuracy: 0.9775\n",
      "Epoch 885/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9820 - val_loss: 0.0759 - val_accuracy: 0.9755\n",
      "Epoch 886/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.0646 - val_accuracy: 0.9771\n",
      "Epoch 887/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9809 - val_loss: 0.0535 - val_accuracy: 0.9785\n",
      "Epoch 888/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9824 - val_loss: 0.0564 - val_accuracy: 0.9779\n",
      "Epoch 889/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9823 - val_loss: 0.0542 - val_accuracy: 0.9785\n",
      "Epoch 890/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9821 - val_loss: 0.0577 - val_accuracy: 0.9779\n",
      "Epoch 891/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9819 - val_loss: 0.0648 - val_accuracy: 0.9779\n",
      "Epoch 892/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9814 - val_loss: 0.0573 - val_accuracy: 0.9778\n",
      "Epoch 893/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9801 - val_loss: 0.0598 - val_accuracy: 0.9784\n",
      "Epoch 894/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9820 - val_loss: 0.0570 - val_accuracy: 0.9785\n",
      "Epoch 895/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9826 - val_loss: 0.0687 - val_accuracy: 0.9766\n",
      "Epoch 896/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9828 - val_loss: 0.0570 - val_accuracy: 0.9768\n",
      "Epoch 897/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9817 - val_loss: 0.0640 - val_accuracy: 0.9771\n",
      "Epoch 898/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9823 - val_loss: 0.0613 - val_accuracy: 0.9774\n",
      "Epoch 899/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0664 - val_accuracy: 0.9765\n",
      "Epoch 900/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9825 - val_loss: 0.0554 - val_accuracy: 0.9784\n",
      "Epoch 901/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0476 - accuracy: 0.9826 - val_loss: 0.0611 - val_accuracy: 0.9770\n",
      "Epoch 902/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9816 - val_loss: 0.0558 - val_accuracy: 0.9773\n",
      "Epoch 903/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0695 - val_accuracy: 0.9759\n",
      "Epoch 904/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.0598 - val_accuracy: 0.9781\n",
      "Epoch 905/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9825 - val_loss: 0.0571 - val_accuracy: 0.9781\n",
      "Epoch 906/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9825 - val_loss: 0.0579 - val_accuracy: 0.9775\n",
      "Epoch 907/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9813 - val_loss: 0.0564 - val_accuracy: 0.9770\n",
      "Epoch 908/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0556 - val_accuracy: 0.9778\n",
      "Epoch 909/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9818 - val_loss: 0.0545 - val_accuracy: 0.9774\n",
      "Epoch 910/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9823 - val_loss: 0.0548 - val_accuracy: 0.9778\n",
      "Epoch 911/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.0580 - val_accuracy: 0.9775\n",
      "Epoch 912/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9817 - val_loss: 0.0592 - val_accuracy: 0.9774\n",
      "Epoch 913/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9823 - val_loss: 0.0627 - val_accuracy: 0.9763\n",
      "Epoch 914/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9814 - val_loss: 0.0738 - val_accuracy: 0.9760\n",
      "Epoch 915/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9815 - val_loss: 0.0674 - val_accuracy: 0.9766\n",
      "Epoch 916/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 0.0583 - val_accuracy: 0.9779\n",
      "Epoch 917/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9826 - val_loss: 0.0518 - val_accuracy: 0.9792\n",
      "Epoch 918/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.0624 - val_accuracy: 0.9771\n",
      "Epoch 919/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9822 - val_loss: 0.0648 - val_accuracy: 0.9776\n",
      "Epoch 920/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9823 - val_loss: 0.0664 - val_accuracy: 0.9767\n",
      "Epoch 921/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9813 - val_loss: 0.0531 - val_accuracy: 0.9785\n",
      "Epoch 922/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9822 - val_loss: 0.0594 - val_accuracy: 0.9784\n",
      "Epoch 923/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9819 - val_loss: 0.0552 - val_accuracy: 0.9782\n",
      "Epoch 924/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9822 - val_loss: 0.0671 - val_accuracy: 0.9765\n",
      "Epoch 925/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 0.0612 - val_accuracy: 0.9771\n",
      "Epoch 926/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.9827 - val_loss: 0.0672 - val_accuracy: 0.9760\n",
      "Epoch 927/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9819 - val_loss: 0.0588 - val_accuracy: 0.9779\n",
      "Epoch 928/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9817 - val_loss: 0.0626 - val_accuracy: 0.9775\n",
      "Epoch 929/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9834 - val_loss: 0.0537 - val_accuracy: 0.9780\n",
      "Epoch 930/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9807 - val_loss: 0.0560 - val_accuracy: 0.9779\n",
      "Epoch 931/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9832 - val_loss: 0.0618 - val_accuracy: 0.9769\n",
      "Epoch 932/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9823 - val_loss: 0.0583 - val_accuracy: 0.9775\n",
      "Epoch 933/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 0.0764 - val_accuracy: 0.9747\n",
      "Epoch 934/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9828 - val_loss: 0.0571 - val_accuracy: 0.9777\n",
      "Epoch 935/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9830 - val_loss: 0.0667 - val_accuracy: 0.9764\n",
      "Epoch 936/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9822 - val_loss: 0.0621 - val_accuracy: 0.9784\n",
      "Epoch 937/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9825 - val_loss: 0.0619 - val_accuracy: 0.9776\n",
      "Epoch 938/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9820 - val_loss: 0.0592 - val_accuracy: 0.9775\n",
      "Epoch 939/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9825 - val_loss: 0.0620 - val_accuracy: 0.9775\n",
      "Epoch 940/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 0.0559 - val_accuracy: 0.9787\n",
      "Epoch 941/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9823 - val_loss: 0.0598 - val_accuracy: 0.9784\n",
      "Epoch 942/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9819 - val_loss: 0.0578 - val_accuracy: 0.9778\n",
      "Epoch 943/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9824 - val_loss: 0.0711 - val_accuracy: 0.9763\n",
      "Epoch 944/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9814 - val_loss: 0.0600 - val_accuracy: 0.9775\n",
      "Epoch 945/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9815 - val_loss: 0.0623 - val_accuracy: 0.9777\n",
      "Epoch 946/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9829 - val_loss: 0.0648 - val_accuracy: 0.9766\n",
      "Epoch 947/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9824 - val_loss: 0.0583 - val_accuracy: 0.9774\n",
      "Epoch 948/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9823 - val_loss: 0.0648 - val_accuracy: 0.9767\n",
      "Epoch 949/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9831 - val_loss: 0.0662 - val_accuracy: 0.9760\n",
      "Epoch 950/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.0679 - val_accuracy: 0.9767\n",
      "Epoch 951/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9827 - val_loss: 0.0621 - val_accuracy: 0.9763\n",
      "Epoch 952/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9818 - val_loss: 0.0562 - val_accuracy: 0.9781\n",
      "Epoch 953/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9826 - val_loss: 0.0596 - val_accuracy: 0.9770\n",
      "Epoch 954/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9823 - val_loss: 0.0583 - val_accuracy: 0.9777\n",
      "Epoch 955/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9829 - val_loss: 0.0761 - val_accuracy: 0.9749\n",
      "Epoch 956/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9814 - val_loss: 0.0624 - val_accuracy: 0.9777\n",
      "Epoch 957/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0468 - accuracy: 0.9829 - val_loss: 0.0680 - val_accuracy: 0.9760\n",
      "Epoch 958/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9821 - val_loss: 0.0583 - val_accuracy: 0.9769\n",
      "Epoch 959/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 0.0598 - val_accuracy: 0.9780\n",
      "Epoch 960/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9836 - val_loss: 0.0570 - val_accuracy: 0.9778\n",
      "Epoch 961/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9824 - val_loss: 0.0564 - val_accuracy: 0.9782\n",
      "Epoch 962/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9825 - val_loss: 0.0608 - val_accuracy: 0.9767\n",
      "Epoch 963/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.9834 - val_loss: 0.0545 - val_accuracy: 0.9788\n",
      "Epoch 964/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9823 - val_loss: 0.0586 - val_accuracy: 0.9767\n",
      "Epoch 965/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.0607 - val_accuracy: 0.9774\n",
      "Epoch 966/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9829 - val_loss: 0.0607 - val_accuracy: 0.9776\n",
      "Epoch 967/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9828 - val_loss: 0.0640 - val_accuracy: 0.9763\n",
      "Epoch 968/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9827 - val_loss: 0.0559 - val_accuracy: 0.9785\n",
      "Epoch 969/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9826 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 970/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9827 - val_loss: 0.0628 - val_accuracy: 0.9771\n",
      "Epoch 971/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9821 - val_loss: 0.0560 - val_accuracy: 0.9775\n",
      "Epoch 972/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9818 - val_loss: 0.0626 - val_accuracy: 0.9771\n",
      "Epoch 973/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9820 - val_loss: 0.0560 - val_accuracy: 0.9774\n",
      "Epoch 974/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0452 - accuracy: 0.9836 - val_loss: 0.0621 - val_accuracy: 0.9767\n",
      "Epoch 975/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.0651 - val_accuracy: 0.9766\n",
      "Epoch 976/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9820 - val_loss: 0.0607 - val_accuracy: 0.9768\n",
      "Epoch 977/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.0677 - val_accuracy: 0.9769\n",
      "Epoch 978/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9830 - val_loss: 0.0636 - val_accuracy: 0.9766\n",
      "Epoch 979/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9818 - val_loss: 0.0655 - val_accuracy: 0.9765\n",
      "Epoch 980/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9824 - val_loss: 0.0651 - val_accuracy: 0.9770\n",
      "Epoch 981/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9829 - val_loss: 0.0622 - val_accuracy: 0.9781\n",
      "Epoch 982/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9832 - val_loss: 0.0630 - val_accuracy: 0.9782\n",
      "Epoch 983/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9823 - val_loss: 0.0629 - val_accuracy: 0.9777\n",
      "Epoch 984/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9832 - val_loss: 0.0736 - val_accuracy: 0.9755\n",
      "Epoch 985/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9819 - val_loss: 0.0614 - val_accuracy: 0.9771\n",
      "Epoch 986/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9826 - val_loss: 0.0634 - val_accuracy: 0.9777\n",
      "Epoch 987/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9832 - val_loss: 0.0688 - val_accuracy: 0.9767\n",
      "Epoch 988/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9832 - val_loss: 0.0624 - val_accuracy: 0.9775\n",
      "Epoch 989/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9826 - val_loss: 0.0552 - val_accuracy: 0.9782\n",
      "Epoch 990/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9824 - val_loss: 0.0547 - val_accuracy: 0.9781\n",
      "Epoch 991/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9815 - val_loss: 0.0633 - val_accuracy: 0.9773\n",
      "Epoch 992/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 0.0597 - val_accuracy: 0.9771\n",
      "Epoch 993/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9829 - val_loss: 0.0648 - val_accuracy: 0.9766\n",
      "Epoch 994/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9825 - val_loss: 0.0567 - val_accuracy: 0.9777\n",
      "Epoch 995/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9830 - val_loss: 0.0620 - val_accuracy: 0.9774\n",
      "Epoch 996/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 0.0682 - val_accuracy: 0.9768\n",
      "Epoch 997/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9831 - val_loss: 0.0581 - val_accuracy: 0.9776\n",
      "Epoch 998/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9827 - val_loss: 0.0594 - val_accuracy: 0.9778\n",
      "Epoch 999/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9827 - val_loss: 0.0711 - val_accuracy: 0.9765\n",
      "Epoch 1000/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9829 - val_loss: 0.0620 - val_accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 996us/step\n",
      "xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,)\n",
      "\n",
      "classification_report_Fold=2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       1.00      0.98      0.99      8725\n",
      " Anomalous 1       0.58      0.88      0.70       287\n",
      "\n",
      "    accuracy                           0.98      9012\n",
      "   macro avg       0.79      0.93      0.84      9012\n",
      "weighted avg       0.98      0.98      0.98      9012\n",
      "\n",
      "confusion_matrix_Fold=2:\n",
      "\n",
      "True Negatives:  8540\n",
      "False Positives:  185\n",
      "False Negatives:  35\n",
      "True Positives:  252\n",
      "accuracy_score_Fold=2:\n",
      " 8792 \n",
      "\n",
      "End running time Fold=2: 210214_095011 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=3: 210214_095011 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 1.0324411145373457e-05, min: 2.940464134534943e-06, max: 0.00013972056179266794\n",
      "End running time Data Augmentation_Fold=3: 210214_100939 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=3: 210214_100939 ,-------------------------- \n",
      "\n",
      "\n",
      " Number of Final yXtrain_Fold=3 labeled 0: 69796\n",
      "Number of Final yXtrain_Fold=3 labeled 1: 16072\n",
      "Number of Final yXtrain_Fold=3 labeled 2: 0 \n",
      "\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.4825 - accuracy: 0.8081 - val_loss: 0.2389 - val_accuracy: 0.9613\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8230 - val_loss: 0.2337 - val_accuracy: 0.9564\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8231 - val_loss: 0.2426 - val_accuracy: 0.9514\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4208 - accuracy: 0.8238 - val_loss: 0.2085 - val_accuracy: 0.9542\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4133 - accuracy: 0.8269 - val_loss: 0.2245 - val_accuracy: 0.9519\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4120 - accuracy: 0.8256 - val_loss: 0.2208 - val_accuracy: 0.9517\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4076 - accuracy: 0.8271 - val_loss: 0.2079 - val_accuracy: 0.9526\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8316 - val_loss: 0.2107 - val_accuracy: 0.9497\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8309 - val_loss: 0.2292 - val_accuracy: 0.9442\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8308 - val_loss: 0.2382 - val_accuracy: 0.9438\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3849 - accuracy: 0.8335 - val_loss: 0.2075 - val_accuracy: 0.9471\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8331 - val_loss: 0.2169 - val_accuracy: 0.9427\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8347 - val_loss: 0.2113 - val_accuracy: 0.9447\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8359 - val_loss: 0.2200 - val_accuracy: 0.9412\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8373 - val_loss: 0.2060 - val_accuracy: 0.9414\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8377 - val_loss: 0.2132 - val_accuracy: 0.9410\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8399 - val_loss: 0.1964 - val_accuracy: 0.9453\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8407 - val_loss: 0.2028 - val_accuracy: 0.9420\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8396 - val_loss: 0.1786 - val_accuracy: 0.9507\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8427 - val_loss: 0.2167 - val_accuracy: 0.9382\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8471 - val_loss: 0.1759 - val_accuracy: 0.9486\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8474 - val_loss: 0.1763 - val_accuracy: 0.9500\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.8490 - val_loss: 0.1894 - val_accuracy: 0.9455\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8512 - val_loss: 0.1902 - val_accuracy: 0.9403\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8512 - val_loss: 0.1927 - val_accuracy: 0.9410\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8529 - val_loss: 0.1983 - val_accuracy: 0.9390\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8544 - val_loss: 0.2121 - val_accuracy: 0.9341\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.8563 - val_loss: 0.1943 - val_accuracy: 0.9411\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.8560 - val_loss: 0.1652 - val_accuracy: 0.9511\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3074 - accuracy: 0.8588 - val_loss: 0.1984 - val_accuracy: 0.9362\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8593 - val_loss: 0.1900 - val_accuracy: 0.9400\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8605 - val_loss: 0.1947 - val_accuracy: 0.9364\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8640 - val_loss: 0.1954 - val_accuracy: 0.9366\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8644 - val_loss: 0.1972 - val_accuracy: 0.9325\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8644 - val_loss: 0.1638 - val_accuracy: 0.9494\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2882 - accuracy: 0.8657 - val_loss: 0.1728 - val_accuracy: 0.9435\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8691 - val_loss: 0.1624 - val_accuracy: 0.9492\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8694 - val_loss: 0.1924 - val_accuracy: 0.9357\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.8716 - val_loss: 0.1731 - val_accuracy: 0.9418\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.8715 - val_loss: 0.1690 - val_accuracy: 0.9437\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.8749 - val_loss: 0.1511 - val_accuracy: 0.9488\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8762 - val_loss: 0.1838 - val_accuracy: 0.9371\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8779 - val_loss: 0.1600 - val_accuracy: 0.9454\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8788 - val_loss: 0.1798 - val_accuracy: 0.9377\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8804 - val_loss: 0.1614 - val_accuracy: 0.9426\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8806 - val_loss: 0.1657 - val_accuracy: 0.9401\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8821 - val_loss: 0.1648 - val_accuracy: 0.9410\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2469 - accuracy: 0.8840 - val_loss: 0.1683 - val_accuracy: 0.9390\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8871 - val_loss: 0.1405 - val_accuracy: 0.9503\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.8890 - val_loss: 0.1392 - val_accuracy: 0.9522\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.8887 - val_loss: 0.1653 - val_accuracy: 0.9395\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.8884 - val_loss: 0.1588 - val_accuracy: 0.9409\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.8933 - val_loss: 0.1563 - val_accuracy: 0.9418\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.8949 - val_loss: 0.1542 - val_accuracy: 0.9433\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.8986 - val_loss: 0.1624 - val_accuracy: 0.9379\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.8976 - val_loss: 0.1480 - val_accuracy: 0.9437\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8997 - val_loss: 0.1309 - val_accuracy: 0.9534\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.9030 - val_loss: 0.1328 - val_accuracy: 0.9516\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9030 - val_loss: 0.1376 - val_accuracy: 0.9513\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9062 - val_loss: 0.1379 - val_accuracy: 0.9493\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9083 - val_loss: 0.1509 - val_accuracy: 0.9424\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9109 - val_loss: 0.1509 - val_accuracy: 0.9416\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9090 - val_loss: 0.1424 - val_accuracy: 0.9470\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2023 - accuracy: 0.9142 - val_loss: 0.1400 - val_accuracy: 0.9464\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2009 - accuracy: 0.9149 - val_loss: 0.1275 - val_accuracy: 0.9525\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9152 - val_loss: 0.1246 - val_accuracy: 0.9528\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9169 - val_loss: 0.1474 - val_accuracy: 0.9415\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.9155 - val_loss: 0.1403 - val_accuracy: 0.9453\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.9193 - val_loss: 0.1496 - val_accuracy: 0.9374\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9192 - val_loss: 0.1373 - val_accuracy: 0.9458\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9216 - val_loss: 0.1208 - val_accuracy: 0.9536\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.9228 - val_loss: 0.1385 - val_accuracy: 0.9447\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9242 - val_loss: 0.1203 - val_accuracy: 0.9522\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9222 - val_loss: 0.1189 - val_accuracy: 0.9525\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9243 - val_loss: 0.1080 - val_accuracy: 0.9578\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9278 - val_loss: 0.1073 - val_accuracy: 0.9583\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1766 - accuracy: 0.9282 - val_loss: 0.1225 - val_accuracy: 0.9513\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9282 - val_loss: 0.1072 - val_accuracy: 0.9571\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9291 - val_loss: 0.0965 - val_accuracy: 0.9605\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9291 - val_loss: 0.1169 - val_accuracy: 0.9536\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9319 - val_loss: 0.1003 - val_accuracy: 0.9599\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9314 - val_loss: 0.1154 - val_accuracy: 0.9538\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1678 - accuracy: 0.9334 - val_loss: 0.1097 - val_accuracy: 0.9554\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1663 - accuracy: 0.9332 - val_loss: 0.1051 - val_accuracy: 0.9568\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9328 - val_loss: 0.1180 - val_accuracy: 0.9501\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9353 - val_loss: 0.1041 - val_accuracy: 0.9566\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9349 - val_loss: 0.1052 - val_accuracy: 0.9556\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9385 - val_loss: 0.1029 - val_accuracy: 0.9573\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9374 - val_loss: 0.1102 - val_accuracy: 0.9534\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.9386 - val_loss: 0.0984 - val_accuracy: 0.9591\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.9383 - val_loss: 0.1083 - val_accuracy: 0.9548\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9404 - val_loss: 0.1120 - val_accuracy: 0.9524\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9421 - val_loss: 0.1154 - val_accuracy: 0.9514\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9391 - val_loss: 0.0979 - val_accuracy: 0.9578\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9414 - val_loss: 0.1148 - val_accuracy: 0.9506\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9421 - val_loss: 0.1148 - val_accuracy: 0.9486\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9421 - val_loss: 0.1085 - val_accuracy: 0.9534\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1480 - accuracy: 0.9427 - val_loss: 0.1003 - val_accuracy: 0.9569\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9425 - val_loss: 0.1136 - val_accuracy: 0.9504\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9445 - val_loss: 0.1094 - val_accuracy: 0.9531\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9444 - val_loss: 0.1185 - val_accuracy: 0.9495\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.9456 - val_loss: 0.1000 - val_accuracy: 0.9556\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9445 - val_loss: 0.1058 - val_accuracy: 0.9552\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9436 - val_loss: 0.0995 - val_accuracy: 0.9571\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9462 - val_loss: 0.1131 - val_accuracy: 0.9509\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1411 - accuracy: 0.9445 - val_loss: 0.0935 - val_accuracy: 0.9596\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9478 - val_loss: 0.1026 - val_accuracy: 0.9551\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9474 - val_loss: 0.0987 - val_accuracy: 0.9581\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9471 - val_loss: 0.0894 - val_accuracy: 0.9619\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9483 - val_loss: 0.1053 - val_accuracy: 0.9546\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9482 - val_loss: 0.0865 - val_accuracy: 0.9629\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9495 - val_loss: 0.0943 - val_accuracy: 0.9581\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9484 - val_loss: 0.1050 - val_accuracy: 0.9542\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9487 - val_loss: 0.0972 - val_accuracy: 0.9573\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9492 - val_loss: 0.0883 - val_accuracy: 0.9608\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9505 - val_loss: 0.1229 - val_accuracy: 0.9464\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9499 - val_loss: 0.0840 - val_accuracy: 0.9626\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9505 - val_loss: 0.0877 - val_accuracy: 0.9616\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9506 - val_loss: 0.0785 - val_accuracy: 0.9639\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.9487 - val_loss: 0.1026 - val_accuracy: 0.9561\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9526 - val_loss: 0.1135 - val_accuracy: 0.9506\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9526 - val_loss: 0.0937 - val_accuracy: 0.9591\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9505 - val_loss: 0.0901 - val_accuracy: 0.9599\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9526 - val_loss: 0.1067 - val_accuracy: 0.9528\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9523 - val_loss: 0.0887 - val_accuracy: 0.9610\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9533 - val_loss: 0.0904 - val_accuracy: 0.9591\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1212 - accuracy: 0.9536 - val_loss: 0.0896 - val_accuracy: 0.9612\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9519 - val_loss: 0.0866 - val_accuracy: 0.9606\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9521 - val_loss: 0.0841 - val_accuracy: 0.9614\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9537 - val_loss: 0.0844 - val_accuracy: 0.9630\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.9544 - val_loss: 0.0782 - val_accuracy: 0.9648\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9544 - val_loss: 0.0770 - val_accuracy: 0.9663\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9551 - val_loss: 0.1066 - val_accuracy: 0.9532\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9557 - val_loss: 0.0966 - val_accuracy: 0.9566\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9549 - val_loss: 0.0824 - val_accuracy: 0.9623\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9555 - val_loss: 0.0866 - val_accuracy: 0.9632\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9547 - val_loss: 0.0968 - val_accuracy: 0.9564\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9544 - val_loss: 0.0831 - val_accuracy: 0.9618\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9563 - val_loss: 0.0908 - val_accuracy: 0.9597\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1145 - accuracy: 0.9564 - val_loss: 0.0889 - val_accuracy: 0.9608\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.9571 - val_loss: 0.0902 - val_accuracy: 0.9585\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9570 - val_loss: 0.0822 - val_accuracy: 0.9638\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9556 - val_loss: 0.0908 - val_accuracy: 0.9592\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9579 - val_loss: 0.0748 - val_accuracy: 0.9667\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9578 - val_loss: 0.1078 - val_accuracy: 0.9529\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1115 - accuracy: 0.9570 - val_loss: 0.0945 - val_accuracy: 0.9577\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.9589 - val_loss: 0.0836 - val_accuracy: 0.9622\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9575 - val_loss: 0.0875 - val_accuracy: 0.9618\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1094 - accuracy: 0.9593 - val_loss: 0.0735 - val_accuracy: 0.9668\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9588 - val_loss: 0.0685 - val_accuracy: 0.9700\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1098 - accuracy: 0.9576 - val_loss: 0.0849 - val_accuracy: 0.9620\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.9602 - val_loss: 0.0984 - val_accuracy: 0.9564\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9601 - val_loss: 0.0798 - val_accuracy: 0.9644\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1094 - accuracy: 0.9589 - val_loss: 0.1003 - val_accuracy: 0.9551\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1084 - accuracy: 0.9589 - val_loss: 0.0921 - val_accuracy: 0.9587\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9592 - val_loss: 0.0895 - val_accuracy: 0.9603\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9603 - val_loss: 0.0795 - val_accuracy: 0.9653\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9600 - val_loss: 0.0791 - val_accuracy: 0.9648\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9598 - val_loss: 0.0899 - val_accuracy: 0.9596\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9612 - val_loss: 0.0779 - val_accuracy: 0.9649\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9606 - val_loss: 0.0785 - val_accuracy: 0.9647\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9601 - val_loss: 0.0850 - val_accuracy: 0.9626\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9605 - val_loss: 0.0723 - val_accuracy: 0.9675\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9617 - val_loss: 0.0796 - val_accuracy: 0.9637\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.9606 - val_loss: 0.0854 - val_accuracy: 0.9622\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9617 - val_loss: 0.0714 - val_accuracy: 0.9668\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9626 - val_loss: 0.0801 - val_accuracy: 0.9635\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9611 - val_loss: 0.0911 - val_accuracy: 0.9598\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9619 - val_loss: 0.0911 - val_accuracy: 0.9600\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.9619 - val_loss: 0.0867 - val_accuracy: 0.9620\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9628 - val_loss: 0.0702 - val_accuracy: 0.9682\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9618 - val_loss: 0.0774 - val_accuracy: 0.9660\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9637 - val_loss: 0.0775 - val_accuracy: 0.9652\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9626 - val_loss: 0.0834 - val_accuracy: 0.9627\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9627 - val_loss: 0.0856 - val_accuracy: 0.9624\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9618 - val_loss: 0.0811 - val_accuracy: 0.9633\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9640 - val_loss: 0.0782 - val_accuracy: 0.9640\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9628 - val_loss: 0.0704 - val_accuracy: 0.9686\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9645 - val_loss: 0.0744 - val_accuracy: 0.9672\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9628 - val_loss: 0.0735 - val_accuracy: 0.9675\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9633 - val_loss: 0.0738 - val_accuracy: 0.9667\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9634 - val_loss: 0.0837 - val_accuracy: 0.9627\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9635 - val_loss: 0.0839 - val_accuracy: 0.9617\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9640 - val_loss: 0.0898 - val_accuracy: 0.9606\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9626 - val_loss: 0.0803 - val_accuracy: 0.9636\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9632 - val_loss: 0.0779 - val_accuracy: 0.9652\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9644 - val_loss: 0.0758 - val_accuracy: 0.9656\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9657 - val_loss: 0.0800 - val_accuracy: 0.9628\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0961 - accuracy: 0.9644 - val_loss: 0.0750 - val_accuracy: 0.9647\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9654 - val_loss: 0.0864 - val_accuracy: 0.9614\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9660 - val_loss: 0.0773 - val_accuracy: 0.9663\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.9662 - val_loss: 0.0728 - val_accuracy: 0.9667\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9655 - val_loss: 0.0894 - val_accuracy: 0.9598\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0948 - accuracy: 0.9651 - val_loss: 0.0833 - val_accuracy: 0.9622\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9646 - val_loss: 0.0689 - val_accuracy: 0.9673\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9654 - val_loss: 0.0852 - val_accuracy: 0.9607\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9655 - val_loss: 0.0778 - val_accuracy: 0.9642\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9659 - val_loss: 0.0818 - val_accuracy: 0.9640\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9674 - val_loss: 0.0757 - val_accuracy: 0.9659\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9658 - val_loss: 0.0752 - val_accuracy: 0.9665\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9650 - val_loss: 0.0681 - val_accuracy: 0.9684\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9670 - val_loss: 0.0726 - val_accuracy: 0.9662\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9661 - val_loss: 0.0657 - val_accuracy: 0.9701\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9662 - val_loss: 0.0740 - val_accuracy: 0.9667\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9683 - val_loss: 0.0750 - val_accuracy: 0.9668\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.9669 - val_loss: 0.0862 - val_accuracy: 0.9614\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.9680 - val_loss: 0.0814 - val_accuracy: 0.9629\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9664 - val_loss: 0.0715 - val_accuracy: 0.9675\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9658 - val_loss: 0.0634 - val_accuracy: 0.9716\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9672 - val_loss: 0.0820 - val_accuracy: 0.9636\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9678 - val_loss: 0.0661 - val_accuracy: 0.9694\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9670 - val_loss: 0.0678 - val_accuracy: 0.9709\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9683 - val_loss: 0.0751 - val_accuracy: 0.9656\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9687 - val_loss: 0.0713 - val_accuracy: 0.9689\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9690 - val_loss: 0.0853 - val_accuracy: 0.9618\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9685 - val_loss: 0.0723 - val_accuracy: 0.9676\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9677 - val_loss: 0.0786 - val_accuracy: 0.9654\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9674 - val_loss: 0.0818 - val_accuracy: 0.9646\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.9688 - val_loss: 0.0729 - val_accuracy: 0.9673\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9693 - val_loss: 0.0756 - val_accuracy: 0.9670\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9695 - val_loss: 0.0821 - val_accuracy: 0.9634\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9690 - val_loss: 0.0822 - val_accuracy: 0.9625\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9687 - val_loss: 0.0597 - val_accuracy: 0.9738\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9690 - val_loss: 0.0753 - val_accuracy: 0.9647\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9704 - val_loss: 0.0799 - val_accuracy: 0.9642\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9695 - val_loss: 0.0647 - val_accuracy: 0.9714\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9696 - val_loss: 0.0828 - val_accuracy: 0.9636\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9685 - val_loss: 0.0729 - val_accuracy: 0.9684\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9706 - val_loss: 0.0696 - val_accuracy: 0.9679\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9695 - val_loss: 0.0706 - val_accuracy: 0.9689\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9698 - val_loss: 0.0893 - val_accuracy: 0.9613\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9708 - val_loss: 0.0673 - val_accuracy: 0.9701\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9694 - val_loss: 0.0592 - val_accuracy: 0.9739\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9710 - val_loss: 0.0737 - val_accuracy: 0.9659\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9699 - val_loss: 0.0672 - val_accuracy: 0.9706\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9709 - val_loss: 0.0678 - val_accuracy: 0.9696\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9694 - val_loss: 0.0726 - val_accuracy: 0.9682\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9704 - val_loss: 0.0766 - val_accuracy: 0.9668\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9706 - val_loss: 0.0694 - val_accuracy: 0.9696\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9699 - val_loss: 0.0777 - val_accuracy: 0.9656\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9698 - val_loss: 0.0639 - val_accuracy: 0.9728\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9709 - val_loss: 0.0672 - val_accuracy: 0.9687\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.9715 - val_loss: 0.0683 - val_accuracy: 0.9705\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9715 - val_loss: 0.0780 - val_accuracy: 0.9656\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9706 - val_loss: 0.0731 - val_accuracy: 0.9680\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9721 - val_loss: 0.0686 - val_accuracy: 0.9703\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9702 - val_loss: 0.0715 - val_accuracy: 0.9685\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9713 - val_loss: 0.0543 - val_accuracy: 0.9765\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9692 - val_loss: 0.0614 - val_accuracy: 0.9731\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0798 - accuracy: 0.9727 - val_loss: 0.0739 - val_accuracy: 0.9677\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9724 - val_loss: 0.0713 - val_accuracy: 0.9679\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9719 - val_loss: 0.0666 - val_accuracy: 0.9711\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9715 - val_loss: 0.0708 - val_accuracy: 0.9679\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9707 - val_loss: 0.0734 - val_accuracy: 0.9684\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9717 - val_loss: 0.0947 - val_accuracy: 0.9599\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9722 - val_loss: 0.0704 - val_accuracy: 0.9706\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9711 - val_loss: 0.0823 - val_accuracy: 0.9644\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9711 - val_loss: 0.0652 - val_accuracy: 0.9708\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9727 - val_loss: 0.0943 - val_accuracy: 0.9587\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9706 - val_loss: 0.0652 - val_accuracy: 0.9725\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9719 - val_loss: 0.0679 - val_accuracy: 0.9697\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9728 - val_loss: 0.0678 - val_accuracy: 0.9700\n",
      "Epoch 263/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9734 - val_loss: 0.0696 - val_accuracy: 0.9699\n",
      "Epoch 264/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9720 - val_loss: 0.0750 - val_accuracy: 0.9674\n",
      "Epoch 265/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9733 - val_loss: 0.0775 - val_accuracy: 0.9670\n",
      "Epoch 266/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9711 - val_loss: 0.0647 - val_accuracy: 0.9717\n",
      "Epoch 267/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9729 - val_loss: 0.0629 - val_accuracy: 0.9743\n",
      "Epoch 268/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9727 - val_loss: 0.0619 - val_accuracy: 0.9747\n",
      "Epoch 269/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9728 - val_loss: 0.0601 - val_accuracy: 0.9750\n",
      "Epoch 270/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9727 - val_loss: 0.0645 - val_accuracy: 0.9723\n",
      "Epoch 271/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9731 - val_loss: 0.0676 - val_accuracy: 0.9701\n",
      "Epoch 272/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9727 - val_loss: 0.0691 - val_accuracy: 0.9710\n",
      "Epoch 273/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9731 - val_loss: 0.0692 - val_accuracy: 0.9709\n",
      "Epoch 274/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9730 - val_loss: 0.0745 - val_accuracy: 0.9680\n",
      "Epoch 275/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9732 - val_loss: 0.0618 - val_accuracy: 0.9727\n",
      "Epoch 276/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9735 - val_loss: 0.0703 - val_accuracy: 0.9697\n",
      "Epoch 277/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9724 - val_loss: 0.0650 - val_accuracy: 0.9726\n",
      "Epoch 278/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.0634 - val_accuracy: 0.9739\n",
      "Epoch 279/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9727 - val_loss: 0.0687 - val_accuracy: 0.9714\n",
      "Epoch 280/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9724 - val_loss: 0.0736 - val_accuracy: 0.9688\n",
      "Epoch 281/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9730 - val_loss: 0.0680 - val_accuracy: 0.9720\n",
      "Epoch 282/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9737 - val_loss: 0.0711 - val_accuracy: 0.9703\n",
      "Epoch 283/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9740 - val_loss: 0.0826 - val_accuracy: 0.9653\n",
      "Epoch 284/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9726 - val_loss: 0.0918 - val_accuracy: 0.9600\n",
      "Epoch 285/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9730 - val_loss: 0.0780 - val_accuracy: 0.9672\n",
      "Epoch 286/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9727 - val_loss: 0.0759 - val_accuracy: 0.9685\n",
      "Epoch 287/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9739 - val_loss: 0.0602 - val_accuracy: 0.9745\n",
      "Epoch 288/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9735 - val_loss: 0.0626 - val_accuracy: 0.9740\n",
      "Epoch 289/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9733 - val_loss: 0.0624 - val_accuracy: 0.9731\n",
      "Epoch 290/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9738 - val_loss: 0.0594 - val_accuracy: 0.9734\n",
      "Epoch 291/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9727 - val_loss: 0.0672 - val_accuracy: 0.9716\n",
      "Epoch 292/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9730 - val_loss: 0.0625 - val_accuracy: 0.9734\n",
      "Epoch 293/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9735 - val_loss: 0.0678 - val_accuracy: 0.9723\n",
      "Epoch 294/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9736 - val_loss: 0.0746 - val_accuracy: 0.9687\n",
      "Epoch 295/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0757 - accuracy: 0.9725 - val_loss: 0.0572 - val_accuracy: 0.9763\n",
      "Epoch 296/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9729 - val_loss: 0.0684 - val_accuracy: 0.9711\n",
      "Epoch 297/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9736 - val_loss: 0.0812 - val_accuracy: 0.9657\n",
      "Epoch 298/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9739 - val_loss: 0.0705 - val_accuracy: 0.9710\n",
      "Epoch 299/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9746 - val_loss: 0.0752 - val_accuracy: 0.9678\n",
      "Epoch 300/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9743 - val_loss: 0.0753 - val_accuracy: 0.9676\n",
      "Epoch 301/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9734 - val_loss: 0.0747 - val_accuracy: 0.9685\n",
      "Epoch 302/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9730 - val_loss: 0.0668 - val_accuracy: 0.9720\n",
      "Epoch 303/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9747 - val_loss: 0.0580 - val_accuracy: 0.9760\n",
      "Epoch 304/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9728 - val_loss: 0.0568 - val_accuracy: 0.9761\n",
      "Epoch 305/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9742 - val_loss: 0.0629 - val_accuracy: 0.9736\n",
      "Epoch 306/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9730 - val_loss: 0.0648 - val_accuracy: 0.9715\n",
      "Epoch 307/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9745 - val_loss: 0.0499 - val_accuracy: 0.9785\n",
      "Epoch 308/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9749 - val_loss: 0.0604 - val_accuracy: 0.9744\n",
      "Epoch 309/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9751 - val_loss: 0.0714 - val_accuracy: 0.9697\n",
      "Epoch 310/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9755 - val_loss: 0.0651 - val_accuracy: 0.9719\n",
      "Epoch 311/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9757 - val_loss: 0.0752 - val_accuracy: 0.9684\n",
      "Epoch 312/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9755 - val_loss: 0.0680 - val_accuracy: 0.9726\n",
      "Epoch 313/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9752 - val_loss: 0.0657 - val_accuracy: 0.9731\n",
      "Epoch 314/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9737 - val_loss: 0.0656 - val_accuracy: 0.9736\n",
      "Epoch 315/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9753 - val_loss: 0.0780 - val_accuracy: 0.9676\n",
      "Epoch 316/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9748 - val_loss: 0.0579 - val_accuracy: 0.9759\n",
      "Epoch 317/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9749 - val_loss: 0.0680 - val_accuracy: 0.9715\n",
      "Epoch 318/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9748 - val_loss: 0.0633 - val_accuracy: 0.9745\n",
      "Epoch 319/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9757 - val_loss: 0.0665 - val_accuracy: 0.9723\n",
      "Epoch 320/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9751 - val_loss: 0.0633 - val_accuracy: 0.9730\n",
      "Epoch 321/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9743 - val_loss: 0.0540 - val_accuracy: 0.9777\n",
      "Epoch 322/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9746 - val_loss: 0.0683 - val_accuracy: 0.9721\n",
      "Epoch 323/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 0.0613 - val_accuracy: 0.9737\n",
      "Epoch 324/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.0622 - val_accuracy: 0.9739\n",
      "Epoch 325/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9749 - val_loss: 0.0619 - val_accuracy: 0.9741\n",
      "Epoch 326/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9746 - val_loss: 0.0618 - val_accuracy: 0.9755\n",
      "Epoch 327/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9766 - val_loss: 0.0725 - val_accuracy: 0.9696\n",
      "Epoch 328/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9749 - val_loss: 0.0669 - val_accuracy: 0.9724\n",
      "Epoch 329/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9745 - val_loss: 0.0657 - val_accuracy: 0.9723\n",
      "Epoch 330/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9753 - val_loss: 0.0834 - val_accuracy: 0.9659\n",
      "Epoch 331/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9756 - val_loss: 0.0643 - val_accuracy: 0.9733\n",
      "Epoch 332/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9753 - val_loss: 0.0690 - val_accuracy: 0.9723\n",
      "Epoch 333/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9756 - val_loss: 0.0565 - val_accuracy: 0.9763\n",
      "Epoch 334/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9766 - val_loss: 0.0700 - val_accuracy: 0.9710\n",
      "Epoch 335/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.0580 - val_accuracy: 0.9754\n",
      "Epoch 336/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9750 - val_loss: 0.0674 - val_accuracy: 0.9723\n",
      "Epoch 337/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9745 - val_loss: 0.0584 - val_accuracy: 0.9764\n",
      "Epoch 338/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9761 - val_loss: 0.0699 - val_accuracy: 0.9724\n",
      "Epoch 339/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9751 - val_loss: 0.0598 - val_accuracy: 0.9743\n",
      "Epoch 340/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9756 - val_loss: 0.0777 - val_accuracy: 0.9679\n",
      "Epoch 341/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9748 - val_loss: 0.0685 - val_accuracy: 0.9716\n",
      "Epoch 342/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9760 - val_loss: 0.0678 - val_accuracy: 0.9729\n",
      "Epoch 343/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.0698 - val_accuracy: 0.9718\n",
      "Epoch 344/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9752 - val_loss: 0.0635 - val_accuracy: 0.9736\n",
      "Epoch 345/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9745 - val_loss: 0.0576 - val_accuracy: 0.9759\n",
      "Epoch 346/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9769 - val_loss: 0.0725 - val_accuracy: 0.9694\n",
      "Epoch 347/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9760 - val_loss: 0.0726 - val_accuracy: 0.9709\n",
      "Epoch 348/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0669 - val_accuracy: 0.9730\n",
      "Epoch 349/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9748 - val_loss: 0.0628 - val_accuracy: 0.9729\n",
      "Epoch 350/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9757 - val_loss: 0.0647 - val_accuracy: 0.9735\n",
      "Epoch 351/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0685 - accuracy: 0.9767 - val_loss: 0.0636 - val_accuracy: 0.9744\n",
      "Epoch 352/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9761 - val_loss: 0.0610 - val_accuracy: 0.9765\n",
      "Epoch 353/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.0649 - val_accuracy: 0.9730\n",
      "Epoch 354/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9766 - val_loss: 0.0528 - val_accuracy: 0.9763\n",
      "Epoch 355/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9753 - val_loss: 0.0682 - val_accuracy: 0.9725\n",
      "Epoch 356/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9759 - val_loss: 0.0682 - val_accuracy: 0.9723\n",
      "Epoch 357/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9752 - val_loss: 0.0758 - val_accuracy: 0.9691\n",
      "Epoch 358/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 0.0670 - val_accuracy: 0.9735\n",
      "Epoch 359/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9749 - val_loss: 0.0676 - val_accuracy: 0.9724\n",
      "Epoch 360/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9754 - val_loss: 0.0733 - val_accuracy: 0.9689\n",
      "Epoch 361/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9757 - val_loss: 0.0675 - val_accuracy: 0.9725\n",
      "Epoch 362/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9763 - val_loss: 0.0624 - val_accuracy: 0.9721\n",
      "Epoch 363/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9769 - val_loss: 0.0727 - val_accuracy: 0.9704\n",
      "Epoch 364/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9753 - val_loss: 0.0615 - val_accuracy: 0.9748\n",
      "Epoch 365/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9761 - val_loss: 0.0642 - val_accuracy: 0.9728\n",
      "Epoch 366/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9767 - val_loss: 0.0706 - val_accuracy: 0.9723\n",
      "Epoch 367/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9760 - val_loss: 0.0589 - val_accuracy: 0.9765\n",
      "Epoch 368/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9773 - val_loss: 0.0780 - val_accuracy: 0.9682\n",
      "Epoch 369/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9771 - val_loss: 0.0692 - val_accuracy: 0.9718\n",
      "Epoch 370/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9746 - val_loss: 0.0701 - val_accuracy: 0.9713\n",
      "Epoch 371/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9762 - val_loss: 0.0734 - val_accuracy: 0.9709\n",
      "Epoch 372/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.0704 - val_accuracy: 0.9711\n",
      "Epoch 373/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9762 - val_loss: 0.0883 - val_accuracy: 0.9660\n",
      "Epoch 374/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9759 - val_loss: 0.0629 - val_accuracy: 0.9731\n",
      "Epoch 375/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9767 - val_loss: 0.0624 - val_accuracy: 0.9741\n",
      "Epoch 376/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9777 - val_loss: 0.0645 - val_accuracy: 0.9730\n",
      "Epoch 377/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9774 - val_loss: 0.0615 - val_accuracy: 0.9753\n",
      "Epoch 378/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9771 - val_loss: 0.0566 - val_accuracy: 0.9758\n",
      "Epoch 379/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9766 - val_loss: 0.0632 - val_accuracy: 0.9737\n",
      "Epoch 380/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9773 - val_loss: 0.0533 - val_accuracy: 0.9778\n",
      "Epoch 381/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9776 - val_loss: 0.0660 - val_accuracy: 0.9723\n",
      "Epoch 382/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9769 - val_loss: 0.0596 - val_accuracy: 0.9751\n",
      "Epoch 383/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9782 - val_loss: 0.0587 - val_accuracy: 0.9757\n",
      "Epoch 384/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9779 - val_loss: 0.0665 - val_accuracy: 0.9725\n",
      "Epoch 385/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.0739 - val_accuracy: 0.9714\n",
      "Epoch 386/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9775 - val_loss: 0.0684 - val_accuracy: 0.9728\n",
      "Epoch 387/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9757 - val_loss: 0.0639 - val_accuracy: 0.9743\n",
      "Epoch 388/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9775 - val_loss: 0.0580 - val_accuracy: 0.9758\n",
      "Epoch 389/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9772 - val_loss: 0.0610 - val_accuracy: 0.9734\n",
      "Epoch 390/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9768 - val_loss: 0.0701 - val_accuracy: 0.9721\n",
      "Epoch 391/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9764 - val_loss: 0.0587 - val_accuracy: 0.9758\n",
      "Epoch 392/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9772 - val_loss: 0.0532 - val_accuracy: 0.9773\n",
      "Epoch 393/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9764 - val_loss: 0.0627 - val_accuracy: 0.9747\n",
      "Epoch 394/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9768 - val_loss: 0.0775 - val_accuracy: 0.9703\n",
      "Epoch 395/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9766 - val_loss: 0.0733 - val_accuracy: 0.9694\n",
      "Epoch 396/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0649 - accuracy: 0.9768 - val_loss: 0.0696 - val_accuracy: 0.9715\n",
      "Epoch 397/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9769 - val_loss: 0.0624 - val_accuracy: 0.9737\n",
      "Epoch 398/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9770 - val_loss: 0.0612 - val_accuracy: 0.9747\n",
      "Epoch 399/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9779 - val_loss: 0.0620 - val_accuracy: 0.9738\n",
      "Epoch 400/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9777 - val_loss: 0.0561 - val_accuracy: 0.9760\n",
      "Epoch 401/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9774 - val_loss: 0.0662 - val_accuracy: 0.9726\n",
      "Epoch 402/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9764 - val_loss: 0.0630 - val_accuracy: 0.9731\n",
      "Epoch 403/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9776 - val_loss: 0.0714 - val_accuracy: 0.9708\n",
      "Epoch 404/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9778 - val_loss: 0.0670 - val_accuracy: 0.9736\n",
      "Epoch 405/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9784 - val_loss: 0.0663 - val_accuracy: 0.9725\n",
      "Epoch 406/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9769 - val_loss: 0.0636 - val_accuracy: 0.9741\n",
      "Epoch 407/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9774 - val_loss: 0.0779 - val_accuracy: 0.9687\n",
      "Epoch 408/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9776 - val_loss: 0.0516 - val_accuracy: 0.9785\n",
      "Epoch 409/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9776 - val_loss: 0.0646 - val_accuracy: 0.9737\n",
      "Epoch 410/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9774 - val_loss: 0.0532 - val_accuracy: 0.9773\n",
      "Epoch 411/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 0.0554 - val_accuracy: 0.9760\n",
      "Epoch 412/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9778 - val_loss: 0.0680 - val_accuracy: 0.9724\n",
      "Epoch 413/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9776 - val_loss: 0.0564 - val_accuracy: 0.9760\n",
      "Epoch 414/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9777 - val_loss: 0.0549 - val_accuracy: 0.9763\n",
      "Epoch 415/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.0686 - val_accuracy: 0.9728\n",
      "Epoch 416/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.0682 - val_accuracy: 0.9728\n",
      "Epoch 417/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9787 - val_loss: 0.0508 - val_accuracy: 0.9782\n",
      "Epoch 418/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9785 - val_loss: 0.0616 - val_accuracy: 0.9753\n",
      "Epoch 419/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9780 - val_loss: 0.0561 - val_accuracy: 0.9760\n",
      "Epoch 420/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9773 - val_loss: 0.0560 - val_accuracy: 0.9760\n",
      "Epoch 421/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9778 - val_loss: 0.0598 - val_accuracy: 0.9753\n",
      "Epoch 422/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9770 - val_loss: 0.0612 - val_accuracy: 0.9754\n",
      "Epoch 423/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.0660 - val_accuracy: 0.9736\n",
      "Epoch 424/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9785 - val_loss: 0.0644 - val_accuracy: 0.9733\n",
      "Epoch 425/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9777 - val_loss: 0.0559 - val_accuracy: 0.9766\n",
      "Epoch 426/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9773 - val_loss: 0.0641 - val_accuracy: 0.9735\n",
      "Epoch 427/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9781 - val_loss: 0.0702 - val_accuracy: 0.9719\n",
      "Epoch 428/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9779 - val_loss: 0.0718 - val_accuracy: 0.9711\n",
      "Epoch 429/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9781 - val_loss: 0.0853 - val_accuracy: 0.9665\n",
      "Epoch 430/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9783 - val_loss: 0.0604 - val_accuracy: 0.9754\n",
      "Epoch 431/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9773 - val_loss: 0.0588 - val_accuracy: 0.9759\n",
      "Epoch 432/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0690 - val_accuracy: 0.9729\n",
      "Epoch 433/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9777 - val_loss: 0.0588 - val_accuracy: 0.9755\n",
      "Epoch 434/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9780 - val_loss: 0.0537 - val_accuracy: 0.9769\n",
      "Epoch 435/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9766 - val_loss: 0.0654 - val_accuracy: 0.9730\n",
      "Epoch 436/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0580 - val_accuracy: 0.9755\n",
      "Epoch 437/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9774 - val_loss: 0.0628 - val_accuracy: 0.9754\n",
      "Epoch 438/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9791 - val_loss: 0.0562 - val_accuracy: 0.9760\n",
      "Epoch 439/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9767 - val_loss: 0.0539 - val_accuracy: 0.9778\n",
      "Epoch 440/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9780 - val_loss: 0.0628 - val_accuracy: 0.9747\n",
      "Epoch 441/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9770 - val_loss: 0.0574 - val_accuracy: 0.9756\n",
      "Epoch 442/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.0635 - val_accuracy: 0.9754\n",
      "Epoch 443/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9790 - val_loss: 0.0708 - val_accuracy: 0.9721\n",
      "Epoch 444/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9783 - val_loss: 0.0670 - val_accuracy: 0.9721\n",
      "Epoch 445/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9765 - val_loss: 0.0585 - val_accuracy: 0.9759\n",
      "Epoch 446/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9771 - val_loss: 0.0611 - val_accuracy: 0.9753\n",
      "Epoch 447/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 0.0642 - val_accuracy: 0.9747\n",
      "Epoch 448/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9780 - val_loss: 0.0607 - val_accuracy: 0.9751\n",
      "Epoch 449/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9783 - val_loss: 0.0786 - val_accuracy: 0.9701\n",
      "Epoch 450/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9772 - val_loss: 0.0603 - val_accuracy: 0.9753\n",
      "Epoch 451/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9788 - val_loss: 0.0558 - val_accuracy: 0.9775\n",
      "Epoch 452/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0621 - accuracy: 0.9786 - val_loss: 0.0507 - val_accuracy: 0.9794\n",
      "Epoch 453/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9791 - val_loss: 0.0529 - val_accuracy: 0.9777\n",
      "Epoch 454/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9770 - val_loss: 0.0668 - val_accuracy: 0.9739\n",
      "Epoch 455/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9779 - val_loss: 0.0584 - val_accuracy: 0.9761\n",
      "Epoch 456/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9784 - val_loss: 0.0557 - val_accuracy: 0.9770\n",
      "Epoch 457/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9782 - val_loss: 0.0553 - val_accuracy: 0.9776\n",
      "Epoch 458/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9783 - val_loss: 0.0642 - val_accuracy: 0.9756\n",
      "Epoch 459/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9779 - val_loss: 0.0574 - val_accuracy: 0.9770\n",
      "Epoch 460/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9786 - val_loss: 0.0512 - val_accuracy: 0.9790\n",
      "Epoch 461/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9774 - val_loss: 0.0556 - val_accuracy: 0.9773\n",
      "Epoch 462/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.0686 - val_accuracy: 0.9730\n",
      "Epoch 463/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9783 - val_loss: 0.0837 - val_accuracy: 0.9691\n",
      "Epoch 464/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.0616 - val_accuracy: 0.9755\n",
      "Epoch 465/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9787 - val_loss: 0.0659 - val_accuracy: 0.9747\n",
      "Epoch 466/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9782 - val_loss: 0.0673 - val_accuracy: 0.9736\n",
      "Epoch 467/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9772 - val_loss: 0.0575 - val_accuracy: 0.9764\n",
      "Epoch 468/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9785 - val_loss: 0.0590 - val_accuracy: 0.9764\n",
      "Epoch 469/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9785 - val_loss: 0.0558 - val_accuracy: 0.9766\n",
      "Epoch 470/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9777 - val_loss: 0.0616 - val_accuracy: 0.9744\n",
      "Epoch 471/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9792 - val_loss: 0.0617 - val_accuracy: 0.9748\n",
      "Epoch 472/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9786 - val_loss: 0.0688 - val_accuracy: 0.9736\n",
      "Epoch 473/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9785 - val_loss: 0.0608 - val_accuracy: 0.9756\n",
      "Epoch 474/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9778 - val_loss: 0.0623 - val_accuracy: 0.9756\n",
      "Epoch 475/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9776 - val_loss: 0.0643 - val_accuracy: 0.9743\n",
      "Epoch 476/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9787 - val_loss: 0.0580 - val_accuracy: 0.9758\n",
      "Epoch 477/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.0625 - val_accuracy: 0.9741\n",
      "Epoch 478/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9782 - val_loss: 0.0620 - val_accuracy: 0.9746\n",
      "Epoch 479/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0585 - val_accuracy: 0.9765\n",
      "Epoch 480/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9780 - val_loss: 0.0610 - val_accuracy: 0.9747\n",
      "Epoch 481/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9783 - val_loss: 0.0538 - val_accuracy: 0.9777\n",
      "Epoch 482/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9790 - val_loss: 0.0605 - val_accuracy: 0.9760\n",
      "Epoch 483/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9794 - val_loss: 0.0604 - val_accuracy: 0.9757\n",
      "Epoch 484/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9781 - val_loss: 0.0589 - val_accuracy: 0.9761\n",
      "Epoch 485/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9788 - val_loss: 0.0642 - val_accuracy: 0.9747\n",
      "Epoch 486/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9779 - val_loss: 0.0616 - val_accuracy: 0.9756\n",
      "Epoch 487/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9790 - val_loss: 0.0551 - val_accuracy: 0.9773\n",
      "Epoch 488/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9783 - val_loss: 0.0628 - val_accuracy: 0.9750\n",
      "Epoch 489/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.0594 - val_accuracy: 0.9751\n",
      "Epoch 490/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9794 - val_loss: 0.0666 - val_accuracy: 0.9735\n",
      "Epoch 491/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9793 - val_loss: 0.0544 - val_accuracy: 0.9775\n",
      "Epoch 492/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9783 - val_loss: 0.0667 - val_accuracy: 0.9734\n",
      "Epoch 493/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9793 - val_loss: 0.0585 - val_accuracy: 0.9760\n",
      "Epoch 494/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9789 - val_loss: 0.0521 - val_accuracy: 0.9777\n",
      "Epoch 495/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9778 - val_loss: 0.0630 - val_accuracy: 0.9753\n",
      "Epoch 496/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9790 - val_loss: 0.0612 - val_accuracy: 0.9758\n",
      "Epoch 497/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0603 - accuracy: 0.9789 - val_loss: 0.0578 - val_accuracy: 0.9760\n",
      "Epoch 498/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9795 - val_loss: 0.0658 - val_accuracy: 0.9737\n",
      "Epoch 499/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0579 - val_accuracy: 0.9764\n",
      "Epoch 500/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9792 - val_loss: 0.0686 - val_accuracy: 0.9735\n",
      "Epoch 501/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9788 - val_loss: 0.0593 - val_accuracy: 0.9754\n",
      "Epoch 502/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0575 - val_accuracy: 0.9773\n",
      "Epoch 503/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9786 - val_loss: 0.0625 - val_accuracy: 0.9756\n",
      "Epoch 504/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9794 - val_loss: 0.0678 - val_accuracy: 0.9733\n",
      "Epoch 505/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9790 - val_loss: 0.0641 - val_accuracy: 0.9745\n",
      "Epoch 506/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9789 - val_loss: 0.0634 - val_accuracy: 0.9739\n",
      "Epoch 507/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9789 - val_loss: 0.0616 - val_accuracy: 0.9754\n",
      "Epoch 508/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.0547 - val_accuracy: 0.9773\n",
      "Epoch 509/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9789 - val_loss: 0.0634 - val_accuracy: 0.9745\n",
      "Epoch 510/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9797 - val_loss: 0.0673 - val_accuracy: 0.9743\n",
      "Epoch 511/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9791 - val_loss: 0.0704 - val_accuracy: 0.9723\n",
      "Epoch 512/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9784 - val_loss: 0.0583 - val_accuracy: 0.9766\n",
      "Epoch 513/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9790 - val_loss: 0.0586 - val_accuracy: 0.9763\n",
      "Epoch 514/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9795 - val_loss: 0.0614 - val_accuracy: 0.9759\n",
      "Epoch 515/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9787 - val_loss: 0.0684 - val_accuracy: 0.9731\n",
      "Epoch 516/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9787 - val_loss: 0.0598 - val_accuracy: 0.9763\n",
      "Epoch 517/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.0569 - val_accuracy: 0.9766\n",
      "Epoch 518/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9796 - val_loss: 0.0734 - val_accuracy: 0.9725\n",
      "Epoch 519/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9788 - val_loss: 0.0632 - val_accuracy: 0.9749\n",
      "Epoch 520/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9798 - val_loss: 0.0608 - val_accuracy: 0.9761\n",
      "Epoch 521/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9793 - val_loss: 0.0569 - val_accuracy: 0.9760\n",
      "Epoch 522/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9786 - val_loss: 0.0594 - val_accuracy: 0.9748\n",
      "Epoch 523/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9789 - val_loss: 0.0659 - val_accuracy: 0.9738\n",
      "Epoch 524/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9793 - val_loss: 0.0561 - val_accuracy: 0.9767\n",
      "Epoch 525/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9788 - val_loss: 0.0579 - val_accuracy: 0.9765\n",
      "Epoch 526/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9785 - val_loss: 0.0532 - val_accuracy: 0.9764\n",
      "Epoch 527/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9797 - val_loss: 0.0702 - val_accuracy: 0.9735\n",
      "Epoch 528/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9781 - val_loss: 0.0659 - val_accuracy: 0.9751\n",
      "Epoch 529/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9791 - val_loss: 0.0653 - val_accuracy: 0.9744\n",
      "Epoch 530/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.0580 - val_accuracy: 0.9763\n",
      "Epoch 531/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0553 - val_accuracy: 0.9782\n",
      "Epoch 532/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9801 - val_loss: 0.0565 - val_accuracy: 0.9765\n",
      "Epoch 533/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.0724 - val_accuracy: 0.9728\n",
      "Epoch 534/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9801 - val_loss: 0.0529 - val_accuracy: 0.9781\n",
      "Epoch 535/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9791 - val_loss: 0.0601 - val_accuracy: 0.9751\n",
      "Epoch 536/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9787 - val_loss: 0.0598 - val_accuracy: 0.9765\n",
      "Epoch 537/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9799 - val_loss: 0.0561 - val_accuracy: 0.9763\n",
      "Epoch 538/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9794 - val_loss: 0.0617 - val_accuracy: 0.9758\n",
      "Epoch 539/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9790 - val_loss: 0.0564 - val_accuracy: 0.9775\n",
      "Epoch 540/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0607 - val_accuracy: 0.9757\n",
      "Epoch 541/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9791 - val_loss: 0.0638 - val_accuracy: 0.9746\n",
      "Epoch 542/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9791 - val_loss: 0.0597 - val_accuracy: 0.9764\n",
      "Epoch 543/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9795 - val_loss: 0.0512 - val_accuracy: 0.9778\n",
      "Epoch 544/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9798 - val_loss: 0.0590 - val_accuracy: 0.9758\n",
      "Epoch 545/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9795 - val_loss: 0.0685 - val_accuracy: 0.9739\n",
      "Epoch 546/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9796 - val_loss: 0.0553 - val_accuracy: 0.9771\n",
      "Epoch 547/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0586 - accuracy: 0.9797 - val_loss: 0.0588 - val_accuracy: 0.9763\n",
      "Epoch 548/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0586 - accuracy: 0.9793 - val_loss: 0.0621 - val_accuracy: 0.9750\n",
      "Epoch 549/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 0.0656 - val_accuracy: 0.9756\n",
      "Epoch 550/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9803 - val_loss: 0.0574 - val_accuracy: 0.9760\n",
      "Epoch 551/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.0530 - val_accuracy: 0.9775\n",
      "Epoch 552/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0595 - accuracy: 0.9788 - val_loss: 0.0600 - val_accuracy: 0.9757\n",
      "Epoch 553/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0585 - accuracy: 0.9794 - val_loss: 0.0608 - val_accuracy: 0.9751\n",
      "Epoch 554/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0572 - accuracy: 0.9800 - val_loss: 0.0612 - val_accuracy: 0.9754\n",
      "Epoch 555/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9802 - val_loss: 0.0688 - val_accuracy: 0.9737\n",
      "Epoch 556/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0579 - accuracy: 0.9798 - val_loss: 0.0539 - val_accuracy: 0.9780\n",
      "Epoch 557/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0582 - accuracy: 0.9805 - val_loss: 0.0650 - val_accuracy: 0.9748\n",
      "Epoch 558/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9797 - val_loss: 0.0511 - val_accuracy: 0.9778\n",
      "Epoch 559/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 0.0579 - val_accuracy: 0.9760\n",
      "Epoch 560/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0569 - accuracy: 0.9797 - val_loss: 0.0662 - val_accuracy: 0.9743\n",
      "Epoch 561/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0565 - accuracy: 0.9806 - val_loss: 0.0717 - val_accuracy: 0.9727\n",
      "Epoch 562/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0568 - accuracy: 0.9803 - val_loss: 0.0522 - val_accuracy: 0.9777\n",
      "Epoch 563/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 0.0543 - val_accuracy: 0.9778\n",
      "Epoch 564/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0577 - accuracy: 0.9799 - val_loss: 0.0533 - val_accuracy: 0.9779\n",
      "Epoch 565/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9799 - val_loss: 0.0651 - val_accuracy: 0.9744\n",
      "Epoch 566/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9805 - val_loss: 0.0516 - val_accuracy: 0.9779\n",
      "Epoch 567/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9796 - val_loss: 0.0539 - val_accuracy: 0.9779\n",
      "Epoch 568/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9797 - val_loss: 0.0641 - val_accuracy: 0.9757\n",
      "Epoch 569/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9801 - val_loss: 0.0525 - val_accuracy: 0.9778\n",
      "Epoch 570/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9806 - val_loss: 0.0658 - val_accuracy: 0.9737\n",
      "Epoch 571/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9792 - val_loss: 0.0630 - val_accuracy: 0.9754\n",
      "Epoch 572/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9801 - val_loss: 0.0616 - val_accuracy: 0.9759\n",
      "Epoch 573/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9797 - val_loss: 0.0553 - val_accuracy: 0.9770\n",
      "Epoch 574/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9792 - val_loss: 0.0649 - val_accuracy: 0.9737\n",
      "Epoch 575/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9797 - val_loss: 0.0521 - val_accuracy: 0.9781\n",
      "Epoch 576/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9799 - val_loss: 0.0686 - val_accuracy: 0.9734\n",
      "Epoch 577/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9782 - val_loss: 0.0526 - val_accuracy: 0.9779\n",
      "Epoch 578/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.0512 - val_accuracy: 0.9776\n",
      "Epoch 579/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9794 - val_loss: 0.0621 - val_accuracy: 0.9759\n",
      "Epoch 580/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9801 - val_loss: 0.0627 - val_accuracy: 0.9755\n",
      "Epoch 581/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9809 - val_loss: 0.0720 - val_accuracy: 0.9726\n",
      "Epoch 582/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9803 - val_loss: 0.0689 - val_accuracy: 0.9738\n",
      "Epoch 583/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9794 - val_loss: 0.0556 - val_accuracy: 0.9768\n",
      "Epoch 584/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9809 - val_loss: 0.0699 - val_accuracy: 0.9724\n",
      "Epoch 585/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9799 - val_loss: 0.0563 - val_accuracy: 0.9769\n",
      "Epoch 586/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9797 - val_loss: 0.0546 - val_accuracy: 0.9775\n",
      "Epoch 587/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0552 - val_accuracy: 0.9774\n",
      "Epoch 588/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9809 - val_loss: 0.0613 - val_accuracy: 0.9747\n",
      "Epoch 589/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9796 - val_loss: 0.0513 - val_accuracy: 0.9791\n",
      "Epoch 590/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9795 - val_loss: 0.0634 - val_accuracy: 0.9755\n",
      "Epoch 591/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9801 - val_loss: 0.0534 - val_accuracy: 0.9768\n",
      "Epoch 592/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9798 - val_loss: 0.0535 - val_accuracy: 0.9778\n",
      "Epoch 593/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0527 - val_accuracy: 0.9781\n",
      "Epoch 594/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9808 - val_loss: 0.0523 - val_accuracy: 0.9780\n",
      "Epoch 595/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.0666 - val_accuracy: 0.9746\n",
      "Epoch 596/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 0.0669 - val_accuracy: 0.9736\n",
      "Epoch 597/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9807 - val_loss: 0.0567 - val_accuracy: 0.9766\n",
      "Epoch 598/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0569 - accuracy: 0.9802 - val_loss: 0.0562 - val_accuracy: 0.9771\n",
      "Epoch 599/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9802 - val_loss: 0.0559 - val_accuracy: 0.9773\n",
      "Epoch 600/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9813 - val_loss: 0.0589 - val_accuracy: 0.9763\n",
      "Epoch 601/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9804 - val_loss: 0.0583 - val_accuracy: 0.9764\n",
      "Epoch 602/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9806 - val_loss: 0.0596 - val_accuracy: 0.9757\n",
      "Epoch 603/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9807 - val_loss: 0.0577 - val_accuracy: 0.9765\n",
      "Epoch 604/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9810 - val_loss: 0.0618 - val_accuracy: 0.9755\n",
      "Epoch 605/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9803 - val_loss: 0.0499 - val_accuracy: 0.9788\n",
      "Epoch 606/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9794 - val_loss: 0.0625 - val_accuracy: 0.9753\n",
      "Epoch 607/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9806 - val_loss: 0.0593 - val_accuracy: 0.9756\n",
      "Epoch 608/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9794 - val_loss: 0.0677 - val_accuracy: 0.9739\n",
      "Epoch 609/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9810 - val_loss: 0.0579 - val_accuracy: 0.9760\n",
      "Epoch 610/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9805 - val_loss: 0.0614 - val_accuracy: 0.9754\n",
      "Epoch 611/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9802 - val_loss: 0.0561 - val_accuracy: 0.9769\n",
      "Epoch 612/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9807 - val_loss: 0.0577 - val_accuracy: 0.9760\n",
      "Epoch 613/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9798 - val_loss: 0.0665 - val_accuracy: 0.9748\n",
      "Epoch 614/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9805 - val_loss: 0.0562 - val_accuracy: 0.9771\n",
      "Epoch 615/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9807 - val_loss: 0.0640 - val_accuracy: 0.9751\n",
      "Epoch 616/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9810 - val_loss: 0.0579 - val_accuracy: 0.9767\n",
      "Epoch 617/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9798 - val_loss: 0.0595 - val_accuracy: 0.9758\n",
      "Epoch 618/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9805 - val_loss: 0.0563 - val_accuracy: 0.9761\n",
      "Epoch 619/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9811 - val_loss: 0.0766 - val_accuracy: 0.9733\n",
      "Epoch 620/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9796 - val_loss: 0.0579 - val_accuracy: 0.9767\n",
      "Epoch 621/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9808 - val_loss: 0.0569 - val_accuracy: 0.9766\n",
      "Epoch 622/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9795 - val_loss: 0.0545 - val_accuracy: 0.9777\n",
      "Epoch 623/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9797 - val_loss: 0.0600 - val_accuracy: 0.9757\n",
      "Epoch 624/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.0627 - val_accuracy: 0.9755\n",
      "Epoch 625/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9798 - val_loss: 0.0645 - val_accuracy: 0.9750\n",
      "Epoch 626/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0568 - val_accuracy: 0.9770\n",
      "Epoch 627/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9814 - val_loss: 0.0531 - val_accuracy: 0.9784\n",
      "Epoch 628/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9802 - val_loss: 0.0554 - val_accuracy: 0.9764\n",
      "Epoch 629/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9804 - val_loss: 0.0715 - val_accuracy: 0.9729\n",
      "Epoch 630/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9789 - val_loss: 0.0564 - val_accuracy: 0.9766\n",
      "Epoch 631/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9804 - val_loss: 0.0640 - val_accuracy: 0.9748\n",
      "Epoch 632/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9808 - val_loss: 0.0645 - val_accuracy: 0.9747\n",
      "Epoch 633/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9791 - val_loss: 0.0572 - val_accuracy: 0.9771\n",
      "Epoch 634/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9802 - val_loss: 0.0675 - val_accuracy: 0.9734\n",
      "Epoch 635/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9807 - val_loss: 0.0513 - val_accuracy: 0.9784\n",
      "Epoch 636/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9808 - val_loss: 0.0569 - val_accuracy: 0.9769\n",
      "Epoch 637/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 0.0531 - val_accuracy: 0.9790\n",
      "Epoch 638/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9807 - val_loss: 0.0541 - val_accuracy: 0.9781\n",
      "Epoch 639/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9806 - val_loss: 0.0614 - val_accuracy: 0.9749\n",
      "Epoch 640/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9799 - val_loss: 0.0645 - val_accuracy: 0.9749\n",
      "Epoch 641/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9803 - val_loss: 0.0565 - val_accuracy: 0.9755\n",
      "Epoch 642/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0635 - val_accuracy: 0.9753\n",
      "Epoch 643/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.0659 - val_accuracy: 0.9750\n",
      "Epoch 644/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9802 - val_loss: 0.0603 - val_accuracy: 0.9753\n",
      "Epoch 645/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9799 - val_loss: 0.0723 - val_accuracy: 0.9735\n",
      "Epoch 646/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9797 - val_loss: 0.0662 - val_accuracy: 0.9748\n",
      "Epoch 647/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9803 - val_loss: 0.0671 - val_accuracy: 0.9750\n",
      "Epoch 648/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9806 - val_loss: 0.0595 - val_accuracy: 0.9758\n",
      "Epoch 649/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 0.0570 - val_accuracy: 0.9770\n",
      "Epoch 650/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9807 - val_loss: 0.0774 - val_accuracy: 0.9733\n",
      "Epoch 651/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9809 - val_loss: 0.0576 - val_accuracy: 0.9773\n",
      "Epoch 652/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9814 - val_loss: 0.0577 - val_accuracy: 0.9760\n",
      "Epoch 653/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9809 - val_loss: 0.0527 - val_accuracy: 0.9770\n",
      "Epoch 654/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0564 - accuracy: 0.9795 - val_loss: 0.0675 - val_accuracy: 0.9740\n",
      "Epoch 655/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9809 - val_loss: 0.0587 - val_accuracy: 0.9765\n",
      "Epoch 656/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9802 - val_loss: 0.0525 - val_accuracy: 0.9787\n",
      "Epoch 657/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0605 - val_accuracy: 0.9754\n",
      "Epoch 658/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9802 - val_loss: 0.0592 - val_accuracy: 0.9761\n",
      "Epoch 659/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.0547 - val_accuracy: 0.9764\n",
      "Epoch 660/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9804 - val_loss: 0.0593 - val_accuracy: 0.9760\n",
      "Epoch 661/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9807 - val_loss: 0.0587 - val_accuracy: 0.9760\n",
      "Epoch 662/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9812 - val_loss: 0.0642 - val_accuracy: 0.9753\n",
      "Epoch 663/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9805 - val_loss: 0.0648 - val_accuracy: 0.9747\n",
      "Epoch 664/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9804 - val_loss: 0.0614 - val_accuracy: 0.9756\n",
      "Epoch 665/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9818 - val_loss: 0.0565 - val_accuracy: 0.9768\n",
      "Epoch 666/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9809 - val_loss: 0.0560 - val_accuracy: 0.9777\n",
      "Epoch 667/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9802 - val_loss: 0.0597 - val_accuracy: 0.9761\n",
      "Epoch 668/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 0.0558 - val_accuracy: 0.9774\n",
      "Epoch 669/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9803 - val_loss: 0.0657 - val_accuracy: 0.9755\n",
      "Epoch 670/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9806 - val_loss: 0.0558 - val_accuracy: 0.9770\n",
      "Epoch 671/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9809 - val_loss: 0.0567 - val_accuracy: 0.9764\n",
      "Epoch 672/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9815 - val_loss: 0.0626 - val_accuracy: 0.9756\n",
      "Epoch 673/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9803 - val_loss: 0.0627 - val_accuracy: 0.9753\n",
      "Epoch 674/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9807 - val_loss: 0.0570 - val_accuracy: 0.9776\n",
      "Epoch 675/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9807 - val_loss: 0.0572 - val_accuracy: 0.9766\n",
      "Epoch 676/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9817 - val_loss: 0.0571 - val_accuracy: 0.9774\n",
      "Epoch 677/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9795 - val_loss: 0.0683 - val_accuracy: 0.9743\n",
      "Epoch 678/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9816 - val_loss: 0.0522 - val_accuracy: 0.9780\n",
      "Epoch 679/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9808 - val_loss: 0.0568 - val_accuracy: 0.9761\n",
      "Epoch 680/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9805 - val_loss: 0.0661 - val_accuracy: 0.9746\n",
      "Epoch 681/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9809 - val_loss: 0.0673 - val_accuracy: 0.9739\n",
      "Epoch 682/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9801 - val_loss: 0.0590 - val_accuracy: 0.9768\n",
      "Epoch 683/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9810 - val_loss: 0.0583 - val_accuracy: 0.9771\n",
      "Epoch 684/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9798 - val_loss: 0.0694 - val_accuracy: 0.9741\n",
      "Epoch 685/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9811 - val_loss: 0.0522 - val_accuracy: 0.9788\n",
      "Epoch 686/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 0.0534 - val_accuracy: 0.9771\n",
      "Epoch 687/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9804 - val_loss: 0.0578 - val_accuracy: 0.9763\n",
      "Epoch 688/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9809 - val_loss: 0.0497 - val_accuracy: 0.9790\n",
      "Epoch 689/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9802 - val_loss: 0.0648 - val_accuracy: 0.9747\n",
      "Epoch 690/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9808 - val_loss: 0.0565 - val_accuracy: 0.9765\n",
      "Epoch 691/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9813 - val_loss: 0.0577 - val_accuracy: 0.9760\n",
      "Epoch 692/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9822 - val_loss: 0.0583 - val_accuracy: 0.9767\n",
      "Epoch 693/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9806 - val_loss: 0.0580 - val_accuracy: 0.9776\n",
      "Epoch 694/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9806 - val_loss: 0.0606 - val_accuracy: 0.9761\n",
      "Epoch 695/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9810 - val_loss: 0.0549 - val_accuracy: 0.9770\n",
      "Epoch 696/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9797 - val_loss: 0.0559 - val_accuracy: 0.9771\n",
      "Epoch 697/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 0.0543 - val_accuracy: 0.9773\n",
      "Epoch 698/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9805 - val_loss: 0.0577 - val_accuracy: 0.9766\n",
      "Epoch 699/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9810 - val_loss: 0.0629 - val_accuracy: 0.9753\n",
      "Epoch 700/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9808 - val_loss: 0.0579 - val_accuracy: 0.9765\n",
      "Epoch 701/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9803 - val_loss: 0.0588 - val_accuracy: 0.9773\n",
      "Epoch 702/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9816 - val_loss: 0.0569 - val_accuracy: 0.9770\n",
      "Epoch 703/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 0.0591 - val_accuracy: 0.9761\n",
      "Epoch 704/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0551 - accuracy: 0.9806 - val_loss: 0.0516 - val_accuracy: 0.9787\n",
      "Epoch 705/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0556 - accuracy: 0.9805 - val_loss: 0.0561 - val_accuracy: 0.9768\n",
      "Epoch 706/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.0515 - val_accuracy: 0.9789\n",
      "Epoch 707/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0577 - val_accuracy: 0.9763\n",
      "Epoch 708/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9809 - val_loss: 0.0579 - val_accuracy: 0.9764\n",
      "Epoch 709/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.0567 - val_accuracy: 0.9765\n",
      "Epoch 710/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9802 - val_loss: 0.0495 - val_accuracy: 0.9787\n",
      "Epoch 711/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.0582 - val_accuracy: 0.9767\n",
      "Epoch 712/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 0.0562 - val_accuracy: 0.9774\n",
      "Epoch 713/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9804 - val_loss: 0.0507 - val_accuracy: 0.9781\n",
      "Epoch 714/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9816 - val_loss: 0.0580 - val_accuracy: 0.9765\n",
      "Epoch 715/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9797 - val_loss: 0.0554 - val_accuracy: 0.9769\n",
      "Epoch 716/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9805 - val_loss: 0.0546 - val_accuracy: 0.9773\n",
      "Epoch 717/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.0662 - val_accuracy: 0.9747\n",
      "Epoch 718/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9805 - val_loss: 0.0514 - val_accuracy: 0.9782\n",
      "Epoch 719/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9814 - val_loss: 0.0553 - val_accuracy: 0.9773\n",
      "Epoch 720/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9809 - val_loss: 0.0550 - val_accuracy: 0.9776\n",
      "Epoch 721/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.0576 - val_accuracy: 0.9764\n",
      "Epoch 722/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9814 - val_loss: 0.0540 - val_accuracy: 0.9776\n",
      "Epoch 723/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9809 - val_loss: 0.0565 - val_accuracy: 0.9765\n",
      "Epoch 724/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9809 - val_loss: 0.0584 - val_accuracy: 0.9771\n",
      "Epoch 725/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.0559 - val_accuracy: 0.9771\n",
      "Epoch 726/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9811 - val_loss: 0.0595 - val_accuracy: 0.9763\n",
      "Epoch 727/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9815 - val_loss: 0.0621 - val_accuracy: 0.9764\n",
      "Epoch 728/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9810 - val_loss: 0.0571 - val_accuracy: 0.9767\n",
      "Epoch 729/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9801 - val_loss: 0.0584 - val_accuracy: 0.9760\n",
      "Epoch 730/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9800 - val_loss: 0.0626 - val_accuracy: 0.9757\n",
      "Epoch 731/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9802 - val_loss: 0.0617 - val_accuracy: 0.9753\n",
      "Epoch 732/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9802 - val_loss: 0.0581 - val_accuracy: 0.9761\n",
      "Epoch 733/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9808 - val_loss: 0.0613 - val_accuracy: 0.9765\n",
      "Epoch 734/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9807 - val_loss: 0.0565 - val_accuracy: 0.9770\n",
      "Epoch 735/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9817 - val_loss: 0.0534 - val_accuracy: 0.9778\n",
      "Epoch 736/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9815 - val_loss: 0.0563 - val_accuracy: 0.9777\n",
      "Epoch 737/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.0573 - val_accuracy: 0.9771\n",
      "Epoch 738/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9817 - val_loss: 0.0556 - val_accuracy: 0.9766\n",
      "Epoch 739/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9814 - val_loss: 0.0585 - val_accuracy: 0.9769\n",
      "Epoch 740/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9815 - val_loss: 0.0576 - val_accuracy: 0.9765\n",
      "Epoch 741/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9819 - val_loss: 0.0560 - val_accuracy: 0.9774\n",
      "Epoch 742/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9818 - val_loss: 0.0543 - val_accuracy: 0.9773\n",
      "Epoch 743/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9805 - val_loss: 0.0568 - val_accuracy: 0.9770\n",
      "Epoch 744/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.0636 - val_accuracy: 0.9756\n",
      "Epoch 745/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9820 - val_loss: 0.0596 - val_accuracy: 0.9764\n",
      "Epoch 746/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9817 - val_loss: 0.0512 - val_accuracy: 0.9782\n",
      "Epoch 747/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9812 - val_loss: 0.0550 - val_accuracy: 0.9770\n",
      "Epoch 748/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9817 - val_loss: 0.0590 - val_accuracy: 0.9759\n",
      "Epoch 749/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9816 - val_loss: 0.0599 - val_accuracy: 0.9761\n",
      "Epoch 750/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9815 - val_loss: 0.0609 - val_accuracy: 0.9764\n",
      "Epoch 751/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9815 - val_loss: 0.0572 - val_accuracy: 0.9774\n",
      "Epoch 752/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.0529 - val_accuracy: 0.9780\n",
      "Epoch 753/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.0669 - val_accuracy: 0.9750\n",
      "Epoch 754/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9808 - val_loss: 0.0588 - val_accuracy: 0.9766\n",
      "Epoch 755/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9818 - val_loss: 0.0574 - val_accuracy: 0.9767\n",
      "Epoch 756/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.0560 - val_accuracy: 0.9761\n",
      "Epoch 757/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9807 - val_loss: 0.0574 - val_accuracy: 0.9770\n",
      "Epoch 758/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9814 - val_loss: 0.0592 - val_accuracy: 0.9765\n",
      "Epoch 759/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9809 - val_loss: 0.0604 - val_accuracy: 0.9760\n",
      "Epoch 760/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9819 - val_loss: 0.0580 - val_accuracy: 0.9760\n",
      "Epoch 761/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.0637 - val_accuracy: 0.9749\n",
      "Epoch 762/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9809 - val_loss: 0.0480 - val_accuracy: 0.9794\n",
      "Epoch 763/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9806 - val_loss: 0.0573 - val_accuracy: 0.9758\n",
      "Epoch 764/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9814 - val_loss: 0.0526 - val_accuracy: 0.9781\n",
      "Epoch 765/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9813 - val_loss: 0.0542 - val_accuracy: 0.9769\n",
      "Epoch 766/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9811 - val_loss: 0.0519 - val_accuracy: 0.9773\n",
      "Epoch 767/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 0.0598 - val_accuracy: 0.9764\n",
      "Epoch 768/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9812 - val_loss: 0.0507 - val_accuracy: 0.9786\n",
      "Epoch 769/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9818 - val_loss: 0.0581 - val_accuracy: 0.9760\n",
      "Epoch 770/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.0574 - val_accuracy: 0.9766\n",
      "Epoch 771/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9811 - val_loss: 0.0561 - val_accuracy: 0.9766\n",
      "Epoch 772/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9816 - val_loss: 0.0522 - val_accuracy: 0.9781\n",
      "Epoch 773/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0588 - val_accuracy: 0.9770\n",
      "Epoch 774/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0515 - val_accuracy: 0.9784\n",
      "Epoch 775/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9814 - val_loss: 0.0745 - val_accuracy: 0.9737\n",
      "Epoch 776/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9808 - val_loss: 0.0602 - val_accuracy: 0.9758\n",
      "Epoch 777/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9813 - val_loss: 0.0539 - val_accuracy: 0.9771\n",
      "Epoch 778/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 0.0634 - val_accuracy: 0.9754\n",
      "Epoch 779/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9818 - val_loss: 0.0571 - val_accuracy: 0.9776\n",
      "Epoch 780/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0520 - val_accuracy: 0.9786\n",
      "Epoch 781/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0709 - val_accuracy: 0.9743\n",
      "Epoch 782/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9817 - val_loss: 0.0579 - val_accuracy: 0.9763\n",
      "Epoch 783/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9803 - val_loss: 0.0568 - val_accuracy: 0.9761\n",
      "Epoch 784/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0488 - val_accuracy: 0.9791\n",
      "Epoch 785/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9814 - val_loss: 0.0588 - val_accuracy: 0.9764\n",
      "Epoch 786/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9812 - val_loss: 0.0521 - val_accuracy: 0.9781\n",
      "Epoch 787/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9812 - val_loss: 0.0573 - val_accuracy: 0.9766\n",
      "Epoch 788/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.0572 - val_accuracy: 0.9766\n",
      "Epoch 789/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9822 - val_loss: 0.0637 - val_accuracy: 0.9751\n",
      "Epoch 790/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9810 - val_loss: 0.0602 - val_accuracy: 0.9764\n",
      "Epoch 791/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 792/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9829 - val_loss: 0.0532 - val_accuracy: 0.9777\n",
      "Epoch 793/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9804 - val_loss: 0.0586 - val_accuracy: 0.9756\n",
      "Epoch 794/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0575 - val_accuracy: 0.9763\n",
      "Epoch 795/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.0562 - val_accuracy: 0.9767\n",
      "Epoch 796/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9804 - val_loss: 0.0597 - val_accuracy: 0.9769\n",
      "Epoch 797/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9824 - val_loss: 0.0646 - val_accuracy: 0.9757\n",
      "Epoch 798/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9809 - val_loss: 0.0571 - val_accuracy: 0.9763\n",
      "Epoch 799/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9807 - val_loss: 0.0516 - val_accuracy: 0.9776\n",
      "Epoch 800/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0569 - val_accuracy: 0.9767\n",
      "Epoch 801/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.0560 - val_accuracy: 0.9775\n",
      "Epoch 802/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9797 - val_loss: 0.0560 - val_accuracy: 0.9760\n",
      "Epoch 803/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9811 - val_loss: 0.0585 - val_accuracy: 0.9767\n",
      "Epoch 804/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9818 - val_loss: 0.0605 - val_accuracy: 0.9759\n",
      "Epoch 805/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.0551 - val_accuracy: 0.9771\n",
      "Epoch 806/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9821 - val_loss: 0.0641 - val_accuracy: 0.9755\n",
      "Epoch 807/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9813 - val_loss: 0.0539 - val_accuracy: 0.9774\n",
      "Epoch 808/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9827 - val_loss: 0.0561 - val_accuracy: 0.9768\n",
      "Epoch 809/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9813 - val_loss: 0.0515 - val_accuracy: 0.9786\n",
      "Epoch 810/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 0.0614 - val_accuracy: 0.9765\n",
      "Epoch 811/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9812 - val_loss: 0.0627 - val_accuracy: 0.9758\n",
      "Epoch 812/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9806 - val_loss: 0.0657 - val_accuracy: 0.9749\n",
      "Epoch 813/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0679 - val_accuracy: 0.9748\n",
      "Epoch 814/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9816 - val_loss: 0.0575 - val_accuracy: 0.9760\n",
      "Epoch 815/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0634 - val_accuracy: 0.9748\n",
      "Epoch 816/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9814 - val_loss: 0.0537 - val_accuracy: 0.9776\n",
      "Epoch 817/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9814 - val_loss: 0.0520 - val_accuracy: 0.9781\n",
      "Epoch 818/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9804 - val_loss: 0.0540 - val_accuracy: 0.9778\n",
      "Epoch 819/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9817 - val_loss: 0.0610 - val_accuracy: 0.9759\n",
      "Epoch 820/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.0708 - val_accuracy: 0.9744\n",
      "Epoch 821/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9806 - val_loss: 0.0586 - val_accuracy: 0.9759\n",
      "Epoch 822/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9808 - val_loss: 0.0625 - val_accuracy: 0.9758\n",
      "Epoch 823/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9810 - val_loss: 0.0616 - val_accuracy: 0.9756\n",
      "Epoch 824/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9808 - val_loss: 0.0577 - val_accuracy: 0.9769\n",
      "Epoch 825/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9824 - val_loss: 0.0522 - val_accuracy: 0.9782\n",
      "Epoch 826/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9820 - val_loss: 0.0593 - val_accuracy: 0.9765\n",
      "Epoch 827/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9811 - val_loss: 0.0569 - val_accuracy: 0.9767\n",
      "Epoch 828/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.0596 - val_accuracy: 0.9758\n",
      "Epoch 829/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0637 - val_accuracy: 0.9751\n",
      "Epoch 830/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9817 - val_loss: 0.0560 - val_accuracy: 0.9779\n",
      "Epoch 831/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9812 - val_loss: 0.0582 - val_accuracy: 0.9775\n",
      "Epoch 832/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9815 - val_loss: 0.0597 - val_accuracy: 0.9767\n",
      "Epoch 833/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0621 - val_accuracy: 0.9763\n",
      "Epoch 834/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9817 - val_loss: 0.0557 - val_accuracy: 0.9766\n",
      "Epoch 835/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.0661 - val_accuracy: 0.9759\n",
      "Epoch 836/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9811 - val_loss: 0.0596 - val_accuracy: 0.9763\n",
      "Epoch 837/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9795 - val_loss: 0.0577 - val_accuracy: 0.9767\n",
      "Epoch 838/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.0595 - val_accuracy: 0.9765\n",
      "Epoch 839/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9807 - val_loss: 0.0606 - val_accuracy: 0.9758\n",
      "Epoch 840/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0659 - val_accuracy: 0.9754\n",
      "Epoch 841/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0606 - val_accuracy: 0.9765\n",
      "Epoch 842/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9818 - val_loss: 0.0540 - val_accuracy: 0.9764\n",
      "Epoch 843/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9820 - val_loss: 0.0563 - val_accuracy: 0.9771\n",
      "Epoch 844/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9816 - val_loss: 0.0594 - val_accuracy: 0.9769\n",
      "Epoch 845/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.0589 - val_accuracy: 0.9769\n",
      "Epoch 846/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9809 - val_loss: 0.0535 - val_accuracy: 0.9778\n",
      "Epoch 847/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9819 - val_loss: 0.0532 - val_accuracy: 0.9778\n",
      "Epoch 848/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9823 - val_loss: 0.0543 - val_accuracy: 0.9778\n",
      "Epoch 849/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0528 - val_accuracy: 0.9777\n",
      "Epoch 850/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.0572 - val_accuracy: 0.9768\n",
      "Epoch 851/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9824 - val_loss: 0.0589 - val_accuracy: 0.9764\n",
      "Epoch 852/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 0.0658 - val_accuracy: 0.9754\n",
      "Epoch 853/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9818 - val_loss: 0.0566 - val_accuracy: 0.9769\n",
      "Epoch 854/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9814 - val_loss: 0.0622 - val_accuracy: 0.9767\n",
      "Epoch 855/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9815 - val_loss: 0.0630 - val_accuracy: 0.9757\n",
      "Epoch 856/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0563 - val_accuracy: 0.9773\n",
      "Epoch 857/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.0535 - val_accuracy: 0.9775\n",
      "Epoch 858/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9824 - val_loss: 0.0585 - val_accuracy: 0.9776\n",
      "Epoch 859/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9823 - val_loss: 0.0517 - val_accuracy: 0.9785\n",
      "Epoch 860/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.0489 - val_accuracy: 0.9792\n",
      "Epoch 861/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9808 - val_loss: 0.0604 - val_accuracy: 0.9759\n",
      "Epoch 862/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9817 - val_loss: 0.0629 - val_accuracy: 0.9758\n",
      "Epoch 863/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9814 - val_loss: 0.0541 - val_accuracy: 0.9776\n",
      "Epoch 864/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9825 - val_loss: 0.0532 - val_accuracy: 0.9771\n",
      "Epoch 865/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0644 - val_accuracy: 0.9746\n",
      "Epoch 866/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9815 - val_loss: 0.0547 - val_accuracy: 0.9779\n",
      "Epoch 867/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.0613 - val_accuracy: 0.9763\n",
      "Epoch 868/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.0550 - val_accuracy: 0.9781\n",
      "Epoch 869/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0582 - val_accuracy: 0.9765\n",
      "Epoch 870/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 0.0592 - val_accuracy: 0.9759\n",
      "Epoch 871/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9814 - val_loss: 0.0616 - val_accuracy: 0.9758\n",
      "Epoch 872/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9807 - val_loss: 0.0724 - val_accuracy: 0.9744\n",
      "Epoch 873/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0545 - val_accuracy: 0.9771\n",
      "Epoch 874/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9818 - val_loss: 0.0639 - val_accuracy: 0.9755\n",
      "Epoch 875/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9816 - val_loss: 0.0652 - val_accuracy: 0.9758\n",
      "Epoch 876/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.0545 - val_accuracy: 0.9767\n",
      "Epoch 877/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9822 - val_loss: 0.0654 - val_accuracy: 0.9755\n",
      "Epoch 878/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9807 - val_loss: 0.0668 - val_accuracy: 0.9751\n",
      "Epoch 879/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9815 - val_loss: 0.0667 - val_accuracy: 0.9745\n",
      "Epoch 880/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9821 - val_loss: 0.0606 - val_accuracy: 0.9765\n",
      "Epoch 881/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9825 - val_loss: 0.0593 - val_accuracy: 0.9767\n",
      "Epoch 882/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9824 - val_loss: 0.0586 - val_accuracy: 0.9763\n",
      "Epoch 883/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.0559 - val_accuracy: 0.9773\n",
      "Epoch 884/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0511 - val_accuracy: 0.9776\n",
      "Epoch 885/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.0643 - val_accuracy: 0.9753\n",
      "Epoch 886/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9809 - val_loss: 0.0614 - val_accuracy: 0.9758\n",
      "Epoch 887/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9815 - val_loss: 0.0732 - val_accuracy: 0.9737\n",
      "Epoch 888/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0527 - val_accuracy: 0.9786\n",
      "Epoch 889/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9825 - val_loss: 0.0561 - val_accuracy: 0.9767\n",
      "Epoch 890/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9824 - val_loss: 0.0523 - val_accuracy: 0.9786\n",
      "Epoch 891/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9810 - val_loss: 0.0601 - val_accuracy: 0.9758\n",
      "Epoch 892/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9813 - val_loss: 0.0564 - val_accuracy: 0.9768\n",
      "Epoch 893/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9821 - val_loss: 0.0611 - val_accuracy: 0.9766\n",
      "Epoch 894/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9818 - val_loss: 0.0556 - val_accuracy: 0.9774\n",
      "Epoch 895/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9817 - val_loss: 0.0581 - val_accuracy: 0.9763\n",
      "Epoch 896/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9821 - val_loss: 0.0598 - val_accuracy: 0.9771\n",
      "Epoch 897/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9809 - val_loss: 0.0620 - val_accuracy: 0.9755\n",
      "Epoch 898/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9818 - val_loss: 0.0528 - val_accuracy: 0.9784\n",
      "Epoch 899/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0544 - val_accuracy: 0.9784\n",
      "Epoch 900/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.0586 - val_accuracy: 0.9766\n",
      "Epoch 901/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0503 - accuracy: 0.9820 - val_loss: 0.0584 - val_accuracy: 0.9770\n",
      "Epoch 902/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9817 - val_loss: 0.0641 - val_accuracy: 0.9756\n",
      "Epoch 903/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0604 - val_accuracy: 0.9761\n",
      "Epoch 904/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9813 - val_loss: 0.0646 - val_accuracy: 0.9761\n",
      "Epoch 905/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9822 - val_loss: 0.0577 - val_accuracy: 0.9768\n",
      "Epoch 906/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0541 - val_accuracy: 0.9777\n",
      "Epoch 907/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0549 - val_accuracy: 0.9773\n",
      "Epoch 908/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9831 - val_loss: 0.0597 - val_accuracy: 0.9761\n",
      "Epoch 909/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0616 - val_accuracy: 0.9761\n",
      "Epoch 910/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9816 - val_loss: 0.0574 - val_accuracy: 0.9779\n",
      "Epoch 911/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9824 - val_loss: 0.0635 - val_accuracy: 0.9753\n",
      "Epoch 912/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9813 - val_loss: 0.0548 - val_accuracy: 0.9781\n",
      "Epoch 913/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9825 - val_loss: 0.0620 - val_accuracy: 0.9763\n",
      "Epoch 914/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0654 - val_accuracy: 0.9758\n",
      "Epoch 915/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9816 - val_loss: 0.0550 - val_accuracy: 0.9775\n",
      "Epoch 916/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9817 - val_loss: 0.0554 - val_accuracy: 0.9777\n",
      "Epoch 917/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9817 - val_loss: 0.0543 - val_accuracy: 0.9779\n",
      "Epoch 918/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9822 - val_loss: 0.0638 - val_accuracy: 0.9759\n",
      "Epoch 919/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9826 - val_loss: 0.0570 - val_accuracy: 0.9779\n",
      "Epoch 920/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9822 - val_loss: 0.0589 - val_accuracy: 0.9754\n",
      "Epoch 921/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9811 - val_loss: 0.0570 - val_accuracy: 0.9776\n",
      "Epoch 922/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0569 - val_accuracy: 0.9769\n",
      "Epoch 923/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9825 - val_loss: 0.0499 - val_accuracy: 0.9786\n",
      "Epoch 924/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9824 - val_loss: 0.0618 - val_accuracy: 0.9755\n",
      "Epoch 925/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9826 - val_loss: 0.0529 - val_accuracy: 0.9787\n",
      "Epoch 926/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9825 - val_loss: 0.0575 - val_accuracy: 0.9771\n",
      "Epoch 927/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9819 - val_loss: 0.0562 - val_accuracy: 0.9774\n",
      "Epoch 928/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0604 - val_accuracy: 0.9764\n",
      "Epoch 929/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9823 - val_loss: 0.0493 - val_accuracy: 0.9788\n",
      "Epoch 930/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9820 - val_loss: 0.0537 - val_accuracy: 0.9787\n",
      "Epoch 931/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9822 - val_loss: 0.0568 - val_accuracy: 0.9760\n",
      "Epoch 932/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0555 - val_accuracy: 0.9780\n",
      "Epoch 933/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.0679 - val_accuracy: 0.9747\n",
      "Epoch 934/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9819 - val_loss: 0.0617 - val_accuracy: 0.9757\n",
      "Epoch 935/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9814 - val_loss: 0.0640 - val_accuracy: 0.9766\n",
      "Epoch 936/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9819 - val_loss: 0.0584 - val_accuracy: 0.9767\n",
      "Epoch 937/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9824 - val_loss: 0.0605 - val_accuracy: 0.9765\n",
      "Epoch 938/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9828 - val_loss: 0.0510 - val_accuracy: 0.9780\n",
      "Epoch 939/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.0620 - val_accuracy: 0.9753\n",
      "Epoch 940/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9811 - val_loss: 0.0571 - val_accuracy: 0.9770\n",
      "Epoch 941/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.0638 - val_accuracy: 0.9757\n",
      "Epoch 942/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 0.0534 - val_accuracy: 0.9777\n",
      "Epoch 943/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9824 - val_loss: 0.0526 - val_accuracy: 0.9784\n",
      "Epoch 944/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9832 - val_loss: 0.0584 - val_accuracy: 0.9768\n",
      "Epoch 945/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9818 - val_loss: 0.0611 - val_accuracy: 0.9763\n",
      "Epoch 946/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9824 - val_loss: 0.0522 - val_accuracy: 0.9782\n",
      "Epoch 947/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.0635 - val_accuracy: 0.9758\n",
      "Epoch 948/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0575 - val_accuracy: 0.9770\n",
      "Epoch 949/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9811 - val_loss: 0.0589 - val_accuracy: 0.9773\n",
      "Epoch 950/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9818 - val_loss: 0.0576 - val_accuracy: 0.9763\n",
      "Epoch 951/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9828 - val_loss: 0.0559 - val_accuracy: 0.9779\n",
      "Epoch 952/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9828 - val_loss: 0.0551 - val_accuracy: 0.9781\n",
      "Epoch 953/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9824 - val_loss: 0.0581 - val_accuracy: 0.9776\n",
      "Epoch 954/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0549 - val_accuracy: 0.9769\n",
      "Epoch 955/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9819 - val_loss: 0.0570 - val_accuracy: 0.9765\n",
      "Epoch 956/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9825 - val_loss: 0.0589 - val_accuracy: 0.9763\n",
      "Epoch 957/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 0.0550 - val_accuracy: 0.9768\n",
      "Epoch 958/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9821 - val_loss: 0.0652 - val_accuracy: 0.9756\n",
      "Epoch 959/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0603 - val_accuracy: 0.9761\n",
      "Epoch 960/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9822 - val_loss: 0.0488 - val_accuracy: 0.9788\n",
      "Epoch 961/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9818 - val_loss: 0.0549 - val_accuracy: 0.9786\n",
      "Epoch 962/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9818 - val_loss: 0.0552 - val_accuracy: 0.9777\n",
      "Epoch 963/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9827 - val_loss: 0.0589 - val_accuracy: 0.9776\n",
      "Epoch 964/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9826 - val_loss: 0.0517 - val_accuracy: 0.9781\n",
      "Epoch 965/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9821 - val_loss: 0.0505 - val_accuracy: 0.9790\n",
      "Epoch 966/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0516 - val_accuracy: 0.9794\n",
      "Epoch 967/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9822 - val_loss: 0.0586 - val_accuracy: 0.9775\n",
      "Epoch 968/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9825 - val_loss: 0.0572 - val_accuracy: 0.9768\n",
      "Epoch 969/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9828 - val_loss: 0.0637 - val_accuracy: 0.9755\n",
      "Epoch 970/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9816 - val_loss: 0.0553 - val_accuracy: 0.9768\n",
      "Epoch 971/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9826 - val_loss: 0.0555 - val_accuracy: 0.9773\n",
      "Epoch 972/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9817 - val_loss: 0.0605 - val_accuracy: 0.9759\n",
      "Epoch 973/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9820 - val_loss: 0.0531 - val_accuracy: 0.9785\n",
      "Epoch 974/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9821 - val_loss: 0.0610 - val_accuracy: 0.9759\n",
      "Epoch 975/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9815 - val_loss: 0.0550 - val_accuracy: 0.9780\n",
      "Epoch 976/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9813 - val_loss: 0.0666 - val_accuracy: 0.9761\n",
      "Epoch 977/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9820 - val_loss: 0.0506 - val_accuracy: 0.9782\n",
      "Epoch 978/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9826 - val_loss: 0.0533 - val_accuracy: 0.9775\n",
      "Epoch 979/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9821 - val_loss: 0.0548 - val_accuracy: 0.9777\n",
      "Epoch 980/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9825 - val_loss: 0.0601 - val_accuracy: 0.9766\n",
      "Epoch 981/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9829 - val_loss: 0.0557 - val_accuracy: 0.9771\n",
      "Epoch 982/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0535 - val_accuracy: 0.9775\n",
      "Epoch 983/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9824 - val_loss: 0.0603 - val_accuracy: 0.9766\n",
      "Epoch 984/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9823 - val_loss: 0.0652 - val_accuracy: 0.9756\n",
      "Epoch 985/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0535 - val_accuracy: 0.9776\n",
      "Epoch 986/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0622 - val_accuracy: 0.9760\n",
      "Epoch 987/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9816 - val_loss: 0.0602 - val_accuracy: 0.9760\n",
      "Epoch 988/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9827 - val_loss: 0.0536 - val_accuracy: 0.9777\n",
      "Epoch 989/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.0552 - val_accuracy: 0.9775\n",
      "Epoch 990/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9827 - val_loss: 0.0512 - val_accuracy: 0.9788\n",
      "Epoch 991/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9819 - val_loss: 0.0584 - val_accuracy: 0.9756\n",
      "Epoch 992/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9822 - val_loss: 0.0619 - val_accuracy: 0.9759\n",
      "Epoch 993/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9825 - val_loss: 0.0655 - val_accuracy: 0.9756\n",
      "Epoch 994/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0568 - val_accuracy: 0.9771\n",
      "Epoch 995/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9825 - val_loss: 0.0548 - val_accuracy: 0.9776\n",
      "Epoch 996/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9830 - val_loss: 0.0582 - val_accuracy: 0.9760\n",
      "Epoch 997/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.0568 - val_accuracy: 0.9773\n",
      "Epoch 998/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9819 - val_loss: 0.0559 - val_accuracy: 0.9774\n",
      "Epoch 999/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9814 - val_loss: 0.0622 - val_accuracy: 0.9767\n",
      "Epoch 1000/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9819 - val_loss: 0.0529 - val_accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 970us/step\n",
      "xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,)\n",
      "\n",
      "classification_report_Fold=3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.98      0.99      8725\n",
      " Anomalous 1       0.63      0.83      0.72       287\n",
      "\n",
      "    accuracy                           0.98      9012\n",
      "   macro avg       0.81      0.91      0.85      9012\n",
      "weighted avg       0.98      0.98      0.98      9012\n",
      "\n",
      "confusion_matrix_Fold=3:\n",
      "\n",
      "True Negatives:  8585\n",
      "False Positives:  140\n",
      "False Negatives:  49\n",
      "True Positives:  238\n",
      "accuracy_score_Fold=3:\n",
      " 8823 \n",
      "\n",
      "End running time Fold=3: 210214_101717 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=4: 210214_101717 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 1.1209774885741043e-05, min: 2.6535811163314677e-06, max: 0.0002376395609542478\n",
      "End running time Data Augmentation_Fold=4: 210214_103651 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=4: 210214_103651 ,-------------------------- \n",
      "\n",
      "\n",
      " Number of Final yXtrain_Fold=4 labeled 0: 69796\n",
      "Number of Final yXtrain_Fold=4 labeled 1: 16072\n",
      "Number of Final yXtrain_Fold=4 labeled 2: 0 \n",
      "\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.8132 - val_loss: 0.2386 - val_accuracy: 0.9624\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8245 - val_loss: 0.2362 - val_accuracy: 0.9595\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.8253 - val_loss: 0.2444 - val_accuracy: 0.9522\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4173 - accuracy: 0.8273 - val_loss: 0.2141 - val_accuracy: 0.9542\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4096 - accuracy: 0.8296 - val_loss: 0.2057 - val_accuracy: 0.9574\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8279 - val_loss: 0.2154 - val_accuracy: 0.9556\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8275 - val_loss: 0.2031 - val_accuracy: 0.9554\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8330 - val_loss: 0.2032 - val_accuracy: 0.9551\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8335 - val_loss: 0.2329 - val_accuracy: 0.9478\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8324 - val_loss: 0.2393 - val_accuracy: 0.9484\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8347 - val_loss: 0.2059 - val_accuracy: 0.9532\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8340 - val_loss: 0.2124 - val_accuracy: 0.9485\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.8366 - val_loss: 0.2054 - val_accuracy: 0.9509\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8383 - val_loss: 0.2167 - val_accuracy: 0.9442\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8377 - val_loss: 0.2034 - val_accuracy: 0.9454\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8389 - val_loss: 0.2088 - val_accuracy: 0.9444\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8402 - val_loss: 0.1899 - val_accuracy: 0.9483\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8423 - val_loss: 0.2020 - val_accuracy: 0.9437\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.8416 - val_loss: 0.1773 - val_accuracy: 0.9518\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8446 - val_loss: 0.2154 - val_accuracy: 0.9371\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.8453 - val_loss: 0.1786 - val_accuracy: 0.9498\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8451 - val_loss: 0.1753 - val_accuracy: 0.9524\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8475 - val_loss: 0.1916 - val_accuracy: 0.9443\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8476 - val_loss: 0.1907 - val_accuracy: 0.9418\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8497 - val_loss: 0.1957 - val_accuracy: 0.9394\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8521 - val_loss: 0.1869 - val_accuracy: 0.9435\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8533 - val_loss: 0.2082 - val_accuracy: 0.9322\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8554 - val_loss: 0.1870 - val_accuracy: 0.9426\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3106 - accuracy: 0.8566 - val_loss: 0.1716 - val_accuracy: 0.9488\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8584 - val_loss: 0.1998 - val_accuracy: 0.9343\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8590 - val_loss: 0.1877 - val_accuracy: 0.9397\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8601 - val_loss: 0.1693 - val_accuracy: 0.9464\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8635 - val_loss: 0.2004 - val_accuracy: 0.9331\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8652 - val_loss: 0.2011 - val_accuracy: 0.9262\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8670 - val_loss: 0.1656 - val_accuracy: 0.9487\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2823 - accuracy: 0.8685 - val_loss: 0.1721 - val_accuracy: 0.9434\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8696 - val_loss: 0.1516 - val_accuracy: 0.9502\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8732 - val_loss: 0.1897 - val_accuracy: 0.9315\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8737 - val_loss: 0.1732 - val_accuracy: 0.9424\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8763 - val_loss: 0.1707 - val_accuracy: 0.9407\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8794 - val_loss: 0.1525 - val_accuracy: 0.9498\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8799 - val_loss: 0.1748 - val_accuracy: 0.9403\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8832 - val_loss: 0.1640 - val_accuracy: 0.9427\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2540 - accuracy: 0.8832 - val_loss: 0.1740 - val_accuracy: 0.9381\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8845 - val_loss: 0.1543 - val_accuracy: 0.9453\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8848 - val_loss: 0.1578 - val_accuracy: 0.9425\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8881 - val_loss: 0.1574 - val_accuracy: 0.9416\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2396 - accuracy: 0.8906 - val_loss: 0.1744 - val_accuracy: 0.9316\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.8927 - val_loss: 0.1433 - val_accuracy: 0.9498\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.8962 - val_loss: 0.1371 - val_accuracy: 0.9512\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.8952 - val_loss: 0.1550 - val_accuracy: 0.9426\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.8959 - val_loss: 0.1451 - val_accuracy: 0.9447\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.8981 - val_loss: 0.1337 - val_accuracy: 0.9509\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.8991 - val_loss: 0.1453 - val_accuracy: 0.9456\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2226 - accuracy: 0.9023 - val_loss: 0.1563 - val_accuracy: 0.9389\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9029 - val_loss: 0.1398 - val_accuracy: 0.9462\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9045 - val_loss: 0.1376 - val_accuracy: 0.9465\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9071 - val_loss: 0.1182 - val_accuracy: 0.9573\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9040 - val_loss: 0.1362 - val_accuracy: 0.9477\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9101 - val_loss: 0.1325 - val_accuracy: 0.9496\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9103 - val_loss: 0.1504 - val_accuracy: 0.9383\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9128 - val_loss: 0.1615 - val_accuracy: 0.9324\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9099 - val_loss: 0.1311 - val_accuracy: 0.9490\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9125 - val_loss: 0.1486 - val_accuracy: 0.9411\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9147 - val_loss: 0.1298 - val_accuracy: 0.9509\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9165 - val_loss: 0.1276 - val_accuracy: 0.9505\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9176 - val_loss: 0.1330 - val_accuracy: 0.9453\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9165 - val_loss: 0.1404 - val_accuracy: 0.9418\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.9180 - val_loss: 0.1360 - val_accuracy: 0.9456\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.9195 - val_loss: 0.1351 - val_accuracy: 0.9458\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9230 - val_loss: 0.1189 - val_accuracy: 0.9544\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.9227 - val_loss: 0.1458 - val_accuracy: 0.9385\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9214 - val_loss: 0.1265 - val_accuracy: 0.9485\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9219 - val_loss: 0.1332 - val_accuracy: 0.9463\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9232 - val_loss: 0.1086 - val_accuracy: 0.9558\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9274 - val_loss: 0.1062 - val_accuracy: 0.9594\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9266 - val_loss: 0.1085 - val_accuracy: 0.9585\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9259 - val_loss: 0.1091 - val_accuracy: 0.9563\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.9272 - val_loss: 0.1032 - val_accuracy: 0.9573\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9276 - val_loss: 0.1241 - val_accuracy: 0.9482\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9293 - val_loss: 0.1093 - val_accuracy: 0.9558\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.9305 - val_loss: 0.1258 - val_accuracy: 0.9488\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9314 - val_loss: 0.1140 - val_accuracy: 0.9501\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9297 - val_loss: 0.1065 - val_accuracy: 0.9561\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1651 - accuracy: 0.9319 - val_loss: 0.1171 - val_accuracy: 0.9517\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9310 - val_loss: 0.1041 - val_accuracy: 0.9565\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9340 - val_loss: 0.1166 - val_accuracy: 0.9495\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9348 - val_loss: 0.1059 - val_accuracy: 0.9563\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9354 - val_loss: 0.1195 - val_accuracy: 0.9500\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.9356 - val_loss: 0.0909 - val_accuracy: 0.9628\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1559 - accuracy: 0.9355 - val_loss: 0.1053 - val_accuracy: 0.9577\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1535 - accuracy: 0.9370 - val_loss: 0.1131 - val_accuracy: 0.9522\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1533 - accuracy: 0.9368 - val_loss: 0.1078 - val_accuracy: 0.9556\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9377 - val_loss: 0.1008 - val_accuracy: 0.9585\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1529 - accuracy: 0.9372 - val_loss: 0.1033 - val_accuracy: 0.9556\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.9387 - val_loss: 0.1038 - val_accuracy: 0.9572\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9386 - val_loss: 0.1030 - val_accuracy: 0.9568\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9402 - val_loss: 0.1004 - val_accuracy: 0.9578\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9431 - val_loss: 0.1010 - val_accuracy: 0.9581\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9409 - val_loss: 0.0980 - val_accuracy: 0.9588\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1460 - accuracy: 0.9413 - val_loss: 0.1249 - val_accuracy: 0.9475\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9439 - val_loss: 0.0961 - val_accuracy: 0.9599\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.9420 - val_loss: 0.0988 - val_accuracy: 0.9576\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.9430 - val_loss: 0.1186 - val_accuracy: 0.9498\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9431 - val_loss: 0.1108 - val_accuracy: 0.9536\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9445 - val_loss: 0.0907 - val_accuracy: 0.9633\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9447 - val_loss: 0.0983 - val_accuracy: 0.9584\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1366 - accuracy: 0.9445 - val_loss: 0.1056 - val_accuracy: 0.9558\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.9458 - val_loss: 0.0920 - val_accuracy: 0.9618\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9450 - val_loss: 0.0936 - val_accuracy: 0.9599\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9461 - val_loss: 0.0938 - val_accuracy: 0.9628\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9466 - val_loss: 0.1026 - val_accuracy: 0.9555\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9465 - val_loss: 0.1017 - val_accuracy: 0.9577\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9471 - val_loss: 0.0945 - val_accuracy: 0.9614\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1297 - accuracy: 0.9487 - val_loss: 0.0797 - val_accuracy: 0.9669\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9474 - val_loss: 0.1360 - val_accuracy: 0.9413\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9482 - val_loss: 0.0830 - val_accuracy: 0.9666\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9463 - val_loss: 0.0935 - val_accuracy: 0.9610\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9502 - val_loss: 0.0975 - val_accuracy: 0.9588\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9479 - val_loss: 0.1154 - val_accuracy: 0.9506\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9484 - val_loss: 0.0854 - val_accuracy: 0.9655\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9506 - val_loss: 0.0911 - val_accuracy: 0.9620\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9497 - val_loss: 0.0818 - val_accuracy: 0.9663\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9499 - val_loss: 0.1088 - val_accuracy: 0.9541\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1244 - accuracy: 0.9515 - val_loss: 0.0931 - val_accuracy: 0.9610\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9508 - val_loss: 0.0730 - val_accuracy: 0.9687\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9519 - val_loss: 0.0863 - val_accuracy: 0.9657\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9524 - val_loss: 0.0825 - val_accuracy: 0.9663\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1220 - accuracy: 0.9529 - val_loss: 0.0961 - val_accuracy: 0.9591\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.9529 - val_loss: 0.0831 - val_accuracy: 0.9654\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9532 - val_loss: 0.0875 - val_accuracy: 0.9632\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9540 - val_loss: 0.0784 - val_accuracy: 0.9687\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9536 - val_loss: 0.1123 - val_accuracy: 0.9511\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1183 - accuracy: 0.9546 - val_loss: 0.0852 - val_accuracy: 0.9645\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9553 - val_loss: 0.0971 - val_accuracy: 0.9579\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9541 - val_loss: 0.0924 - val_accuracy: 0.9615\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9542 - val_loss: 0.0833 - val_accuracy: 0.9640\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9549 - val_loss: 0.0804 - val_accuracy: 0.9673\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9557 - val_loss: 0.0859 - val_accuracy: 0.9636\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9551 - val_loss: 0.0845 - val_accuracy: 0.9657\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.9545 - val_loss: 0.0799 - val_accuracy: 0.9673\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9575 - val_loss: 0.0823 - val_accuracy: 0.9658\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1145 - accuracy: 0.9560 - val_loss: 0.0911 - val_accuracy: 0.9622\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1115 - accuracy: 0.9579 - val_loss: 0.0892 - val_accuracy: 0.9618\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9566 - val_loss: 0.0840 - val_accuracy: 0.9655\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 0.9575 - val_loss: 0.0943 - val_accuracy: 0.9586\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9573 - val_loss: 0.0943 - val_accuracy: 0.9613\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9574 - val_loss: 0.0811 - val_accuracy: 0.9666\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1095 - accuracy: 0.9581 - val_loss: 0.0740 - val_accuracy: 0.9700\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9564 - val_loss: 0.0738 - val_accuracy: 0.9707\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9584 - val_loss: 0.0924 - val_accuracy: 0.9607\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.9583 - val_loss: 0.0863 - val_accuracy: 0.9643\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1094 - accuracy: 0.9584 - val_loss: 0.0753 - val_accuracy: 0.9698\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9601 - val_loss: 0.0818 - val_accuracy: 0.9652\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.9590 - val_loss: 0.0963 - val_accuracy: 0.9597\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9612 - val_loss: 0.0836 - val_accuracy: 0.9646\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1073 - accuracy: 0.9591 - val_loss: 0.0849 - val_accuracy: 0.9633\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9596 - val_loss: 0.0682 - val_accuracy: 0.9727\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9601 - val_loss: 0.0732 - val_accuracy: 0.9698\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9606 - val_loss: 0.0715 - val_accuracy: 0.9699\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9586 - val_loss: 0.0973 - val_accuracy: 0.9585\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9617 - val_loss: 0.0983 - val_accuracy: 0.9587\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9614 - val_loss: 0.0819 - val_accuracy: 0.9652\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.9619 - val_loss: 0.0939 - val_accuracy: 0.9605\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1064 - accuracy: 0.9599 - val_loss: 0.0809 - val_accuracy: 0.9658\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9628 - val_loss: 0.0784 - val_accuracy: 0.9674\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9630 - val_loss: 0.0855 - val_accuracy: 0.9634\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9622 - val_loss: 0.0719 - val_accuracy: 0.9708\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9632 - val_loss: 0.0921 - val_accuracy: 0.9599\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.9635 - val_loss: 0.0857 - val_accuracy: 0.9626\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9614 - val_loss: 0.0733 - val_accuracy: 0.9699\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9635 - val_loss: 0.0832 - val_accuracy: 0.9643\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9636 - val_loss: 0.0656 - val_accuracy: 0.9730\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9634 - val_loss: 0.0857 - val_accuracy: 0.9618\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9641 - val_loss: 0.0740 - val_accuracy: 0.9690\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9631 - val_loss: 0.0764 - val_accuracy: 0.9685\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9642 - val_loss: 0.0856 - val_accuracy: 0.9637\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9628 - val_loss: 0.0867 - val_accuracy: 0.9610\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9637 - val_loss: 0.0858 - val_accuracy: 0.9625\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9654 - val_loss: 0.0686 - val_accuracy: 0.9718\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9646 - val_loss: 0.0779 - val_accuracy: 0.9662\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9656 - val_loss: 0.0726 - val_accuracy: 0.9705\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9634 - val_loss: 0.0831 - val_accuracy: 0.9643\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9655 - val_loss: 0.0857 - val_accuracy: 0.9617\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9638 - val_loss: 0.0800 - val_accuracy: 0.9664\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9668 - val_loss: 0.0781 - val_accuracy: 0.9672\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9649 - val_loss: 0.0730 - val_accuracy: 0.9687\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9649 - val_loss: 0.0752 - val_accuracy: 0.9682\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9651 - val_loss: 0.0649 - val_accuracy: 0.9733\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9665 - val_loss: 0.0832 - val_accuracy: 0.9636\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9659 - val_loss: 0.0902 - val_accuracy: 0.9632\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9652 - val_loss: 0.0710 - val_accuracy: 0.9701\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9667 - val_loss: 0.0799 - val_accuracy: 0.9658\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0948 - accuracy: 0.9659 - val_loss: 0.0722 - val_accuracy: 0.9687\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9672 - val_loss: 0.0710 - val_accuracy: 0.9701\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9680 - val_loss: 0.0782 - val_accuracy: 0.9662\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9663 - val_loss: 0.0722 - val_accuracy: 0.9695\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9671 - val_loss: 0.0919 - val_accuracy: 0.9604\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9678 - val_loss: 0.0690 - val_accuracy: 0.9699\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9667 - val_loss: 0.0751 - val_accuracy: 0.9693\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9667 - val_loss: 0.0752 - val_accuracy: 0.9675\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9676 - val_loss: 0.0818 - val_accuracy: 0.9649\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9677 - val_loss: 0.0741 - val_accuracy: 0.9684\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9674 - val_loss: 0.0794 - val_accuracy: 0.9663\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9681 - val_loss: 0.0777 - val_accuracy: 0.9673\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9689 - val_loss: 0.0754 - val_accuracy: 0.9680\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9681 - val_loss: 0.0657 - val_accuracy: 0.9719\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9669 - val_loss: 0.0715 - val_accuracy: 0.9711\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9680 - val_loss: 0.0705 - val_accuracy: 0.9699\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9686 - val_loss: 0.0757 - val_accuracy: 0.9677\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9694 - val_loss: 0.0722 - val_accuracy: 0.9695\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9683 - val_loss: 0.0595 - val_accuracy: 0.9754\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9683 - val_loss: 0.0884 - val_accuracy: 0.9624\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9670 - val_loss: 0.0708 - val_accuracy: 0.9693\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9689 - val_loss: 0.0693 - val_accuracy: 0.9700\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9701 - val_loss: 0.0758 - val_accuracy: 0.9667\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9686 - val_loss: 0.0762 - val_accuracy: 0.9670\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9685 - val_loss: 0.0766 - val_accuracy: 0.9666\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9688 - val_loss: 0.0839 - val_accuracy: 0.9630\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9701 - val_loss: 0.0684 - val_accuracy: 0.9707\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9691 - val_loss: 0.0743 - val_accuracy: 0.9682\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0871 - accuracy: 0.9688 - val_loss: 0.0639 - val_accuracy: 0.9728\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9701 - val_loss: 0.0746 - val_accuracy: 0.9676\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9692 - val_loss: 0.0684 - val_accuracy: 0.9705\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9695 - val_loss: 0.0723 - val_accuracy: 0.9695\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9706 - val_loss: 0.0565 - val_accuracy: 0.9767\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9686 - val_loss: 0.0773 - val_accuracy: 0.9676\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9677 - val_loss: 0.0698 - val_accuracy: 0.9701\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9697 - val_loss: 0.0703 - val_accuracy: 0.9701\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9702 - val_loss: 0.0732 - val_accuracy: 0.9691\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9702 - val_loss: 0.0716 - val_accuracy: 0.9680\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9700 - val_loss: 0.0708 - val_accuracy: 0.9707\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9699 - val_loss: 0.0752 - val_accuracy: 0.9677\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9700 - val_loss: 0.0709 - val_accuracy: 0.9688\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9708 - val_loss: 0.0801 - val_accuracy: 0.9648\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9700 - val_loss: 0.0648 - val_accuracy: 0.9724\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9706 - val_loss: 0.0763 - val_accuracy: 0.9668\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9711 - val_loss: 0.0707 - val_accuracy: 0.9707\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.9710 - val_loss: 0.0666 - val_accuracy: 0.9717\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9699 - val_loss: 0.0662 - val_accuracy: 0.9725\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9710 - val_loss: 0.0686 - val_accuracy: 0.9705\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9707 - val_loss: 0.0631 - val_accuracy: 0.9734\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9708 - val_loss: 0.0749 - val_accuracy: 0.9678\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9708 - val_loss: 0.0818 - val_accuracy: 0.9645\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9719 - val_loss: 0.0803 - val_accuracy: 0.9649\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9712 - val_loss: 0.0686 - val_accuracy: 0.9707\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9724 - val_loss: 0.0817 - val_accuracy: 0.9655\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9708 - val_loss: 0.0661 - val_accuracy: 0.9716\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 0.0778 - val_accuracy: 0.9652\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0808 - accuracy: 0.9723 - val_loss: 0.0778 - val_accuracy: 0.9655\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9713 - val_loss: 0.0731 - val_accuracy: 0.9680\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9712 - val_loss: 0.0711 - val_accuracy: 0.9694\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9741 - val_loss: 0.0703 - val_accuracy: 0.9704\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9732 - val_loss: 0.0743 - val_accuracy: 0.9678\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9715 - val_loss: 0.0952 - val_accuracy: 0.9596\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9710 - val_loss: 0.0553 - val_accuracy: 0.9780\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9718 - val_loss: 0.0688 - val_accuracy: 0.9710\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9729 - val_loss: 0.0760 - val_accuracy: 0.9673\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9720 - val_loss: 0.0693 - val_accuracy: 0.9700\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9719 - val_loss: 0.0725 - val_accuracy: 0.9682\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9731 - val_loss: 0.0672 - val_accuracy: 0.9711\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9732 - val_loss: 0.0623 - val_accuracy: 0.9736\n",
      "Epoch 263/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9729 - val_loss: 0.0601 - val_accuracy: 0.9738\n",
      "Epoch 264/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9735 - val_loss: 0.0698 - val_accuracy: 0.9705\n",
      "Epoch 265/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9730 - val_loss: 0.0576 - val_accuracy: 0.9755\n",
      "Epoch 266/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9721 - val_loss: 0.0674 - val_accuracy: 0.9707\n",
      "Epoch 267/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9711 - val_loss: 0.0586 - val_accuracy: 0.9756\n",
      "Epoch 268/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9733 - val_loss: 0.0591 - val_accuracy: 0.9741\n",
      "Epoch 269/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9718 - val_loss: 0.0575 - val_accuracy: 0.9755\n",
      "Epoch 270/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9729 - val_loss: 0.0683 - val_accuracy: 0.9703\n",
      "Epoch 271/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.9731 - val_loss: 0.0697 - val_accuracy: 0.9706\n",
      "Epoch 272/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9734 - val_loss: 0.0652 - val_accuracy: 0.9726\n",
      "Epoch 273/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9745 - val_loss: 0.0736 - val_accuracy: 0.9684\n",
      "Epoch 274/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9727 - val_loss: 0.0788 - val_accuracy: 0.9663\n",
      "Epoch 275/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9738 - val_loss: 0.0658 - val_accuracy: 0.9719\n",
      "Epoch 276/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9733 - val_loss: 0.0740 - val_accuracy: 0.9698\n",
      "Epoch 277/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9735 - val_loss: 0.0690 - val_accuracy: 0.9715\n",
      "Epoch 278/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9741 - val_loss: 0.0638 - val_accuracy: 0.9726\n",
      "Epoch 279/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9730 - val_loss: 0.0718 - val_accuracy: 0.9696\n",
      "Epoch 280/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.9735 - val_loss: 0.0927 - val_accuracy: 0.9616\n",
      "Epoch 281/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9731 - val_loss: 0.0652 - val_accuracy: 0.9717\n",
      "Epoch 282/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.0639 - val_accuracy: 0.9726\n",
      "Epoch 283/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9741 - val_loss: 0.0687 - val_accuracy: 0.9725\n",
      "Epoch 284/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9745 - val_loss: 0.0692 - val_accuracy: 0.9700\n",
      "Epoch 285/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9740 - val_loss: 0.0580 - val_accuracy: 0.9758\n",
      "Epoch 286/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9740 - val_loss: 0.0596 - val_accuracy: 0.9746\n",
      "Epoch 287/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9735 - val_loss: 0.0694 - val_accuracy: 0.9711\n",
      "Epoch 288/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9737 - val_loss: 0.0674 - val_accuracy: 0.9710\n",
      "Epoch 289/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9747 - val_loss: 0.0659 - val_accuracy: 0.9718\n",
      "Epoch 290/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9746 - val_loss: 0.0605 - val_accuracy: 0.9743\n",
      "Epoch 291/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9738 - val_loss: 0.0653 - val_accuracy: 0.9728\n",
      "Epoch 292/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9744 - val_loss: 0.0671 - val_accuracy: 0.9716\n",
      "Epoch 293/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9738 - val_loss: 0.0744 - val_accuracy: 0.9697\n",
      "Epoch 294/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9745 - val_loss: 0.0711 - val_accuracy: 0.9707\n",
      "Epoch 295/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0747 - accuracy: 0.9739 - val_loss: 0.0714 - val_accuracy: 0.9703\n",
      "Epoch 296/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9746 - val_loss: 0.0799 - val_accuracy: 0.9665\n",
      "Epoch 297/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9744 - val_loss: 0.0748 - val_accuracy: 0.9687\n",
      "Epoch 298/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9747 - val_loss: 0.0623 - val_accuracy: 0.9739\n",
      "Epoch 299/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9751 - val_loss: 0.0642 - val_accuracy: 0.9729\n",
      "Epoch 300/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9742 - val_loss: 0.0663 - val_accuracy: 0.9727\n",
      "Epoch 301/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9737 - val_loss: 0.0692 - val_accuracy: 0.9728\n",
      "Epoch 302/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9752 - val_loss: 0.0668 - val_accuracy: 0.9718\n",
      "Epoch 303/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.0594 - val_accuracy: 0.9747\n",
      "Epoch 304/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9742 - val_loss: 0.0643 - val_accuracy: 0.9730\n",
      "Epoch 305/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9751 - val_loss: 0.0727 - val_accuracy: 0.9698\n",
      "Epoch 306/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9755 - val_loss: 0.0593 - val_accuracy: 0.9757\n",
      "Epoch 307/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9757 - val_loss: 0.0722 - val_accuracy: 0.9711\n",
      "Epoch 308/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9741 - val_loss: 0.0685 - val_accuracy: 0.9723\n",
      "Epoch 309/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9755 - val_loss: 0.0758 - val_accuracy: 0.9695\n",
      "Epoch 310/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9753 - val_loss: 0.0631 - val_accuracy: 0.9741\n",
      "Epoch 311/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9767 - val_loss: 0.0610 - val_accuracy: 0.9749\n",
      "Epoch 312/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9759 - val_loss: 0.0750 - val_accuracy: 0.9690\n",
      "Epoch 313/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9745 - val_loss: 0.0673 - val_accuracy: 0.9739\n",
      "Epoch 314/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9762 - val_loss: 0.0556 - val_accuracy: 0.9758\n",
      "Epoch 315/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9750 - val_loss: 0.0566 - val_accuracy: 0.9758\n",
      "Epoch 316/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9753 - val_loss: 0.0683 - val_accuracy: 0.9718\n",
      "Epoch 317/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9756 - val_loss: 0.0571 - val_accuracy: 0.9758\n",
      "Epoch 318/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9760 - val_loss: 0.0700 - val_accuracy: 0.9714\n",
      "Epoch 319/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9751 - val_loss: 0.0619 - val_accuracy: 0.9744\n",
      "Epoch 320/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 0.0625 - val_accuracy: 0.9743\n",
      "Epoch 321/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9762 - val_loss: 0.0615 - val_accuracy: 0.9739\n",
      "Epoch 322/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9768 - val_loss: 0.0636 - val_accuracy: 0.9744\n",
      "Epoch 323/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9761 - val_loss: 0.0604 - val_accuracy: 0.9749\n",
      "Epoch 324/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9760 - val_loss: 0.0681 - val_accuracy: 0.9726\n",
      "Epoch 325/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9754 - val_loss: 0.0754 - val_accuracy: 0.9694\n",
      "Epoch 326/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9753 - val_loss: 0.0656 - val_accuracy: 0.9734\n",
      "Epoch 327/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9755 - val_loss: 0.0684 - val_accuracy: 0.9724\n",
      "Epoch 328/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9755 - val_loss: 0.0654 - val_accuracy: 0.9729\n",
      "Epoch 329/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9765 - val_loss: 0.0691 - val_accuracy: 0.9724\n",
      "Epoch 330/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9760 - val_loss: 0.0770 - val_accuracy: 0.9699\n",
      "Epoch 331/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9761 - val_loss: 0.0604 - val_accuracy: 0.9748\n",
      "Epoch 332/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9765 - val_loss: 0.0631 - val_accuracy: 0.9735\n",
      "Epoch 333/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9768 - val_loss: 0.0607 - val_accuracy: 0.9757\n",
      "Epoch 334/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9758 - val_loss: 0.0612 - val_accuracy: 0.9757\n",
      "Epoch 335/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9762 - val_loss: 0.0618 - val_accuracy: 0.9743\n",
      "Epoch 336/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9776 - val_loss: 0.0716 - val_accuracy: 0.9721\n",
      "Epoch 337/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9767 - val_loss: 0.0603 - val_accuracy: 0.9760\n",
      "Epoch 338/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9764 - val_loss: 0.0668 - val_accuracy: 0.9730\n",
      "Epoch 339/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9769 - val_loss: 0.0641 - val_accuracy: 0.9729\n",
      "Epoch 340/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9765 - val_loss: 0.0567 - val_accuracy: 0.9767\n",
      "Epoch 341/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9765 - val_loss: 0.0813 - val_accuracy: 0.9683\n",
      "Epoch 342/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9771 - val_loss: 0.0694 - val_accuracy: 0.9720\n",
      "Epoch 343/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9767 - val_loss: 0.0637 - val_accuracy: 0.9740\n",
      "Epoch 344/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9763 - val_loss: 0.0700 - val_accuracy: 0.9720\n",
      "Epoch 345/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9771 - val_loss: 0.0623 - val_accuracy: 0.9746\n",
      "Epoch 346/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9752 - val_loss: 0.0699 - val_accuracy: 0.9715\n",
      "Epoch 347/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9769 - val_loss: 0.0710 - val_accuracy: 0.9717\n",
      "Epoch 348/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9765 - val_loss: 0.0676 - val_accuracy: 0.9728\n",
      "Epoch 349/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9765 - val_loss: 0.0547 - val_accuracy: 0.9767\n",
      "Epoch 350/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9767 - val_loss: 0.0574 - val_accuracy: 0.9759\n",
      "Epoch 351/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.0571 - val_accuracy: 0.9767\n",
      "Epoch 352/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9766 - val_loss: 0.0606 - val_accuracy: 0.9753\n",
      "Epoch 353/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9771 - val_loss: 0.0592 - val_accuracy: 0.9757\n",
      "Epoch 354/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9774 - val_loss: 0.0638 - val_accuracy: 0.9747\n",
      "Epoch 355/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9768 - val_loss: 0.0583 - val_accuracy: 0.9759\n",
      "Epoch 356/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9773 - val_loss: 0.0604 - val_accuracy: 0.9749\n",
      "Epoch 357/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9774 - val_loss: 0.0784 - val_accuracy: 0.9698\n",
      "Epoch 358/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9767 - val_loss: 0.0654 - val_accuracy: 0.9735\n",
      "Epoch 359/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9770 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 360/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9772 - val_loss: 0.0599 - val_accuracy: 0.9749\n",
      "Epoch 361/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9764 - val_loss: 0.0734 - val_accuracy: 0.9724\n",
      "Epoch 362/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.0571 - val_accuracy: 0.9761\n",
      "Epoch 363/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 0.0689 - val_accuracy: 0.9734\n",
      "Epoch 364/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9776 - val_loss: 0.0581 - val_accuracy: 0.9751\n",
      "Epoch 365/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9772 - val_loss: 0.0682 - val_accuracy: 0.9729\n",
      "Epoch 366/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9767 - val_loss: 0.0820 - val_accuracy: 0.9680\n",
      "Epoch 367/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9764 - val_loss: 0.0617 - val_accuracy: 0.9760\n",
      "Epoch 368/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9774 - val_loss: 0.0604 - val_accuracy: 0.9745\n",
      "Epoch 369/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9771 - val_loss: 0.0709 - val_accuracy: 0.9720\n",
      "Epoch 370/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0685 - accuracy: 0.9763 - val_loss: 0.0578 - val_accuracy: 0.9754\n",
      "Epoch 371/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0649 - accuracy: 0.9775 - val_loss: 0.0665 - val_accuracy: 0.9730\n",
      "Epoch 372/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.0695 - val_accuracy: 0.9730\n",
      "Epoch 373/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0643 - accuracy: 0.9784 - val_loss: 0.0758 - val_accuracy: 0.9709\n",
      "Epoch 374/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.0582 - val_accuracy: 0.9756\n",
      "Epoch 375/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0658 - accuracy: 0.9773 - val_loss: 0.0623 - val_accuracy: 0.9745\n",
      "Epoch 376/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0668 - accuracy: 0.9770 - val_loss: 0.0695 - val_accuracy: 0.9735\n",
      "Epoch 377/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0661 - accuracy: 0.9778 - val_loss: 0.0566 - val_accuracy: 0.9758\n",
      "Epoch 378/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0673 - accuracy: 0.9768 - val_loss: 0.0543 - val_accuracy: 0.9781\n",
      "Epoch 379/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9771 - val_loss: 0.0732 - val_accuracy: 0.9716\n",
      "Epoch 380/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0661 - accuracy: 0.9775 - val_loss: 0.0601 - val_accuracy: 0.9755\n",
      "Epoch 381/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0661 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9718\n",
      "Epoch 382/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0666 - accuracy: 0.9771 - val_loss: 0.0610 - val_accuracy: 0.9758\n",
      "Epoch 383/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0650 - accuracy: 0.9775 - val_loss: 0.0597 - val_accuracy: 0.9757\n",
      "Epoch 384/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0657 - accuracy: 0.9781 - val_loss: 0.0631 - val_accuracy: 0.9749\n",
      "Epoch 385/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0646 - accuracy: 0.9778 - val_loss: 0.0669 - val_accuracy: 0.9744\n",
      "Epoch 386/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0653 - accuracy: 0.9773 - val_loss: 0.0699 - val_accuracy: 0.9734\n",
      "Epoch 387/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0645 - accuracy: 0.9778 - val_loss: 0.0682 - val_accuracy: 0.9740\n",
      "Epoch 388/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0652 - accuracy: 0.9777 - val_loss: 0.0583 - val_accuracy: 0.9764\n",
      "Epoch 389/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9766 - val_loss: 0.0630 - val_accuracy: 0.9745\n",
      "Epoch 390/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9781 - val_loss: 0.0645 - val_accuracy: 0.9748\n",
      "Epoch 391/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9778 - val_loss: 0.0635 - val_accuracy: 0.9751\n",
      "Epoch 392/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9790 - val_loss: 0.0677 - val_accuracy: 0.9729\n",
      "Epoch 393/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9779 - val_loss: 0.0639 - val_accuracy: 0.9747\n",
      "Epoch 394/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 0.0712 - val_accuracy: 0.9714\n",
      "Epoch 395/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9778 - val_loss: 0.0652 - val_accuracy: 0.9744\n",
      "Epoch 396/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0641 - accuracy: 0.9779 - val_loss: 0.0679 - val_accuracy: 0.9734\n",
      "Epoch 397/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9772 - val_loss: 0.0639 - val_accuracy: 0.9745\n",
      "Epoch 398/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9792 - val_loss: 0.0645 - val_accuracy: 0.9754\n",
      "Epoch 399/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9778 - val_loss: 0.0559 - val_accuracy: 0.9768\n",
      "Epoch 400/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0577 - val_accuracy: 0.9768\n",
      "Epoch 401/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9771 - val_loss: 0.0624 - val_accuracy: 0.9750\n",
      "Epoch 402/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9775 - val_loss: 0.0634 - val_accuracy: 0.9746\n",
      "Epoch 403/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9770 - val_loss: 0.0663 - val_accuracy: 0.9745\n",
      "Epoch 404/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9773 - val_loss: 0.0667 - val_accuracy: 0.9744\n",
      "Epoch 405/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9786 - val_loss: 0.0552 - val_accuracy: 0.9766\n",
      "Epoch 406/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.0670 - val_accuracy: 0.9749\n",
      "Epoch 407/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.0687 - val_accuracy: 0.9745\n",
      "Epoch 408/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9779 - val_loss: 0.0648 - val_accuracy: 0.9750\n",
      "Epoch 409/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9792 - val_loss: 0.0571 - val_accuracy: 0.9770\n",
      "Epoch 410/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9791 - val_loss: 0.0563 - val_accuracy: 0.9770\n",
      "Epoch 411/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9777 - val_loss: 0.0563 - val_accuracy: 0.9768\n",
      "Epoch 412/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9785 - val_loss: 0.0675 - val_accuracy: 0.9736\n",
      "Epoch 413/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9773 - val_loss: 0.0630 - val_accuracy: 0.9750\n",
      "Epoch 414/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9779 - val_loss: 0.0607 - val_accuracy: 0.9755\n",
      "Epoch 415/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9779 - val_loss: 0.0569 - val_accuracy: 0.9759\n",
      "Epoch 416/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9784 - val_loss: 0.0544 - val_accuracy: 0.9775\n",
      "Epoch 417/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.0616 - val_accuracy: 0.9753\n",
      "Epoch 418/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9777 - val_loss: 0.0662 - val_accuracy: 0.9751\n",
      "Epoch 419/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.0528 - val_accuracy: 0.9774\n",
      "Epoch 420/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9774 - val_loss: 0.0699 - val_accuracy: 0.9743\n",
      "Epoch 421/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9773 - val_loss: 0.0607 - val_accuracy: 0.9748\n",
      "Epoch 422/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9783 - val_loss: 0.0623 - val_accuracy: 0.9757\n",
      "Epoch 423/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9784 - val_loss: 0.0583 - val_accuracy: 0.9757\n",
      "Epoch 424/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9781 - val_loss: 0.0610 - val_accuracy: 0.9753\n",
      "Epoch 425/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9785 - val_loss: 0.0611 - val_accuracy: 0.9755\n",
      "Epoch 426/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9779 - val_loss: 0.0641 - val_accuracy: 0.9747\n",
      "Epoch 427/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9787 - val_loss: 0.0682 - val_accuracy: 0.9739\n",
      "Epoch 428/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9783 - val_loss: 0.0638 - val_accuracy: 0.9745\n",
      "Epoch 429/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9788 - val_loss: 0.0707 - val_accuracy: 0.9734\n",
      "Epoch 430/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9779 - val_loss: 0.0591 - val_accuracy: 0.9759\n",
      "Epoch 431/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9784 - val_loss: 0.0681 - val_accuracy: 0.9739\n",
      "Epoch 432/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9773 - val_loss: 0.0679 - val_accuracy: 0.9728\n",
      "Epoch 433/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9788 - val_loss: 0.0660 - val_accuracy: 0.9750\n",
      "Epoch 434/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9794 - val_loss: 0.0538 - val_accuracy: 0.9777\n",
      "Epoch 435/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9790 - val_loss: 0.0674 - val_accuracy: 0.9739\n",
      "Epoch 436/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.0575 - val_accuracy: 0.9764\n",
      "Epoch 437/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.0604 - val_accuracy: 0.9759\n",
      "Epoch 438/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9776 - val_loss: 0.0679 - val_accuracy: 0.9739\n",
      "Epoch 439/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9774 - val_loss: 0.0615 - val_accuracy: 0.9755\n",
      "Epoch 440/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9785 - val_loss: 0.0678 - val_accuracy: 0.9737\n",
      "Epoch 441/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9788 - val_loss: 0.0704 - val_accuracy: 0.9735\n",
      "Epoch 442/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9776 - val_loss: 0.0644 - val_accuracy: 0.9753\n",
      "Epoch 443/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9784 - val_loss: 0.0674 - val_accuracy: 0.9746\n",
      "Epoch 444/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9778 - val_loss: 0.0616 - val_accuracy: 0.9763\n",
      "Epoch 445/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9785 - val_loss: 0.0667 - val_accuracy: 0.9744\n",
      "Epoch 446/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9794 - val_loss: 0.0565 - val_accuracy: 0.9768\n",
      "Epoch 447/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9791 - val_loss: 0.0599 - val_accuracy: 0.9763\n",
      "Epoch 448/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9788 - val_loss: 0.0569 - val_accuracy: 0.9770\n",
      "Epoch 449/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9789 - val_loss: 0.0769 - val_accuracy: 0.9714\n",
      "Epoch 450/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 0.0548 - val_accuracy: 0.9774\n",
      "Epoch 451/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9791 - val_loss: 0.0705 - val_accuracy: 0.9736\n",
      "Epoch 452/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0629 - val_accuracy: 0.9753\n",
      "Epoch 453/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9788 - val_loss: 0.0623 - val_accuracy: 0.9755\n",
      "Epoch 454/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9791 - val_loss: 0.0651 - val_accuracy: 0.9749\n",
      "Epoch 455/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9792 - val_loss: 0.0531 - val_accuracy: 0.9776\n",
      "Epoch 456/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.0586 - val_accuracy: 0.9767\n",
      "Epoch 457/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9794 - val_loss: 0.0582 - val_accuracy: 0.9765\n",
      "Epoch 458/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9788 - val_loss: 0.0593 - val_accuracy: 0.9763\n",
      "Epoch 459/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9783 - val_loss: 0.0664 - val_accuracy: 0.9748\n",
      "Epoch 460/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9791 - val_loss: 0.0621 - val_accuracy: 0.9750\n",
      "Epoch 461/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9787 - val_loss: 0.0571 - val_accuracy: 0.9755\n",
      "Epoch 462/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9794 - val_loss: 0.0587 - val_accuracy: 0.9768\n",
      "Epoch 463/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9782 - val_loss: 0.0631 - val_accuracy: 0.9750\n",
      "Epoch 464/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9792 - val_loss: 0.0574 - val_accuracy: 0.9768\n",
      "Epoch 465/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9780 - val_loss: 0.0633 - val_accuracy: 0.9755\n",
      "Epoch 466/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9784 - val_loss: 0.0665 - val_accuracy: 0.9744\n",
      "Epoch 467/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9788 - val_loss: 0.0650 - val_accuracy: 0.9749\n",
      "Epoch 468/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9786 - val_loss: 0.0596 - val_accuracy: 0.9761\n",
      "Epoch 469/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9790 - val_loss: 0.0544 - val_accuracy: 0.9768\n",
      "Epoch 470/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9790 - val_loss: 0.0726 - val_accuracy: 0.9731\n",
      "Epoch 471/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9782 - val_loss: 0.0603 - val_accuracy: 0.9766\n",
      "Epoch 472/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9793 - val_loss: 0.0701 - val_accuracy: 0.9740\n",
      "Epoch 473/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9777 - val_loss: 0.0545 - val_accuracy: 0.9775\n",
      "Epoch 474/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9790 - val_loss: 0.0631 - val_accuracy: 0.9758\n",
      "Epoch 475/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9794 - val_loss: 0.0669 - val_accuracy: 0.9739\n",
      "Epoch 476/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9783 - val_loss: 0.0535 - val_accuracy: 0.9776\n",
      "Epoch 477/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9782 - val_loss: 0.0549 - val_accuracy: 0.9784\n",
      "Epoch 478/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9788 - val_loss: 0.0633 - val_accuracy: 0.9758\n",
      "Epoch 479/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9797 - val_loss: 0.0605 - val_accuracy: 0.9758\n",
      "Epoch 480/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9793 - val_loss: 0.0643 - val_accuracy: 0.9750\n",
      "Epoch 481/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9795 - val_loss: 0.0558 - val_accuracy: 0.9770\n",
      "Epoch 482/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9782 - val_loss: 0.0628 - val_accuracy: 0.9759\n",
      "Epoch 483/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9794 - val_loss: 0.0597 - val_accuracy: 0.9757\n",
      "Epoch 484/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9787 - val_loss: 0.0689 - val_accuracy: 0.9735\n",
      "Epoch 485/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9785 - val_loss: 0.0630 - val_accuracy: 0.9759\n",
      "Epoch 486/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9802 - val_loss: 0.0680 - val_accuracy: 0.9744\n",
      "Epoch 487/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9789 - val_loss: 0.0564 - val_accuracy: 0.9775\n",
      "Epoch 488/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9789 - val_loss: 0.0724 - val_accuracy: 0.9731\n",
      "Epoch 489/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9797 - val_loss: 0.0649 - val_accuracy: 0.9754\n",
      "Epoch 490/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9789 - val_loss: 0.0597 - val_accuracy: 0.9760\n",
      "Epoch 491/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9806 - val_loss: 0.0660 - val_accuracy: 0.9751\n",
      "Epoch 492/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 0.0696 - val_accuracy: 0.9744\n",
      "Epoch 493/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9796 - val_loss: 0.0597 - val_accuracy: 0.9764\n",
      "Epoch 494/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9798 - val_loss: 0.0598 - val_accuracy: 0.9766\n",
      "Epoch 495/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9799 - val_loss: 0.0616 - val_accuracy: 0.9760\n",
      "Epoch 496/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.0591 - val_accuracy: 0.9768\n",
      "Epoch 497/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0593 - accuracy: 0.9788 - val_loss: 0.0541 - val_accuracy: 0.9775\n",
      "Epoch 498/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9789 - val_loss: 0.0721 - val_accuracy: 0.9724\n",
      "Epoch 499/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 0.0566 - val_accuracy: 0.9773\n",
      "Epoch 500/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.0570 - val_accuracy: 0.9768\n",
      "Epoch 501/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9796 - val_loss: 0.0695 - val_accuracy: 0.9741\n",
      "Epoch 502/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.0650 - val_accuracy: 0.9751\n",
      "Epoch 503/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9789 - val_loss: 0.0638 - val_accuracy: 0.9756\n",
      "Epoch 504/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.0710 - val_accuracy: 0.9733\n",
      "Epoch 505/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9798 - val_loss: 0.0603 - val_accuracy: 0.9764\n",
      "Epoch 506/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9787 - val_loss: 0.0595 - val_accuracy: 0.9763\n",
      "Epoch 507/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9794 - val_loss: 0.0625 - val_accuracy: 0.9767\n",
      "Epoch 508/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9795 - val_loss: 0.0657 - val_accuracy: 0.9756\n",
      "Epoch 509/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9789 - val_loss: 0.0624 - val_accuracy: 0.9761\n",
      "Epoch 510/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9797 - val_loss: 0.0677 - val_accuracy: 0.9741\n",
      "Epoch 511/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9794 - val_loss: 0.0747 - val_accuracy: 0.9726\n",
      "Epoch 512/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.0601 - val_accuracy: 0.9767\n",
      "Epoch 513/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9799 - val_loss: 0.0599 - val_accuracy: 0.9763\n",
      "Epoch 514/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9795 - val_loss: 0.0624 - val_accuracy: 0.9754\n",
      "Epoch 515/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9793 - val_loss: 0.0622 - val_accuracy: 0.9760\n",
      "Epoch 516/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9798 - val_loss: 0.0590 - val_accuracy: 0.9775\n",
      "Epoch 517/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0702 - val_accuracy: 0.9741\n",
      "Epoch 518/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.0604 - val_accuracy: 0.9770\n",
      "Epoch 519/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0566 - val_accuracy: 0.9768\n",
      "Epoch 520/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9798 - val_loss: 0.0614 - val_accuracy: 0.9773\n",
      "Epoch 521/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9798 - val_loss: 0.0578 - val_accuracy: 0.9776\n",
      "Epoch 522/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9790 - val_loss: 0.0552 - val_accuracy: 0.9770\n",
      "Epoch 523/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9799 - val_loss: 0.0562 - val_accuracy: 0.9770\n",
      "Epoch 524/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9792 - val_loss: 0.0672 - val_accuracy: 0.9748\n",
      "Epoch 525/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9803 - val_loss: 0.0669 - val_accuracy: 0.9744\n",
      "Epoch 526/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 0.0535 - val_accuracy: 0.9774\n",
      "Epoch 527/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 0.0640 - val_accuracy: 0.9763\n",
      "Epoch 528/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9806 - val_loss: 0.0560 - val_accuracy: 0.9773\n",
      "Epoch 529/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9797 - val_loss: 0.0659 - val_accuracy: 0.9748\n",
      "Epoch 530/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9809 - val_loss: 0.0558 - val_accuracy: 0.9771\n",
      "Epoch 531/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9807 - val_loss: 0.0590 - val_accuracy: 0.9761\n",
      "Epoch 532/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 0.0690 - val_accuracy: 0.9744\n",
      "Epoch 533/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9795 - val_loss: 0.0601 - val_accuracy: 0.9764\n",
      "Epoch 534/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9805 - val_loss: 0.0646 - val_accuracy: 0.9759\n",
      "Epoch 535/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9800 - val_loss: 0.0634 - val_accuracy: 0.9749\n",
      "Epoch 536/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9793 - val_loss: 0.0585 - val_accuracy: 0.9767\n",
      "Epoch 537/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9792 - val_loss: 0.0633 - val_accuracy: 0.9763\n",
      "Epoch 538/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9802 - val_loss: 0.0619 - val_accuracy: 0.9769\n",
      "Epoch 539/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9798 - val_loss: 0.0555 - val_accuracy: 0.9781\n",
      "Epoch 540/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 0.0640 - val_accuracy: 0.9750\n",
      "Epoch 541/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9799 - val_loss: 0.0667 - val_accuracy: 0.9750\n",
      "Epoch 542/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9800 - val_loss: 0.0630 - val_accuracy: 0.9763\n",
      "Epoch 543/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9790 - val_loss: 0.0583 - val_accuracy: 0.9771\n",
      "Epoch 544/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0633 - val_accuracy: 0.9755\n",
      "Epoch 545/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.0672 - val_accuracy: 0.9745\n",
      "Epoch 546/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9802 - val_loss: 0.0613 - val_accuracy: 0.9777\n",
      "Epoch 547/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9798 - val_loss: 0.0761 - val_accuracy: 0.9735\n",
      "Epoch 548/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9803 - val_loss: 0.0619 - val_accuracy: 0.9759\n",
      "Epoch 549/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9800 - val_loss: 0.0588 - val_accuracy: 0.9773\n",
      "Epoch 550/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9789 - val_loss: 0.0811 - val_accuracy: 0.9718\n",
      "Epoch 551/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9798 - val_loss: 0.0615 - val_accuracy: 0.9768\n",
      "Epoch 552/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9793 - val_loss: 0.0618 - val_accuracy: 0.9764\n",
      "Epoch 553/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.0571 - val_accuracy: 0.9774\n",
      "Epoch 554/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9808 - val_loss: 0.0663 - val_accuracy: 0.9754\n",
      "Epoch 555/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.0678 - val_accuracy: 0.9754\n",
      "Epoch 556/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9798 - val_loss: 0.0623 - val_accuracy: 0.9756\n",
      "Epoch 557/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9793 - val_loss: 0.0598 - val_accuracy: 0.9769\n",
      "Epoch 558/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9809 - val_loss: 0.0573 - val_accuracy: 0.9775\n",
      "Epoch 559/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9808 - val_loss: 0.0687 - val_accuracy: 0.9750\n",
      "Epoch 560/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9808 - val_loss: 0.0571 - val_accuracy: 0.9769\n",
      "Epoch 561/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9807 - val_loss: 0.0664 - val_accuracy: 0.9754\n",
      "Epoch 562/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9800 - val_loss: 0.0526 - val_accuracy: 0.9778\n",
      "Epoch 563/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9809 - val_loss: 0.0831 - val_accuracy: 0.9719\n",
      "Epoch 564/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9803 - val_loss: 0.0612 - val_accuracy: 0.9764\n",
      "Epoch 565/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9802 - val_loss: 0.0589 - val_accuracy: 0.9771\n",
      "Epoch 566/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9805 - val_loss: 0.0682 - val_accuracy: 0.9759\n",
      "Epoch 567/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0569 - val_accuracy: 0.9765\n",
      "Epoch 568/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.0638 - val_accuracy: 0.9753\n",
      "Epoch 569/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9802 - val_loss: 0.0674 - val_accuracy: 0.9753\n",
      "Epoch 570/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9807 - val_loss: 0.0661 - val_accuracy: 0.9757\n",
      "Epoch 571/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.0699 - val_accuracy: 0.9754\n",
      "Epoch 572/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9808 - val_loss: 0.0628 - val_accuracy: 0.9761\n",
      "Epoch 573/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9808 - val_loss: 0.0563 - val_accuracy: 0.9768\n",
      "Epoch 574/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.0599 - val_accuracy: 0.9768\n",
      "Epoch 575/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9803 - val_loss: 0.0666 - val_accuracy: 0.9758\n",
      "Epoch 576/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.0631 - val_accuracy: 0.9763\n",
      "Epoch 577/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9801 - val_loss: 0.0632 - val_accuracy: 0.9765\n",
      "Epoch 578/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9804 - val_loss: 0.0634 - val_accuracy: 0.9755\n",
      "Epoch 579/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0753 - val_accuracy: 0.9729\n",
      "Epoch 580/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.0716 - val_accuracy: 0.9740\n",
      "Epoch 581/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9811 - val_loss: 0.0657 - val_accuracy: 0.9755\n",
      "Epoch 582/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.0739 - val_accuracy: 0.9736\n",
      "Epoch 583/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9805 - val_loss: 0.0613 - val_accuracy: 0.9769\n",
      "Epoch 584/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9805 - val_loss: 0.0646 - val_accuracy: 0.9761\n",
      "Epoch 585/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9806 - val_loss: 0.0804 - val_accuracy: 0.9720\n",
      "Epoch 586/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9800 - val_loss: 0.0605 - val_accuracy: 0.9769\n",
      "Epoch 587/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.0634 - val_accuracy: 0.9769\n",
      "Epoch 588/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9806 - val_loss: 0.0682 - val_accuracy: 0.9754\n",
      "Epoch 589/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0619 - val_accuracy: 0.9763\n",
      "Epoch 590/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9797 - val_loss: 0.0551 - val_accuracy: 0.9779\n",
      "Epoch 591/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9815 - val_loss: 0.0594 - val_accuracy: 0.9766\n",
      "Epoch 592/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9799 - val_loss: 0.0600 - val_accuracy: 0.9770\n",
      "Epoch 593/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9797 - val_loss: 0.0685 - val_accuracy: 0.9746\n",
      "Epoch 594/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9794 - val_loss: 0.0695 - val_accuracy: 0.9747\n",
      "Epoch 595/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9812 - val_loss: 0.0611 - val_accuracy: 0.9766\n",
      "Epoch 596/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9820 - val_loss: 0.0602 - val_accuracy: 0.9766\n",
      "Epoch 597/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 0.0684 - val_accuracy: 0.9753\n",
      "Epoch 598/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9805 - val_loss: 0.0644 - val_accuracy: 0.9753\n",
      "Epoch 599/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9813 - val_loss: 0.0588 - val_accuracy: 0.9771\n",
      "Epoch 600/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9815 - val_loss: 0.0626 - val_accuracy: 0.9759\n",
      "Epoch 601/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9807 - val_loss: 0.0604 - val_accuracy: 0.9771\n",
      "Epoch 602/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9808 - val_loss: 0.0592 - val_accuracy: 0.9768\n",
      "Epoch 603/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9817 - val_loss: 0.0653 - val_accuracy: 0.9755\n",
      "Epoch 604/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9803 - val_loss: 0.0687 - val_accuracy: 0.9760\n",
      "Epoch 605/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.0611 - val_accuracy: 0.9763\n",
      "Epoch 606/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9808 - val_loss: 0.0635 - val_accuracy: 0.9758\n",
      "Epoch 607/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9808 - val_loss: 0.0642 - val_accuracy: 0.9767\n",
      "Epoch 608/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9814 - val_loss: 0.0591 - val_accuracy: 0.9770\n",
      "Epoch 609/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9806 - val_loss: 0.0656 - val_accuracy: 0.9761\n",
      "Epoch 610/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9804 - val_loss: 0.0632 - val_accuracy: 0.9765\n",
      "Epoch 611/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.0669 - val_accuracy: 0.9757\n",
      "Epoch 612/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9811 - val_loss: 0.0656 - val_accuracy: 0.9759\n",
      "Epoch 613/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9802 - val_loss: 0.0665 - val_accuracy: 0.9755\n",
      "Epoch 614/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9806 - val_loss: 0.0659 - val_accuracy: 0.9758\n",
      "Epoch 615/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9810 - val_loss: 0.0606 - val_accuracy: 0.9761\n",
      "Epoch 616/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9815 - val_loss: 0.0611 - val_accuracy: 0.9774\n",
      "Epoch 617/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.0691 - val_accuracy: 0.9751\n",
      "Epoch 618/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 0.0623 - val_accuracy: 0.9763\n",
      "Epoch 619/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9824 - val_loss: 0.0593 - val_accuracy: 0.9769\n",
      "Epoch 620/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9817 - val_loss: 0.0554 - val_accuracy: 0.9771\n",
      "Epoch 621/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9807 - val_loss: 0.0589 - val_accuracy: 0.9782\n",
      "Epoch 622/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9805 - val_loss: 0.0623 - val_accuracy: 0.9766\n",
      "Epoch 623/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 0.0653 - val_accuracy: 0.9749\n",
      "Epoch 624/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9817 - val_loss: 0.0638 - val_accuracy: 0.9757\n",
      "Epoch 625/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0613 - val_accuracy: 0.9760\n",
      "Epoch 626/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9808 - val_loss: 0.0582 - val_accuracy: 0.9777\n",
      "Epoch 627/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9805 - val_loss: 0.0555 - val_accuracy: 0.9773\n",
      "Epoch 628/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9803 - val_loss: 0.0634 - val_accuracy: 0.9766\n",
      "Epoch 629/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9808 - val_loss: 0.0612 - val_accuracy: 0.9760\n",
      "Epoch 630/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9813 - val_loss: 0.0561 - val_accuracy: 0.9776\n",
      "Epoch 631/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9818 - val_loss: 0.0719 - val_accuracy: 0.9743\n",
      "Epoch 632/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9816 - val_loss: 0.0578 - val_accuracy: 0.9777\n",
      "Epoch 633/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9789 - val_loss: 0.0657 - val_accuracy: 0.9754\n",
      "Epoch 634/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9814 - val_loss: 0.0560 - val_accuracy: 0.9780\n",
      "Epoch 635/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9805 - val_loss: 0.0592 - val_accuracy: 0.9770\n",
      "Epoch 636/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9810 - val_loss: 0.0696 - val_accuracy: 0.9749\n",
      "Epoch 637/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9807 - val_loss: 0.0625 - val_accuracy: 0.9763\n",
      "Epoch 638/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9808 - val_loss: 0.0622 - val_accuracy: 0.9765\n",
      "Epoch 639/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9820 - val_loss: 0.0567 - val_accuracy: 0.9767\n",
      "Epoch 640/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9815 - val_loss: 0.0660 - val_accuracy: 0.9759\n",
      "Epoch 641/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9808 - val_loss: 0.0579 - val_accuracy: 0.9776\n",
      "Epoch 642/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9806 - val_loss: 0.0666 - val_accuracy: 0.9764\n",
      "Epoch 643/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9760\n",
      "Epoch 644/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 0.0700 - val_accuracy: 0.9755\n",
      "Epoch 645/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9810 - val_loss: 0.0583 - val_accuracy: 0.9769\n",
      "Epoch 646/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9815 - val_loss: 0.0614 - val_accuracy: 0.9766\n",
      "Epoch 647/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9810 - val_loss: 0.0643 - val_accuracy: 0.9760\n",
      "Epoch 648/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 0.0668 - val_accuracy: 0.9761\n",
      "Epoch 649/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9816 - val_loss: 0.0695 - val_accuracy: 0.9751\n",
      "Epoch 650/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9809 - val_loss: 0.0772 - val_accuracy: 0.9731\n",
      "Epoch 651/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 0.0624 - val_accuracy: 0.9765\n",
      "Epoch 652/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9813 - val_loss: 0.0584 - val_accuracy: 0.9768\n",
      "Epoch 653/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 0.0593 - val_accuracy: 0.9769\n",
      "Epoch 654/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.0753 - val_accuracy: 0.9735\n",
      "Epoch 655/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9799 - val_loss: 0.0746 - val_accuracy: 0.9745\n",
      "Epoch 656/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.0581 - val_accuracy: 0.9765\n",
      "Epoch 657/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9822 - val_loss: 0.0697 - val_accuracy: 0.9754\n",
      "Epoch 658/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9811 - val_loss: 0.0645 - val_accuracy: 0.9765\n",
      "Epoch 659/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9806 - val_loss: 0.0630 - val_accuracy: 0.9767\n",
      "Epoch 660/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.0604 - val_accuracy: 0.9756\n",
      "Epoch 661/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9813 - val_loss: 0.0661 - val_accuracy: 0.9757\n",
      "Epoch 662/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9813 - val_loss: 0.0603 - val_accuracy: 0.9767\n",
      "Epoch 663/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9819 - val_loss: 0.0729 - val_accuracy: 0.9735\n",
      "Epoch 664/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.0624 - val_accuracy: 0.9753\n",
      "Epoch 665/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9803 - val_loss: 0.0727 - val_accuracy: 0.9750\n",
      "Epoch 666/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9813 - val_loss: 0.0579 - val_accuracy: 0.9765\n",
      "Epoch 667/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9801 - val_loss: 0.0598 - val_accuracy: 0.9769\n",
      "Epoch 668/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9805 - val_loss: 0.0619 - val_accuracy: 0.9761\n",
      "Epoch 669/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.0647 - val_accuracy: 0.9758\n",
      "Epoch 670/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.0702 - val_accuracy: 0.9757\n",
      "Epoch 671/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.0660 - val_accuracy: 0.9765\n",
      "Epoch 672/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9805 - val_loss: 0.0691 - val_accuracy: 0.9746\n",
      "Epoch 673/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9809 - val_loss: 0.0622 - val_accuracy: 0.9759\n",
      "Epoch 674/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9814 - val_loss: 0.0622 - val_accuracy: 0.9769\n",
      "Epoch 675/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9810 - val_loss: 0.0597 - val_accuracy: 0.9759\n",
      "Epoch 676/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9802 - val_loss: 0.0609 - val_accuracy: 0.9765\n",
      "Epoch 677/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9822 - val_loss: 0.0621 - val_accuracy: 0.9766\n",
      "Epoch 678/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.0645 - val_accuracy: 0.9765\n",
      "Epoch 679/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9821 - val_loss: 0.0649 - val_accuracy: 0.9759\n",
      "Epoch 680/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9821 - val_loss: 0.0589 - val_accuracy: 0.9776\n",
      "Epoch 681/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9819 - val_loss: 0.0584 - val_accuracy: 0.9777\n",
      "Epoch 682/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9810 - val_loss: 0.0632 - val_accuracy: 0.9768\n",
      "Epoch 683/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9802 - val_loss: 0.0577 - val_accuracy: 0.9777\n",
      "Epoch 684/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0651 - val_accuracy: 0.9757\n",
      "Epoch 685/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 0.0643 - val_accuracy: 0.9761\n",
      "Epoch 686/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9815 - val_loss: 0.0733 - val_accuracy: 0.9754\n",
      "Epoch 687/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9803 - val_loss: 0.0707 - val_accuracy: 0.9755\n",
      "Epoch 688/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9823 - val_loss: 0.0573 - val_accuracy: 0.9777\n",
      "Epoch 689/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.0609 - val_accuracy: 0.9768\n",
      "Epoch 690/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.0701 - val_accuracy: 0.9754\n",
      "Epoch 691/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9814 - val_loss: 0.0689 - val_accuracy: 0.9756\n",
      "Epoch 692/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 0.0626 - val_accuracy: 0.9771\n",
      "Epoch 693/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9818 - val_loss: 0.0667 - val_accuracy: 0.9757\n",
      "Epoch 694/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9819 - val_loss: 0.0721 - val_accuracy: 0.9744\n",
      "Epoch 695/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9802 - val_loss: 0.0673 - val_accuracy: 0.9758\n",
      "Epoch 696/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9811 - val_loss: 0.0770 - val_accuracy: 0.9741\n",
      "Epoch 697/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9825 - val_loss: 0.0635 - val_accuracy: 0.9763\n",
      "Epoch 698/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.0641 - val_accuracy: 0.9757\n",
      "Epoch 699/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9826 - val_loss: 0.0643 - val_accuracy: 0.9758\n",
      "Epoch 700/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.0656 - val_accuracy: 0.9761\n",
      "Epoch 701/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 0.0629 - val_accuracy: 0.9766\n",
      "Epoch 702/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9813 - val_loss: 0.0716 - val_accuracy: 0.9758\n",
      "Epoch 703/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9820 - val_loss: 0.0650 - val_accuracy: 0.9768\n",
      "Epoch 704/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9815 - val_loss: 0.0644 - val_accuracy: 0.9764\n",
      "Epoch 705/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9811 - val_loss: 0.0642 - val_accuracy: 0.9769\n",
      "Epoch 706/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9802 - val_loss: 0.0598 - val_accuracy: 0.9771\n",
      "Epoch 707/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9814 - val_loss: 0.0580 - val_accuracy: 0.9774\n",
      "Epoch 708/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9823 - val_loss: 0.0571 - val_accuracy: 0.9778\n",
      "Epoch 709/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9811 - val_loss: 0.0684 - val_accuracy: 0.9761\n",
      "Epoch 710/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9822 - val_loss: 0.0606 - val_accuracy: 0.9776\n",
      "Epoch 711/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.0603 - val_accuracy: 0.9764\n",
      "Epoch 712/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0619 - val_accuracy: 0.9774\n",
      "Epoch 713/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9817 - val_loss: 0.0595 - val_accuracy: 0.9775\n",
      "Epoch 714/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.0594 - val_accuracy: 0.9773\n",
      "Epoch 715/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 0.0581 - val_accuracy: 0.9775\n",
      "Epoch 716/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9814 - val_loss: 0.0674 - val_accuracy: 0.9766\n",
      "Epoch 717/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9808 - val_loss: 0.0575 - val_accuracy: 0.9780\n",
      "Epoch 718/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9823 - val_loss: 0.0697 - val_accuracy: 0.9749\n",
      "Epoch 719/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.0712 - val_accuracy: 0.9754\n",
      "Epoch 720/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 0.0626 - val_accuracy: 0.9767\n",
      "Epoch 721/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.0722 - val_accuracy: 0.9747\n",
      "Epoch 722/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9821 - val_loss: 0.0738 - val_accuracy: 0.9739\n",
      "Epoch 723/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0741 - val_accuracy: 0.9741\n",
      "Epoch 724/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9808 - val_loss: 0.0617 - val_accuracy: 0.9774\n",
      "Epoch 725/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9811 - val_loss: 0.0557 - val_accuracy: 0.9782\n",
      "Epoch 726/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9814 - val_loss: 0.0733 - val_accuracy: 0.9749\n",
      "Epoch 727/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.0587 - val_accuracy: 0.9774\n",
      "Epoch 728/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9813 - val_loss: 0.0604 - val_accuracy: 0.9777\n",
      "Epoch 729/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9815 - val_loss: 0.0634 - val_accuracy: 0.9764\n",
      "Epoch 730/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 0.0670 - val_accuracy: 0.9758\n",
      "Epoch 731/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.0686 - val_accuracy: 0.9763\n",
      "Epoch 732/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9819 - val_loss: 0.0745 - val_accuracy: 0.9748\n",
      "Epoch 733/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.0681 - val_accuracy: 0.9760\n",
      "Epoch 734/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0636 - val_accuracy: 0.9763\n",
      "Epoch 735/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9820 - val_loss: 0.0669 - val_accuracy: 0.9761\n",
      "Epoch 736/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.0562 - val_accuracy: 0.9779\n",
      "Epoch 737/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9821 - val_loss: 0.0729 - val_accuracy: 0.9747\n",
      "Epoch 738/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9820 - val_loss: 0.0696 - val_accuracy: 0.9746\n",
      "Epoch 739/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.0603 - val_accuracy: 0.9774\n",
      "Epoch 740/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9810 - val_loss: 0.0599 - val_accuracy: 0.9773\n",
      "Epoch 741/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9819 - val_loss: 0.0658 - val_accuracy: 0.9760\n",
      "Epoch 742/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.0731 - val_accuracy: 0.9743\n",
      "Epoch 743/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9818 - val_loss: 0.0634 - val_accuracy: 0.9761\n",
      "Epoch 744/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.0795 - val_accuracy: 0.9728\n",
      "Epoch 745/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9816 - val_loss: 0.0649 - val_accuracy: 0.9759\n",
      "Epoch 746/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9821 - val_loss: 0.0707 - val_accuracy: 0.9754\n",
      "Epoch 747/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9812 - val_loss: 0.0649 - val_accuracy: 0.9763\n",
      "Epoch 748/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9817 - val_loss: 0.0733 - val_accuracy: 0.9747\n",
      "Epoch 749/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9818 - val_loss: 0.0634 - val_accuracy: 0.9770\n",
      "Epoch 750/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9829 - val_loss: 0.0646 - val_accuracy: 0.9766\n",
      "Epoch 751/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9823 - val_loss: 0.0667 - val_accuracy: 0.9753\n",
      "Epoch 752/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9818 - val_loss: 0.0610 - val_accuracy: 0.9770\n",
      "Epoch 753/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0595 - val_accuracy: 0.9765\n",
      "Epoch 754/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.0646 - val_accuracy: 0.9764\n",
      "Epoch 755/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0506 - accuracy: 0.9824 - val_loss: 0.0605 - val_accuracy: 0.9778\n",
      "Epoch 756/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.0637 - val_accuracy: 0.9769\n",
      "Epoch 757/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9820 - val_loss: 0.0633 - val_accuracy: 0.9764\n",
      "Epoch 758/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9817 - val_loss: 0.0612 - val_accuracy: 0.9769\n",
      "Epoch 759/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9820 - val_loss: 0.0646 - val_accuracy: 0.9761\n",
      "Epoch 760/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9807 - val_loss: 0.0773 - val_accuracy: 0.9739\n",
      "Epoch 761/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9822 - val_loss: 0.0589 - val_accuracy: 0.9781\n",
      "Epoch 762/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0597 - val_accuracy: 0.9779\n",
      "Epoch 763/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0664 - val_accuracy: 0.9759\n",
      "Epoch 764/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9815 - val_loss: 0.0604 - val_accuracy: 0.9775\n",
      "Epoch 765/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9824 - val_loss: 0.0651 - val_accuracy: 0.9761\n",
      "Epoch 766/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9820 - val_loss: 0.0589 - val_accuracy: 0.9771\n",
      "Epoch 767/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9824 - val_loss: 0.0677 - val_accuracy: 0.9758\n",
      "Epoch 768/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9808 - val_loss: 0.0655 - val_accuracy: 0.9757\n",
      "Epoch 769/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9818 - val_loss: 0.0578 - val_accuracy: 0.9780\n",
      "Epoch 770/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9815 - val_loss: 0.0825 - val_accuracy: 0.9731\n",
      "Epoch 771/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9817 - val_loss: 0.0667 - val_accuracy: 0.9755\n",
      "Epoch 772/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9811 - val_loss: 0.0634 - val_accuracy: 0.9764\n",
      "Epoch 773/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9823 - val_loss: 0.0678 - val_accuracy: 0.9758\n",
      "Epoch 774/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.0616 - val_accuracy: 0.9755\n",
      "Epoch 775/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0577 - val_accuracy: 0.9776\n",
      "Epoch 776/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9821 - val_loss: 0.0647 - val_accuracy: 0.9773\n",
      "Epoch 777/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9813 - val_loss: 0.0658 - val_accuracy: 0.9764\n",
      "Epoch 778/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9814 - val_loss: 0.0744 - val_accuracy: 0.9753\n",
      "Epoch 779/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9822 - val_loss: 0.0678 - val_accuracy: 0.9756\n",
      "Epoch 780/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9820 - val_loss: 0.0723 - val_accuracy: 0.9755\n",
      "Epoch 781/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.0553 - val_accuracy: 0.9775\n",
      "Epoch 782/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9807 - val_loss: 0.0603 - val_accuracy: 0.9768\n",
      "Epoch 783/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9819 - val_loss: 0.0669 - val_accuracy: 0.9768\n",
      "Epoch 784/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.0634 - val_accuracy: 0.9763\n",
      "Epoch 785/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.0638 - val_accuracy: 0.9765\n",
      "Epoch 786/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.0583 - val_accuracy: 0.9781\n",
      "Epoch 787/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9828 - val_loss: 0.0627 - val_accuracy: 0.9769\n",
      "Epoch 788/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.0602 - val_accuracy: 0.9777\n",
      "Epoch 789/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9826 - val_loss: 0.0665 - val_accuracy: 0.9761\n",
      "Epoch 790/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9820 - val_loss: 0.0697 - val_accuracy: 0.9756\n",
      "Epoch 791/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.0671 - val_accuracy: 0.9759\n",
      "Epoch 792/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9820 - val_loss: 0.0683 - val_accuracy: 0.9759\n",
      "Epoch 793/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.0561 - val_accuracy: 0.9775\n",
      "Epoch 794/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.0652 - val_accuracy: 0.9769\n",
      "Epoch 795/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9825 - val_loss: 0.0624 - val_accuracy: 0.9767\n",
      "Epoch 796/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9822 - val_loss: 0.0589 - val_accuracy: 0.9769\n",
      "Epoch 797/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9814 - val_loss: 0.0664 - val_accuracy: 0.9766\n",
      "Epoch 798/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9818 - val_loss: 0.0639 - val_accuracy: 0.9769\n",
      "Epoch 799/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9827 - val_loss: 0.0611 - val_accuracy: 0.9767\n",
      "Epoch 800/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0685 - val_accuracy: 0.9758\n",
      "Epoch 801/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9817 - val_loss: 0.0681 - val_accuracy: 0.9760\n",
      "Epoch 802/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9813 - val_loss: 0.0660 - val_accuracy: 0.9761\n",
      "Epoch 803/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9826 - val_loss: 0.0653 - val_accuracy: 0.9758\n",
      "Epoch 804/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 0.0637 - val_accuracy: 0.9768\n",
      "Epoch 805/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9829 - val_loss: 0.0623 - val_accuracy: 0.9766\n",
      "Epoch 806/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9812 - val_loss: 0.0647 - val_accuracy: 0.9759\n",
      "Epoch 807/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9826 - val_loss: 0.0643 - val_accuracy: 0.9759\n",
      "Epoch 808/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9824 - val_loss: 0.0560 - val_accuracy: 0.9782\n",
      "Epoch 809/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9799 - val_loss: 0.0654 - val_accuracy: 0.9766\n",
      "Epoch 810/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0647 - val_accuracy: 0.9767\n",
      "Epoch 811/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9812 - val_loss: 0.0577 - val_accuracy: 0.9780\n",
      "Epoch 812/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9813 - val_loss: 0.0668 - val_accuracy: 0.9754\n",
      "Epoch 813/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9825 - val_loss: 0.0559 - val_accuracy: 0.9781\n",
      "Epoch 814/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.0650 - val_accuracy: 0.9768\n",
      "Epoch 815/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9820 - val_loss: 0.0575 - val_accuracy: 0.9776\n",
      "Epoch 816/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9813 - val_loss: 0.0569 - val_accuracy: 0.9777\n",
      "Epoch 817/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 0.0617 - val_accuracy: 0.9769\n",
      "Epoch 818/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9822 - val_loss: 0.0673 - val_accuracy: 0.9759\n",
      "Epoch 819/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9824 - val_loss: 0.0705 - val_accuracy: 0.9757\n",
      "Epoch 820/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.0632 - val_accuracy: 0.9766\n",
      "Epoch 821/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 0.0654 - val_accuracy: 0.9768\n",
      "Epoch 822/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0717 - val_accuracy: 0.9756\n",
      "Epoch 823/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0582 - val_accuracy: 0.9769\n",
      "Epoch 824/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0613 - val_accuracy: 0.9765\n",
      "Epoch 825/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9819 - val_loss: 0.0565 - val_accuracy: 0.9784\n",
      "Epoch 826/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 0.0666 - val_accuracy: 0.9759\n",
      "Epoch 827/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9818 - val_loss: 0.0696 - val_accuracy: 0.9761\n",
      "Epoch 828/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9823 - val_loss: 0.0601 - val_accuracy: 0.9769\n",
      "Epoch 829/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9826 - val_loss: 0.0639 - val_accuracy: 0.9768\n",
      "Epoch 830/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9827 - val_loss: 0.0597 - val_accuracy: 0.9767\n",
      "Epoch 831/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9815 - val_loss: 0.0594 - val_accuracy: 0.9770\n",
      "Epoch 832/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.0628 - val_accuracy: 0.9768\n",
      "Epoch 833/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0684 - val_accuracy: 0.9764\n",
      "Epoch 834/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9825 - val_loss: 0.0676 - val_accuracy: 0.9755\n",
      "Epoch 835/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9825 - val_loss: 0.0664 - val_accuracy: 0.9757\n",
      "Epoch 836/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9827 - val_loss: 0.0681 - val_accuracy: 0.9753\n",
      "Epoch 837/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.0671 - val_accuracy: 0.9755\n",
      "Epoch 838/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9827 - val_loss: 0.0677 - val_accuracy: 0.9765\n",
      "Epoch 839/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9815 - val_loss: 0.0677 - val_accuracy: 0.9756\n",
      "Epoch 840/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9824 - val_loss: 0.0781 - val_accuracy: 0.9747\n",
      "Epoch 841/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0598 - val_accuracy: 0.9775\n",
      "Epoch 842/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9820 - val_loss: 0.0788 - val_accuracy: 0.9749\n",
      "Epoch 843/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9827 - val_loss: 0.0686 - val_accuracy: 0.9760\n",
      "Epoch 844/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9817 - val_loss: 0.0691 - val_accuracy: 0.9750\n",
      "Epoch 845/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.0609 - val_accuracy: 0.9765\n",
      "Epoch 846/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9813 - val_loss: 0.0627 - val_accuracy: 0.9765\n",
      "Epoch 847/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9829 - val_loss: 0.0645 - val_accuracy: 0.9765\n",
      "Epoch 848/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9827 - val_loss: 0.0582 - val_accuracy: 0.9779\n",
      "Epoch 849/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0667 - val_accuracy: 0.9756\n",
      "Epoch 850/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9822 - val_loss: 0.0606 - val_accuracy: 0.9774\n",
      "Epoch 851/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9828 - val_loss: 0.0575 - val_accuracy: 0.9771\n",
      "Epoch 852/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9821 - val_loss: 0.0630 - val_accuracy: 0.9770\n",
      "Epoch 853/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.0590 - val_accuracy: 0.9777\n",
      "Epoch 854/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9824 - val_loss: 0.0732 - val_accuracy: 0.9749\n",
      "Epoch 855/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9822 - val_loss: 0.0673 - val_accuracy: 0.9757\n",
      "Epoch 856/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0475 - accuracy: 0.9831 - val_loss: 0.0627 - val_accuracy: 0.9770\n",
      "Epoch 857/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0663 - val_accuracy: 0.9761\n",
      "Epoch 858/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9821 - val_loss: 0.0617 - val_accuracy: 0.9775\n",
      "Epoch 859/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9832 - val_loss: 0.0619 - val_accuracy: 0.9776\n",
      "Epoch 860/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9824 - val_loss: 0.0592 - val_accuracy: 0.9770\n",
      "Epoch 861/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.0643 - val_accuracy: 0.9765\n",
      "Epoch 862/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.0645 - val_accuracy: 0.9763\n",
      "Epoch 863/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9830 - val_loss: 0.0601 - val_accuracy: 0.9773\n",
      "Epoch 864/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9823 - val_loss: 0.0604 - val_accuracy: 0.9769\n",
      "Epoch 865/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9826 - val_loss: 0.0682 - val_accuracy: 0.9764\n",
      "Epoch 866/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9809 - val_loss: 0.0647 - val_accuracy: 0.9761\n",
      "Epoch 867/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9826 - val_loss: 0.0634 - val_accuracy: 0.9769\n",
      "Epoch 868/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9816 - val_loss: 0.0639 - val_accuracy: 0.9775\n",
      "Epoch 869/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9836 - val_loss: 0.0628 - val_accuracy: 0.9769\n",
      "Epoch 870/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9834 - val_loss: 0.0715 - val_accuracy: 0.9755\n",
      "Epoch 871/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9826 - val_loss: 0.0669 - val_accuracy: 0.9760\n",
      "Epoch 872/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9833 - val_loss: 0.0642 - val_accuracy: 0.9763\n",
      "Epoch 873/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.0697 - val_accuracy: 0.9760\n",
      "Epoch 874/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9817 - val_loss: 0.0812 - val_accuracy: 0.9744\n",
      "Epoch 875/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9813 - val_loss: 0.0831 - val_accuracy: 0.9739\n",
      "Epoch 876/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.0659 - val_accuracy: 0.9763\n",
      "Epoch 877/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9828 - val_loss: 0.0654 - val_accuracy: 0.9767\n",
      "Epoch 878/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9827 - val_loss: 0.0657 - val_accuracy: 0.9768\n",
      "Epoch 879/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9812 - val_loss: 0.0714 - val_accuracy: 0.9755\n",
      "Epoch 880/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9828 - val_loss: 0.0628 - val_accuracy: 0.9771\n",
      "Epoch 881/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0649 - val_accuracy: 0.9767\n",
      "Epoch 882/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.0710 - val_accuracy: 0.9757\n",
      "Epoch 883/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.0620 - val_accuracy: 0.9768\n",
      "Epoch 884/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0650 - val_accuracy: 0.9764\n",
      "Epoch 885/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9830 - val_loss: 0.0759 - val_accuracy: 0.9743\n",
      "Epoch 886/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 0.0601 - val_accuracy: 0.9777\n",
      "Epoch 887/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9825 - val_loss: 0.0726 - val_accuracy: 0.9751\n",
      "Epoch 888/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0771 - val_accuracy: 0.9749\n",
      "Epoch 889/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 0.0641 - val_accuracy: 0.9771\n",
      "Epoch 890/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9817 - val_loss: 0.0636 - val_accuracy: 0.9764\n",
      "Epoch 891/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.0650 - val_accuracy: 0.9760\n",
      "Epoch 892/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9826 - val_loss: 0.0686 - val_accuracy: 0.9763\n",
      "Epoch 893/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9821 - val_loss: 0.0612 - val_accuracy: 0.9774\n",
      "Epoch 894/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9824 - val_loss: 0.0681 - val_accuracy: 0.9767\n",
      "Epoch 895/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9824 - val_loss: 0.0756 - val_accuracy: 0.9746\n",
      "Epoch 896/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0652 - val_accuracy: 0.9761\n",
      "Epoch 897/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.0698 - val_accuracy: 0.9760\n",
      "Epoch 898/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9821 - val_loss: 0.0678 - val_accuracy: 0.9761\n",
      "Epoch 899/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9830 - val_loss: 0.0635 - val_accuracy: 0.9774\n",
      "Epoch 900/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0717 - val_accuracy: 0.9756\n",
      "Epoch 901/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0660 - val_accuracy: 0.9757\n",
      "Epoch 902/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9816 - val_loss: 0.0635 - val_accuracy: 0.9770\n",
      "Epoch 903/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9819 - val_loss: 0.0684 - val_accuracy: 0.9757\n",
      "Epoch 904/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9828 - val_loss: 0.0647 - val_accuracy: 0.9773\n",
      "Epoch 905/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0610 - val_accuracy: 0.9764\n",
      "Epoch 906/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9838 - val_loss: 0.0651 - val_accuracy: 0.9761\n",
      "Epoch 907/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9822 - val_loss: 0.0626 - val_accuracy: 0.9775\n",
      "Epoch 908/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9832 - val_loss: 0.0750 - val_accuracy: 0.9749\n",
      "Epoch 909/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9821 - val_loss: 0.0661 - val_accuracy: 0.9771\n",
      "Epoch 910/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.0611 - val_accuracy: 0.9776\n",
      "Epoch 911/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9833 - val_loss: 0.0676 - val_accuracy: 0.9760\n",
      "Epoch 912/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9831 - val_loss: 0.0618 - val_accuracy: 0.9774\n",
      "Epoch 913/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 0.0719 - val_accuracy: 0.9759\n",
      "Epoch 914/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 915/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9821 - val_loss: 0.0674 - val_accuracy: 0.9758\n",
      "Epoch 916/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9824 - val_loss: 0.0668 - val_accuracy: 0.9756\n",
      "Epoch 917/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9825 - val_loss: 0.0623 - val_accuracy: 0.9771\n",
      "Epoch 918/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9825 - val_loss: 0.0702 - val_accuracy: 0.9760\n",
      "Epoch 919/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9828 - val_loss: 0.0657 - val_accuracy: 0.9766\n",
      "Epoch 920/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9830 - val_loss: 0.0640 - val_accuracy: 0.9776\n",
      "Epoch 921/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.0650 - val_accuracy: 0.9765\n",
      "Epoch 922/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9829 - val_loss: 0.0656 - val_accuracy: 0.9766\n",
      "Epoch 923/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9825 - val_loss: 0.0593 - val_accuracy: 0.9780\n",
      "Epoch 924/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.0713 - val_accuracy: 0.9759\n",
      "Epoch 925/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9825 - val_loss: 0.0591 - val_accuracy: 0.9786\n",
      "Epoch 926/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9825 - val_loss: 0.0639 - val_accuracy: 0.9767\n",
      "Epoch 927/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0680 - val_accuracy: 0.9760\n",
      "Epoch 928/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9823 - val_loss: 0.0641 - val_accuracy: 0.9766\n",
      "Epoch 929/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.0673 - val_accuracy: 0.9758\n",
      "Epoch 930/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.0577 - val_accuracy: 0.9777\n",
      "Epoch 931/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0729 - val_accuracy: 0.9756\n",
      "Epoch 932/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9822 - val_loss: 0.0630 - val_accuracy: 0.9771\n",
      "Epoch 933/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9831 - val_loss: 0.0726 - val_accuracy: 0.9765\n",
      "Epoch 934/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9830 - val_loss: 0.0592 - val_accuracy: 0.9780\n",
      "Epoch 935/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9822 - val_loss: 0.0695 - val_accuracy: 0.9767\n",
      "Epoch 936/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9824 - val_loss: 0.0653 - val_accuracy: 0.9770\n",
      "Epoch 937/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9828 - val_loss: 0.0667 - val_accuracy: 0.9765\n",
      "Epoch 938/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9823 - val_loss: 0.0704 - val_accuracy: 0.9753\n",
      "Epoch 939/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9829 - val_loss: 0.0802 - val_accuracy: 0.9747\n",
      "Epoch 940/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9828 - val_loss: 0.0760 - val_accuracy: 0.9747\n",
      "Epoch 941/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9831 - val_loss: 0.0688 - val_accuracy: 0.9760\n",
      "Epoch 942/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9829 - val_loss: 0.0657 - val_accuracy: 0.9768\n",
      "Epoch 943/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9829 - val_loss: 0.0599 - val_accuracy: 0.9777\n",
      "Epoch 944/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9828 - val_loss: 0.0642 - val_accuracy: 0.9773\n",
      "Epoch 945/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0480 - accuracy: 0.9827 - val_loss: 0.0682 - val_accuracy: 0.9770\n",
      "Epoch 946/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9831 - val_loss: 0.0800 - val_accuracy: 0.9740\n",
      "Epoch 947/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9826 - val_loss: 0.0655 - val_accuracy: 0.9763\n",
      "Epoch 948/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9829 - val_loss: 0.0602 - val_accuracy: 0.9779\n",
      "Epoch 949/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.0724 - val_accuracy: 0.9759\n",
      "Epoch 950/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9821 - val_loss: 0.0646 - val_accuracy: 0.9769\n",
      "Epoch 951/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9834 - val_loss: 0.0680 - val_accuracy: 0.9761\n",
      "Epoch 952/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 0.0683 - val_accuracy: 0.9760\n",
      "Epoch 953/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.0626 - val_accuracy: 0.9769\n",
      "Epoch 954/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9829 - val_loss: 0.0652 - val_accuracy: 0.9761\n",
      "Epoch 955/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9829 - val_loss: 0.0646 - val_accuracy: 0.9769\n",
      "Epoch 956/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.9836 - val_loss: 0.0646 - val_accuracy: 0.9771\n",
      "Epoch 957/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0692 - val_accuracy: 0.9756\n",
      "Epoch 958/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 0.0770 - val_accuracy: 0.9744\n",
      "Epoch 959/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9819 - val_loss: 0.0646 - val_accuracy: 0.9763\n",
      "Epoch 960/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9832 - val_loss: 0.0692 - val_accuracy: 0.9764\n",
      "Epoch 961/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9827 - val_loss: 0.0687 - val_accuracy: 0.9758\n",
      "Epoch 962/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9826 - val_loss: 0.0642 - val_accuracy: 0.9766\n",
      "Epoch 963/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9832 - val_loss: 0.0631 - val_accuracy: 0.9764\n",
      "Epoch 964/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9834 - val_loss: 0.0705 - val_accuracy: 0.9759\n",
      "Epoch 965/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9824 - val_loss: 0.0669 - val_accuracy: 0.9757\n",
      "Epoch 966/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9829 - val_loss: 0.0663 - val_accuracy: 0.9766\n",
      "Epoch 967/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9830 - val_loss: 0.0695 - val_accuracy: 0.9760\n",
      "Epoch 968/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9823 - val_loss: 0.0612 - val_accuracy: 0.9778\n",
      "Epoch 969/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9825 - val_loss: 0.0645 - val_accuracy: 0.9768\n",
      "Epoch 970/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9826 - val_loss: 0.0651 - val_accuracy: 0.9764\n",
      "Epoch 971/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9831 - val_loss: 0.0683 - val_accuracy: 0.9763\n",
      "Epoch 972/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9828 - val_loss: 0.0619 - val_accuracy: 0.9775\n",
      "Epoch 973/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9831 - val_loss: 0.0674 - val_accuracy: 0.9765\n",
      "Epoch 974/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9833 - val_loss: 0.0628 - val_accuracy: 0.9765\n",
      "Epoch 975/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9827 - val_loss: 0.0655 - val_accuracy: 0.9765\n",
      "Epoch 976/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9820 - val_loss: 0.0709 - val_accuracy: 0.9750\n",
      "Epoch 977/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9837 - val_loss: 0.0632 - val_accuracy: 0.9774\n",
      "Epoch 978/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9826 - val_loss: 0.0622 - val_accuracy: 0.9773\n",
      "Epoch 979/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9830 - val_loss: 0.0690 - val_accuracy: 0.9763\n",
      "Epoch 980/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9827 - val_loss: 0.0679 - val_accuracy: 0.9756\n",
      "Epoch 981/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9828 - val_loss: 0.0697 - val_accuracy: 0.9763\n",
      "Epoch 982/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9825 - val_loss: 0.0705 - val_accuracy: 0.9758\n",
      "Epoch 983/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9836 - val_loss: 0.0689 - val_accuracy: 0.9764\n",
      "Epoch 984/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9832 - val_loss: 0.0750 - val_accuracy: 0.9751\n",
      "Epoch 985/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9827 - val_loss: 0.0663 - val_accuracy: 0.9764\n",
      "Epoch 986/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9833 - val_loss: 0.0653 - val_accuracy: 0.9763\n",
      "Epoch 987/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.9834 - val_loss: 0.0672 - val_accuracy: 0.9763\n",
      "Epoch 988/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9833 - val_loss: 0.0629 - val_accuracy: 0.9773\n",
      "Epoch 989/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9836 - val_loss: 0.0616 - val_accuracy: 0.9777\n",
      "Epoch 990/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9824 - val_loss: 0.0605 - val_accuracy: 0.9777\n",
      "Epoch 991/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.0687 - val_accuracy: 0.9765\n",
      "Epoch 992/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9834 - val_loss: 0.0668 - val_accuracy: 0.9760\n",
      "Epoch 993/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9835 - val_loss: 0.0744 - val_accuracy: 0.9749\n",
      "Epoch 994/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9827 - val_loss: 0.0630 - val_accuracy: 0.9766\n",
      "Epoch 995/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9818 - val_loss: 0.0612 - val_accuracy: 0.9771\n",
      "Epoch 996/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9832 - val_loss: 0.0661 - val_accuracy: 0.9765\n",
      "Epoch 997/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9824 - val_loss: 0.0721 - val_accuracy: 0.9754\n",
      "Epoch 998/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9825 - val_loss: 0.0699 - val_accuracy: 0.9757\n",
      "Epoch 999/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9831 - val_loss: 0.0688 - val_accuracy: 0.9755\n",
      "Epoch 1000/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9835 - val_loss: 0.0602 - val_accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAEeCAYAAAANcYvwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVfrA8e+bSugt1NBEWkCKRLAgKCpiWTtKsXd3XVdZXfW3rrqou/a6umIXXUXEsqi4iIJgQ+m9hU4gEGpCSZ3z++PeITczd1oyQ9r7eZ555pZz75wZwrxzuhhjUEoppWqCuMrOgFJKKRUtGtSUUkrVGBrUlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1RgJlZ0BpZSqyebPn98iISHhDaAXWpCoKA+wrLi4+Mb+/fvvdEugQU0ppWIoISHhjVatWvVITU3dGxcXpwODK8Dj8UhOTk56dnb2G8AFbmn0V4NSSsVWr9TU1FwNaBUXFxdnUlNT92OVet3THMX8KKVUbRSnAS167M8yYOzS6kellKqhsrOz40877bRuALt27UqMi4szTZs2LQZYtGjRyjp16gQMtrNnz6771ltvNXvnnXe2HK38RoMGNaWUqqFatWpVsmrVqhUAY8eObVO/fv2ScePG7fCeLyoqIjEx0fXawYMHHxo8ePCho5TVqNHqR6WUqkUuvfTSjqNHj27fu3fv7rfddlvazJkz6/bt27d7jx490vv169d98eLFyQBffvllg9NPP/1YsALiiBEjOg4YMKBbWlracY8++miLyn0XgWlJTdVqItIR2AAkGmOKQ6S9FrjRGDOoIvepLCJyGvC+MSYtwPl3gK3GmAeOZr7U0bd9+/akBQsWrEpISGDPnj1xc+fOXZWYmMjnn3/e4C9/+UvatGnT1vlek5mZWefnn39evW/fvvgePXr0uueee3KSk5OrXFuhBjVVbYjIRqAN0MYYs8txfCHQF+hkjNlYObk7uuzPoiVQ4jjc1RizrXJyBCLyIPB34CxjzLeVlY+qrON9X/WPxX03Pn7e/EjSX3LJJXsTEqyv/z179sRfccUVnTZu3FhHRExRUZG4XTNs2LB9KSkpJiUlpbhp06ZFW7duTejcuXNRFLIfVVr9qKqbDcAo746IHAfUrbzsVKrfGWPqOx6VGdA6AyOA7ZWVBxW++vXre7zb9957b9shQ4bkrV27dvkXX3yRWVhY6BoXnKWy+Ph4iouLXYNfZdOSmqpu3gOuBl6y968BJgCPehOISCP7/DnAIeB14B/GGI+IxANPANcCucAzzpvb1z4LnIs1e8HbwEPGGGeJKCQRaQO8CgwC9gBPGGNet88NAF4BugKHgf8YY8aKSB3gDTvf8cBa4HxjzA6Xlwj0usn2+7vcPjQJuNcYU+CSth/wJtAFmApUpCrpZeBerPelAoi0RHU05ObmxqelpRUCjB8/vnll56eitKSmqps5QEMR6WEHqJHA+z5pXgIaAccAQ7CC4HX2uZuA84F+QAZwmc+17wDFwLF2mmHAjeXI50RgK1Z16WXAP0RkqH3uBeAFY0xDoDNW4AErQDcC2gHNgFuxgl4k/gqciFUd2wcYAPi1kYlIEvA51o+EpsDHwKWO8+1FZF+Qx2hH2hFAgTFmaoR5VVXAvffem/3www+n9ejRI724uEo2B0dEjKly7XxKubLbkW7E+tKuB8wC/oxVsikCOgFbsAJBX2PMCvu6W4BRxpjTRGQGMMkY86p9bhgwDUjECiSbgcbGmMP2+VHAzcaY08PtKAK0Bjba98mzz/8TaG2MuVZEZgMzgZd82gavt9/frcaYJWF8Fs2xAjDA98aYi0RkHfBHb4ARkbOB8caYjs6OIiIyGCvwtjX2l4CI/AzMiKSjiIg0ABZgtaNt9P4baZtaqcWLF2/s06fPrtApVbgWL17cvE+fPh3dzmn1o6qO3gNmYwWxCT7nmmMFlk2OY5uAtvZ2G6zA5zzn1cG+drvIkeaCOJ/04WgD7PEGNMfrZNjbNwDjgFUisgH4uzHmS/t9tQMmikhjrBLoX40xgRrjL3IJHm3wf+9tAuQxy5T9VbvJJV0oDwPv1ZYOOqrq0+pHVe0YYzZhlYrOBT71Ob0Lq9TWwXGsPZBlb2/HChzOc15bgAKguTGmsf1oaIzpGWEWtwFN7VKMXx6MMWuNMaOAFljtX5NFpJ4xpsgY83djTDpwMlY16dXleG3f9+7WgWQ70FYc0RvHZ2FXPx4I8hhjJz0DuENEskUkG+uznSQi90aYb6WiQoOaqq5uAIYaYw46D9odOiYBj4lIAxHpAIyltN1tEtaXcJqINAHuc1y7HfgGeEZEGopInIh0FpEhkWTMGLMF+Bn4p4jUEZHedn7fBxCRK0Uk1RjjAfbZl3lE5HQROc5uK8zFCs4el5cI5kPgARFJFZHmwIP4tzkC/IJVdXmHiCSKyCVY7W/e97DZp2el7+M/dtIzsCaX7Ws/tgG3YHUcUeqo06CmqiVjzDpjzLwAp/8IHATWAz8CHwBv2edex2pDW4zVFuRb0rsaSAJWAHuByVhtZJEaBXTE+pL/DKsHpbeqcDiwXEQOYHUaGWm34bWyXy8XWInVZvhehK/7KDAPWAIsxXqPj/omMsYUApdg9QLdA1yB/2cRkjFmtzEm2/vAGje31xhzINJ7KRUN2lFEKaViSDuKRF+wjiJaUlNKKVVjxDSoichwEVktIpkicp/L+bEiskJElojId3b7h/fcNSKy1n5c4zjeX0SW2vd80aehWymllMPAgQO7fvLJJw2dx8aNG9dizJgx7d3SDxgwoNvs2bPrAgwZMuTYXbt2xfumGTt2bJsHH3ywZbDXfe+99xrPnz+/jnf/zjvvbPP55583CHZNNMQsqNmN3S9jjSFKB0aJSLpPsoVAhjGmN1ZbwpP2tU2Bh4CBWI3XD9mN+gD/xhpA28V+DI/Ve1BKqepuxIgRez788MOmzmOffPJJ0yuvvHJPqGtnzZqV2bx584hm0/H6/PPPGy9ZsiTFu//8889vu+iii/KCXRMNsSypDQAyjTHr7UbpicCFzgTGmJnGGO96PXMA7+zhZwPTjTF7jDF7genAcBFpDTQ0xsyxx9dMAC6K4XtQSqlq7aqrrto7Y8aMRvn5+QKwevXqpJ07dya+//77TXv16tXj2GOP7XnXXXe5jWWkbdu2x23fvj0B4N57723VsWPHXv379++2du3aZG+aZ555pnmvXr16dOvWLf3ss8/unJeXFzd9+vR63377beMHHnggrXv37unLly9PvvTSSzu+/fbbTQD++9//NujRo0d6165d00eMGNHx8OHD4n29u+66q016enqPrl27pi9cuLCOW76CiWVQa0vZQatbKR0A6+YG4OsQ17a1t8O9p1JK1WotW7Ys6dOnz8HJkyc3Anj33Xeb/u53v9v77LPPZi1btmzlqlWrlv/0008Nfv3115RA9/jhhx/qfvbZZ02XLl26Yvr06WsXL15cz3tuzJgxe5ctW7Zy9erVK7p163b4xRdfbH7WWWcdPPPMM/c9+uijW1etWrWiZ8+eR+YePXTokNxyyy2dPvroo3Vr1qxZUVxczFNPPZXqPd+8efPiFStWrLz++utzHn/88aBVnG6qxIwiInIl1mwLEY0HCnHPm4GbAerVq9e/e/fuFb9p4QHYtTbw+Za9IN59Fdk1O/IoKPbQpUV96iT6VVErpWqoJ598khUrVnQASJ90UkxeY8XlvwQ9f8455/DBBx80Ov744/n000955JFHeOWVV1p+/PHHlJSUkJOTww8//JDeoIHV5JWdnd1jxYoVeHvHz5w5s/655567r0GDBh6wlqHx3nv+/PkpDz74YNu8vLz4gwcPxg8ZMmR/sLwsXry4TlpaWkHv3r0LAK699trdL7/8cgtgJ8Do0aP3AgwYMODQlClTmgS5latYBrUsys7ckEbprA5HiMiZWJOwDnHMJJ4FnOZz7ff28TSf4373BDDGvAa8BpCRkWHmzQs0pCkCm+fAW2cHPn/nl9C4neup69+Zy4xVO3n6yuMZ3qs8w56UUtXRypUr6dGjR0xfIz3dt7tCWe3bt+fpp58mPz8fj8dDRkYG999/P3PnzqVJkyZce+21NG/enPT0dOrWrcsxxxzjvWfIMV8333xzp8mTJ2eedNJJh1988cVms2bNqlBnkDp16hiAhIQEU57lbWIZ1OYCXUSkE1bgGQmMdiawl74YDww3xux0nJqGNau5N0oPA+43xuwRkVwRORH4lbJLkMReXIiPywSe/KF9U2vJr027DwVMo5Sq4R4OWoiJmfr163P66adz/fXXM2rUKHJzc6lXrx6NGjVix44dfP3115x22mkBrx86dOiB66+/vuOjjz66vaioSKZPn974mmuuyQE4dOhQXPv27YsKCgpk4sSJTVu3bl1kv2ZJbm6uXxNXnz598rOyspKWLVuW3KtXr4IJEyY0O/XUU6PWgSRmQc0YUywit2MFqHjgLWPMchEZB8wzxkwBngLqAx/bPfM3G2MusIPXI1iBEWCcMcbbU+f3WMuDpGC1wX3N0RIXotowSFDr2MwOans0qCmljr5Ro0Zx8cUXM3HiRLp3706/fv3o3r077dq145RTTgl67aBBgw5dfPHFe3r16tWzWbNmRb179z4yPd199923bcCAAT2aNm1afPzxxx84cOBAPMCYMWP23HbbbR1fffXVlpMnT17nTV+3bl3z6quvbhwxYkTnkpIS+vTpc+juu+/Oidb7rBUzikSt+nH7Ehh/auDzf1wAzTq7npq5aifXvTOXU45txn9uPLHieVFKVQtHo/oxVpYtW3aoV69eKys7H750RpFoCVn9GPgHQvtmWv2olFKxpkEtEhVoU0trkkKcwLZ9hyksjnTidaWUUuHQoBaJCrSpJSfE07pRCh4DW/dqaU0ppWJBg1o0BQlqAB20s4hStVJt6LtwtHg8HiHIOoMa1CLhCTEFWsigZg3C36ztakrVGnXq1GH37t0a2KLA4/FITk5OI2BZoDRVYkaRaiNE0GLxh9BiXMBqyg7aWUSpWictLY2tW7eSkxO1XutHTXZ2dkJJSUnzys6HgwdYVlxcfGOgBBrUItEgxDRkv/wLmnSEATe5nu5wZAD2QdfzSqmaJzExkU6dOlV2NsolPT19qTEmo7LzEQmtfoxEShjTkG35LeApb/WjtqkppVRsaFCLtmBTZdnVj5v3HMLj0fp1pZSKNg1q0WYCdyapn5xA8/pJFBZ7yM7NP4qZUkqp2kGDWsRCTBodojOJTmyslFKxo0EtUhLiIwsR1Do1rw/A2p0xX9VcKaVqHQ1qkZIQJTVP8KB2fIfGAMzftDdaOVJKKWXToBaxilU/dmlhrZ+n1Y9KKRV9GtQiFaqkFqSjCEDbJikAbN17OFo5UkopZdOgFrFQ1Y/Bg1qrhnVoUCeBXQcKWJWdG8V8KaWU0qAWqZAlteDVj/Fxwlnp1swkv67fEzStUkqpyGhQi1jFghrAcW0bAbAqW3tAKqVUNGlQi1QFS2pgVUEC7D5QEI0cKaWUssU0qInIcBFZLSKZInKfy/nBIrJARIpF5DLH8dNFZJHjkS8iF9nn3hGRDY5zfWP5HvxVPKg1q58MwO6DhdHIkFJKKVvMgpqIxAMvA+cA6cAoEUn3SbYZuBb4wHnQGDPTGNPXGNMXGAocAr5xJLnHe94YsyhW78HV6InBz4cV1JIA2KUlNaWUiqpYltQGAJnGmPXGmEJgInChM4ExZqMxZglBVjEFLgO+NsZUjYFdnQYHP793I4RYDLBNoxTixOrWn18UYuFRpZRSYYtlUGsLbHHsb7WPRWok8KHPscdEZImIPCciyeXNYLmd4L5eGgB52+GxVvDBSMjb4ZokJSmeY1LrU+IxrNbOIkopFTVVuqOIiLQGjgOmOQ7fD3QHTgCaAvcGuPZmEZknIvOivuLsOU/A7fOhcXv388X5sOZr+OaBgLfo1aYhAMu36Vg1pZSKllgGtSygnWM/zT4WicuBz4wxRd4DxpjtxlIAvI1VzenHGPOaMSbDGJORmpoa4cuGEBcPzY8NPbnx4cDj0Hq2sbr1L9+2P5o5U0qpWi2WQW0u0EVEOolIElY14pQI7zEKn6pHu/SGiAhwEbAsCnktpxA9IYMEvZ52SW2ZltSUUipqYhbUjDHFwO1YVYcrgUnGmOUiMk5ELgAQkRNEZCswAhgvIsu914tIR6yS3iyfW/9HRJYCS4HmwKOxeg8hnfSH4OeDBLV0O6it2p5LcUnoHpNKKaVCS4jlzY0xU4GpPscedGzPxaqWdLt2Iy4dS4wxQ6Obywo44UaYenfg80GCWuO6SbRtnELWvsOs33WQri0bxCCDSilVu1TpjiJVngg0dI3J9vngH2+vtt7OItquppRS0aBBraKu/jzwuRBTah3pLJKl7WpKKRUNGtQqqnmXICdDBTXt1q+UUtGkQS2WQlQ/Orv1mxCzkCillApNg1osxcUHPd2yYTLN6yeRm1+sK2ErpVQUaFCLhnOfdj8eoqQmIqTrIGyllIoaDWrR0OHkACdCDM7GMQhbO4sopVSFaVCLhrgAw/1CTaMF9G/fBIAvl2yLZo6UUqpW0qAWDRKg7cwb1HK3w/uXwXrfyVHg9O4taFgngY27D7Ftn7arKaVURWhQi4ZAHUK8Qe3reyBzOky4wC9JfJzQ1y6tLdi8N1Y5VEqpWkGDWjQEDGp2m9qBnUEv79euMQBLt2pnEaWUqggNatEQqk3NUxz08mNS6wGwZW/VWNxbKaWqKw1q0RCwTc0uqYUIam0bpwCwZscBHYStlFIVoEEtGgKV1OISoSAPPCVBL+/aqgEpifFk7jzAmh0HYpBBpZSqHTSoRUNcgI9x6ST4ZxrsCL6OacM6iZzRowUAC7WziFJKlZsGtWgIVFLLD7/jR580q7PIYu0sopRS5aZBLRoCtalFoI/dA3LJ1n0VvpdSStVWGtSiIVBJLQK92jYkTmB1dh75RcHb4JRSSrmLaVATkeEislpEMkXkPpfzg0VkgYgUi8hlPudKRGSR/ZjiON5JRH617/mRiCTF8j2ExXecWrDVsAOom5RAlxYNKPYYlmVpFaRSSpVHzIKaiMQDLwPnAOnAKBFJ90m2GbgW+MDlFoeNMX3th3MqjieA54wxxwJ7gRuinvlIOed4bNwe+l9brttkdLRmFvlkwdYoZEoppWqfWJbUBgCZxpj1xphCYCJwoTOBMWajMWYJ4AnnhiIiwFBgsn3oXeCi6GW5nESgzyjodSncuRTqNinXbS7s2xaAD3/bEs3cKaVUrRHLoNYWcH47b7WPhauOiMwTkTki4g1czYB9xhjvaOZI7xk7F78Kl71lbYcxO7+bbi0bHNnWeSCVUipyVbmjSAdjTAYwGnheRDpHcrGI3GwHxXk5OTmxyWHAFw/ysRYXBDzVqG7ikUlIFm7WXpBKKRWpWAa1LKCdYz/NPhYWY0yW/bwe+B7oB+wGGouIt7thwHsaY14zxmQYYzJSU1Mjz31FBOviv/C9oJc+dL7V7Ji5U2cWUUqpSMUyqM0Futi9FZOAkcCUENcAICJNRCTZ3m4OnAKsMNbEiDMBb0/Ja4D/Rj3nFRWspJYffIXrY1tYVZCZO/OimSOllKoVYhbU7Hav24FpwEpgkjFmuYiME5ELAETkBBHZCowAxovIcvvyHsA8EVmMFcQeN8assM/dC4wVkUysNrY3Y/Ueyi1YUCvIgy/+BJt/dT3dtWV9AOZu3MvM1cGXrFFKKVVWxUcNB2GMmQpM9Tn2oGN7LlYVou91PwPHBbjneqyelVVXsKD247PW8/x34GH/8WgtGtY5sv3ZgixO79YiyplTSqmaqyp3FKm+Ai0aGqYHzusBwKHC4EvWKKWUKkuDWix4uzCW08BOzQBYv+tgNHKjlFK1hga1WIiv2MxdXVvVp05iHOtzDjJ7zVEejqCUUtWYBrVYSKpXocuTE+IZM7ADAP/+fl00cqSUUrWCBrVYSKpf4VtcfZIV1Dbu1ipIpZQKlwa1WEisW+FbtGmcQkKcsH1/PnsPFkYhU0opVfNpUIuFpIoHtcT4OE7qbHUY+XLp9grfTymlagMNarEQhZIawCXHW3M1f6ZL0SilVFg0qMVClILa2T1bAbBg8z6+WbYdPLoitlJKBaNBLRbqNIQuZ1f4NnWTEqifbE360vGLy+DprlBSVOH7KqVUTaVBLVYG3RWV2zw9og8AXQuWwaFdsHdjVO6rlFI1kQa1Ku707qkkxFVshhKllKotNKjFSsM2odPk7QiZJDkhnvQ2DUsPGFOBTCmlVM2mQS1WmnSA0ZOCp5nzcli3OqtHyyPbBg1qSikViAa1WOoaorNIXHgr/1zYt+2R7V837q1IjpRSqkbToFaZEuqETgO0b1Y6RODZb1bFKjdKKVXtaVCrTAnJEV+y+0Ah+UU6Xk0ppdxoUKtMYZbUnAzCWz9tiEFmlFKq+otpUBOR4SKyWkQyReQ+l/ODRWSBiBSLyGWO431F5BcRWS4iS0TkCse5d0Rkg4gssh99Y/keYqocJTXBMOHnTRjtBamUUn5iFtREJB54GTgHSAdGiUi6T7LNwLXABz7HDwFXG2N6AsOB50WkseP8PcaYvvZjUUzeQLQ1agcN08oei488qDVKSSI7N5/New5FKWNKKVVzxLKkNgDINMasN8YUAhOBC50JjDEbjTFLAI/P8TXGmLX29jZgJ5Aaw7zGXlJ9uHNp2WMHsuG5XvDzS/7pA5TEetpj1n7bsCfaOVRKqWovlkGtLbDFsb/VPhYRERkAJAHOJaAfs6slnxORyIs7lSXO5+P+4TnYvwW+eaDs8cxv4Z9psPp/frfo084qsE7V5WiUUspPle4oIiKtgfeA64wx3tLc/UB34ASgKXBvgGtvFpF5IjIvJyfnqOQ3KHGZ6qpgv3vaD66AwgPw4RV+p07vlkpKYjwzV+ewcntulDOplFLVWyyDWhbQzrGfZh8Li4g0BL4C/mqMmeM9bozZbiwFwNtY1Zx+jDGvGWMyjDEZqanVu+bSqVm9pCPrrP1j6spKzo1SSlUtsQxqc4EuItJJRJKAkcCUcC60038GTDDGTPY519p+FuAiYFlUc10VhOjZeGl/q8PJD2t3sXWvdhhRSimvmAU1Y0wxcDswDVgJTDLGLBeRcSJyAYCInCAiW4ERwHgRWW5ffjkwGLjWpev+f0RkKbAUaA48Gqv3UGX4BLnj2zfhhI5NAPjg182VkSOllKqSwpt8sJyMMVOBqT7HHnRsz8WqlvS97n3g/QD3HBrlbB4lIZaPyd9vrZgdn+h/zhnU7O2/DO/OiFd/YdK8rYw9qysJ8VW6eVQppY4K/SasKh5vD/8+xd7xrX50BjWrv0xGhyZ0Tq3HrgMFTFm87ahkUSmlqjoNakdbUoPA53at9j+W+e2RQGaxApyIcNOpxwAw8bct/tcppVQtpEEt1rqdZz33HW093/oDSAQf+1d3u1Y/Apzfx1qI9LeNe/jXjLUVzalSSlV7GtRi7bK34IbpcOLvrf2mnWDQXeFfX1KEW/UjQP3kBNo2TgHg6W/WRCGzSilVvWlQi7XEOtBuQNnZRCQ+/Os9RWVLajMegeKCI7uvjDn+yHZRSZnZxpRSqtbRoFYZ4iIIaiWFZdvU1n4Dv44/stunXWM62ouI/nnS4mjlUCmlqiUNapUhWEnNmLIls5Ji/HpD7ttUZnfssG4ATFm8jZ/X7YpSJpVSqvrRoFYZfCc2dvr42rL7JYUuM4zYY96K8mHPBs47rvWRM+/P2USJR9daU0rVThrUKkOwktqKz8vue3w6ikBp78k3z4QX+xK/bQHv3zAQgKlLs3l2usvQAKWUqgU0qFWGSNrUjMdnnBqlM/5n2+uzrZ7KKcc2O3L65Znr2HOwsIKZVEqp6keDWmWIpPcj+Fc/+o1zM4gI/7vz1CNH/jRxYfnyppRS1ZgGtcoQSUkN8Kt+XPgfOLzXL1X3Vg2PLCL6w9pdbNp9sJwZVEqp6kmDWmUIOaOITxDzLakV7IdPbnQ9P+H60uXl7vl4STkzqJRS1VNYQU1E6olY38Qi0lVELhARl+nkVVjqRbhoqdv6apnfuiZtlJLI9ad0Aqzps35ZtzvS3CmlVLUVbkltNlBHRNoC3wBXAe/EKlM1Xv2WEV4QWRf9vwzvdmT70a9WRPhaSilVfYUb1MQYcwi4BHjFGDMC6Bm7bNVwzTpHlv6310IkKBv06iTG8+3YIQAs35bLrDU5kb2eUkpVU2EHNRE5CRgDfGUfi7S3g/Kq3wKOGxF++llPRPwSx7aoz4j+1vqr1739m3bxV0rVCuEGtTuB+4HPjDHLReQYYGbsslULpJ0QvXu5tbkBd5zRBQCPgf6PTsejM40opWq4sIKaMWaWMeYCY8wTdoeRXcaYO0JdJyLDRWS1iGSKyH0u5weLyAIRKRaRy3zOXSMia+3HNY7j/UVkqX3PF0W8I5GrmWbHlm5f9GoFb+YerNo1rcuXfxxkpTBw7TtzK/g6SilVtYXb+/EDEWkoIvWAZcAKEbknxDXxwMvAOUA6MEpE0n2SbQauBT7wubYp8BAwEBgAPCQiTezT/wZuArrYj+HhvIcqp/NQOPdpuPE76DsqZi/Tq22jI7ONzF6TQ25+UcxeSymlKlu41Y/pxphc4CLga6ATVg/IYAYAmcaY9caYQmAicKEzgTFmozFmCeC7ENjZwHRjzB5jzF5gOjBcRFoDDY0xc4wxBphg56n6EYEBN0FaRsXvFaD60esvZ3c/sj369TnsP6SBTSlVM4Ub1BLtcWkXAVOMMS6z7PppC2xx7G+1j4Uj0LVt7e3y3LPW6tOuMf+5cSDN6yexLCuXh79YXtlZUkqpmAg3qI0HNgL1gNki0gHIjVWmokFEbhaReSIyLyenGnVp7zM6Jrc95djmTLrlJJIT4vhsYRbPTl8Tk9dRSqnKFG5HkReNMW2NMecayybg9BCXZQHtHPtp9rFwBLo2y94OeU9jzGvGmAxjTEZqaoQzeFSmdgNCp/ETpNA86RoYPwRKijkmtT5/ON3qoPLid2v5x9SVmBBVl0opVZ2E21GkkYg86y35iMgzWKW2YAZoQiAAACAASURBVOYCXUSkk4gkASOBKWHmaxowTESa2B1EhgHTjDHbgVwROdHu9Xg18N8w71k9hJwX0sXij9yP78+y1mfbvggO7ADg96eVDvx+bfZ6vliyvTy5VEqpKincb9C3gDzgcvuRC7wd7AJjTDFwO1aAWglMsse4jRORCwBE5AQR2QqMAMaLyHL72j3AI1iBcS4wzj4G8HvgDSATWIfVcaXmiIuHy96K7JqDO8vuGwMTx8D7lzqOlQCQEB/HJ7eddOTwKzMzdfyaUqrGkHCqn0RkkTGmb6hjVVVGRoaZN29eZWcjuIcbWc8X/RvaDYSXjo/w+v2l2wd3w1PHlD1/x0JoWnps277DnPnsLA4VlpAUH8eqR4YTFxeFIX87lsOcV+D0B6Bh64rfTylVaURkvjEmCl20j55wS2qHRWSQd0dETgEOxyZLtZzEW3ND3uA+C39AnhJYMQU+GAm7VvufL8grs9umcQp/O98aNlhY4uHPHy+mJBoltteHwsL3YcrtFb+XUkpFKCHMdLcCE0TELk6wF7gmSHpVXt42tTYRFoIL8mCSPXSwyGVx0DfPhgeyyxwaNaA9a3cc4K2fNvDZwix25ObzwU0nliPTDsX51vOeDRW7j1JKlUO4vR8XG2P6AL2B3saYfsDQmOastoqz/0kkwvmi8x3Vjxtm+58vPgzb/RcNffB36bw82qrq/GVdDj3++iWLtuyL7LXdBOrw8ukt8N4lIQeMK6VUeUTU1c4Yk2vPLAIwNgb5Ud5gEBdhL8jDe0OnGX8qbJ3vd/i83q255+xufJb0ILMTbuPSl2dzqLA4stf3FSioLZkI676Dg9Vo7KBSqtooR//xI6rnRMJVXVy4NcI+JoWatcy23mVxhYO7+MPJLekbt55UyaUVe3j861XkVWSeyJBDE/TPRykVfRUJalp/FE2D7oLWfaHLsNJj3gDXe2To6/dtDu91igvK7hcehKc6w+Ptyxye8Msmjnv4G74q7zi2UIsnVNPFFZRSVVvQoCYieSKS6/LIA9ocpTzWDmc+DLfMgoTk0mP3bYb7tkBCUvRep8QnqHmDoSmdU7p3e++CCIZXPvyEu/4zpxwv5BK0tB1NKRVjQeu6jDENjlZGlIske9KW8lZJuvEtqZX4r4j93OV96bW0iA3fvsbTieOZtao3b/04gesHdQr/ddyqHz0lpdvGd2EGpZSquIpUP6qjJapBLR+2/AbbFlr7Jf7tZnUS4vj9aZ0ZVecXAIbEL2Hclys4/6Ufwh/L5la96HF0PnEGOKWUihINatVBNIPammnw5lnw2mmw+mv38WSmBBGhT7umZQ4vy8ql8/9N5c0fN4SeCNmtpGZK3LeVUipKNKhVB+WZ5DiQPEfHjw9Hwqc3+qfxlM4T6XV+79Iprx75cgVTFm8L/jqhSmpa/aiUigENatVBXIQDsSvKWzXoCKbP9VzHKR3qHtn/08RFvDZ7XeB7hGpT0+pHpVQMaFCrDqJZ/RiOI1WDpaWtxM9u5D/tv+Sjm0un0frH1FV0vO8r7v90KetyDpStknStfvS4b6uK27EC5rwKHv1cj7pdmfDzv/w7YalKoUGtOjjaQe3gLuvZNzCt/IKBxzRj4+PncVZ6yyOHJ/62kWHPzGDaMufckpVc/ZiXDeu/j+1rVCX/Pgn+d681Y0tts2oqvHsBHKikWWr+1R+++SssfK9yXl+VoUGtOoh0HsiKevd8a55I33axuMQjm6+MOZ6URCtf7yQ+ybo6V9H24+FHzptYd+kvPAiznoLdAapAn+8NEy6EdS4zqNQE+fth0y/+Y/9yXFZoqOkmjoINs2DGuMrNR64uuFsVaFCrDpxtauc+fXRec/GH/iW1+NISYyIeVo7tzpd/HMSQeGui5OPiNh45v2HXQfYc9BkDF6hLf0lR5NVmM/8BMx+Ff5/sft47yHxrFV9Hr7xeHwpvD4eVX5Q97vZjYe4bsOiDo5OvyuSc1LsyNEqLzn22zrdqGgoOQFF+dO5Zi2hQqw6cQe0El96KsbDxR/yqEOMdM5tMugpe6EOvg7+6Xp5zsIjjH5nO+3M2sXyb/WXj1qW/uBCe6Q5vDfO/STDZS+3rQ/ynr6nTce3OtJ4zpwdPV3AAvvozfH5b7PMUK7sy4ZsH4NCe4Okqo53W+eMsPgoz/+SsgTeGwjPd4J9t4cljQl+jytCgVh10/5313O5E9y/pS16HxLpw6p+j95rZwasfWT3Vep7/tuvlHmP9aT3w+TLOe/FH1n71PLxzfmkC7xfQvk1waBdsnRtZ/tx6hObnWiWS/NzSYzU1qHn5jRf02XeZMabaeX0o/PwSfP2X4OmiPQ3bog+sv9lgJUDn5AWeCq5sAbBzRdl9t7URVVAxDWoiMlxEVotIpojc53I+WUQ+ss//KiId7eNjRGSR4+ERkb72ue/te3rPtYjle6gSmh8L96yH66a6n+99Ofx1O5x8R3Rf13c5m/gEa65I5y/mAD2+PD6lvC5zH4LcLEcC/2EDEXXzd2tnnHK7VSJxlkqiOcavSvL5Ivf9YnfuR+tL32UWmpgqsIPKDscXfnEh7N1UNl00g9qO5dbf0cYf4JeXA6dz/miIxow5iXVDp1FBxex/vIjEAy8D5wDpwCgRSfdJdgOw1xhzLPAc8ASAMeY/xpi+xpi+wFXABmPMIsd1Y7znjTE7Y/UeqpR6zfxLJ70uhcsdPa4C/YdIbuR+PJQ968vuFx6E54+DJx1zQAao/ju+QxPX417zNu22NpxfCgV5wfMz72149VQ4uNu9pOZtX1r1peNgDEtqO1dav+Rd1qirMkyU59vM3Q6PpMKUCH9AFR6CN4fBL69U4MUdQWvChfBCb6uzzJHTUax+nPFo6XbBgcDpygS1ktJrH+8Q/soZTvEuPZ2Nsf4vvj4U1nwT+T1rmVj+jB0AZBpj1htjCoGJwIU+aS4E3rW3JwNniPjVF42yr1W++l0J6ReU7scnuqdr06d898/z6c3lthDppp9cL03Z+iMb/68fGx8/jxH9/RvQ//HFMp6dvoYnv1pcetAb1H54FlZMKXtB4SH48k6rWnTOywGGObgEsN1rYfpDselEMHGM9Uv+rbOjf+9gVgUosYN/yazocOl+NKrHFn8IGFjwLpREcL/FH8KWX2Ha/RXPA8Dmn63nFf8tPRbNoFZmooAg79O3pLb5V5j9FBTmWc8rplilynC5lYJLiuCLOyFrPnwwIvx71VKxDGptgS2O/a32Mdc0xphiYD/QzCfNFcCHPsfetqse/+YSBGsRn7ce6KOI1ji3SAPDtw8D8NQI/6Aah4cXv1vL3DWOKsmCPMheBt/93eqI4g1sezbAP0qn6cJ43KsV3Y4tfB9+er7sL+9o8Y7n80ShOu7wXlj2aXgDeCeOKt32q3FzHPj4Wqs04xWNWVyc93htSPiBLdKByYf3htcj1vlvbjzRm6km3HlKfYOas8PTggnW3/HsJ639rPnw+R+smoZA3D4nTxHk7wue372b4LNbrY4mvj9mapkq3eAgIgOBQ8aYZY7DY4wxxwGn2g/XJZ9F5GYRmSci83JyKmlQZqyF214UF6AEF6lIf+l7S14uv1Tj7C/fj5NLxxY9N3UB+3Y7BnB7V/Ne8pHPxYnu1Y/Bft/sWhtWlivNh6Nh8nXw/T8rdh9nSW3F52XPRaOk5gzgO5ZZpa+w8hUiQHk81o+gmf+0ejs+0REmjg59X+e/+cGd8EQn+F8USoPhto+F01HEW7J+fSgset8aqB3wfi6lupKi0MH642us0vD7l8AXf4LHWlmfYy0Uy6CWBbRz7KfZx1zTiEgC0Ahw/owZiU8pzRiTZT/nAR9gVXP6Mca8ZozJMMZkpKamVuBtVGHBvsR7XBD43NHi/YIt9G+T+OimE3jx8p5ljv26djsvfPC5X1q/L8T4RPeOIsGC/PqZMOnqyKqCKqK4MLJSg7c6bc20CF8oRO/HMqeiUIrxrR47vBcWfwSbXRaSzd8Pb51jlVhCvfb6GfDjczDrcWuGDoA1X8P8d0vT7FxRtroRyv4f2L7Y6lQypyLtdjaPT0nNGCtYzPYZJ+rWpuYrzufv0m1ljLxs+O11OORSivMUh/5b2rnSet6/xaoahtLnWiaWQW0u0EVEOolIElaA8mkoYQpwjb19GTDD2BMIikgccDmO9jQRSRCR5vZ2InA+sIzaqklH/2O/nwND7oPjry49tjbSL8ooWfO19Vx0yO+UYLggvezSNufG/cpDiWWnGjrtqZkUF/v8Ao6LDzDJc4ia6BX/ha/uCpXrCAQIIMWFVknj1UFlj6//Ht44y/oFnbcDfnzef+xVNMY6BRJp1VzOGv9f+75Vrcsmw2c3W+2Kmd+WPbfwP1awnvJHWD/LcQ+XUpt3fT9fX/h0SJl0ddn9QD9kDkTQf6y40L9K0PlDyuOxgsX8d2DGIz7XOqoLA1VDFx0uO4h620L/nptvnwNT73YftjDtr6FL2W7/tj+/WHlTh1WimAU1u43sdmAasBKYZIxZLiLjRMRbjHgTaCYimcBYwNntfzCwxRjj7IKXDEwTkSXAIqyS3uuxeg9V1h8XwHVfQ+P2/uda9IDT74d6zY9+vtwY4z4rgqfEr95/ZPwMv2Qbdx9i/CyfqsPvxpWtklz5pVWCCKc6duH79ut7YNaTZXvPBeNWwgvUhTw3yxpf5DvmaMKFsPU3+OwWa9mfbx+yOps4p/JKSA4vP4EE69YeSfVjSTG8fEJpqenIPXy+PJ09P+f5jFl0/vBY913p9hMdypbAPJ7yt3kG+je323PDMv5UeOqYstNc+XUUcfxg8n7GRYfLdhKa9YT7/XdnwrPdS/dLCkrbOtd/D1P/4t/T2GnJxMAl3UN7YN+WwOcnXxf4vjVUTGfKNcZMBab6HHvQsZ0PuHbnMcZ8D5zoc+wg0N8tfa3SrLP1CMbZjnb5e9YA5y+jWUoJU/5+927/xuNXgksS//+YnSWLREKUMD4aA0P/Fv5A631bIGsezHzM2n84QAeYQ3sgpYk1wPzLu2DMZOhyFiz60Bo07uQpKf0Sd37R5udCnYZl0x7MKb1+88/w3kWl53atscZFDbzNv9rKjd+4NA/kboP6Lf3TRlJSc1arGWN9tns3lR1rCGUD5drpVkefVr2s/TqN3e9dkGuVwPrblTTLJoefLz8B/s3dqvHcLPoQclZZ25t/gV6XWNu+HUWcJbfCg5Bc3+rIFO7gdreewz+9CNP/Ft71zs/53QvgvGet8avO4TVuNv4Q3v1rkCrdUURVgPMLvvv5kHF95eTjiQ7u3bi/+JM19VEI3yXfw80JX4VMZ1b8l7DHpD3fq7TEFkjWAusL45MbSn8MfDXWev78VqtDR6FjXN0GR/Wa87P/9CY7g47g4xsQnQ7vhWn/B8s+KT12aE+QXoY+QW3Lr/BsD/9qOrCDyZ/KVgU6ZS+zvrCh7Jf6+MGw4QerdOHbppXnWCy2pABePaU0r+F2sd/0c3jp3AQqqSXVszpovNjPamsL5PNbS7cnX1f6/pw/AA7uKhtUvB2gKjqEINyABmWrbDfMskrQkVSx1iIa1GoqZ5VfOL/4nXNKhjMEIM21f467DbP9j+Vm+QySrpg5WcUURdIJxLf9Jz8X3rsElkyy9r2N7M7gkrs98EDcQkep0xnA1vzPev74GiKye23paz7ZCV4/PXDa6Q+Wbmdbk0u7frYfX2e1C024wFqFYcKFsMBuw1w/ywpI3uo0Z4eQ7CXWyg3h8lbDhZqX86cXYNuiio0hDFQ6T0ixhj7sWW8F5T0b4Ku73TtpOE262lqXbtuC0mPrZ5b9PAoPwOF9pZ/10ZC71f/Yi/3Cu7aWTYp8lBfqUkdNYkroNCfdDr/8y9r2lMC1X8Gcf0PHQfA/v1nNyqrb1P14fFKlzDeYIvkkesq5SOO2RVYQWPed9cjNwn09uCJrklk3nhBdu31LOKEUF1jTQu2w+0EF+gJd7DuEM4iclaXb40+1ntd/D8dfBavtTj3eiaIrMt5r7hsw9K+hx6Z5g3FFOscEWoXBN9j98i8rXwsmwN9ClHD+d6//Mee/b0EufHRlabVlZXHpVezqYA40bhc6XQ2hQa2maplutTOldvc/17At3DTDanfxBjVTYgWzjoNgsc8ELs7g5xWo6uVor/1mS6YCA6C/vhe2OLqkf/tw5F+0xQVWNVrrvrD047Lnti1yvyaYn563Hk0dbaf7NsN7F0d+r1AmXGSVRpwqMqD8yAoMYZYQIv0R5FwzzjffXr4Lds59w34tO9D+Oj6yf5fPf1+6nbej8gNaJGrZitwa1GqywXcHPtegVdl9Z529b0nDLYAFCmqJKVB89Gcz6NamCWRvCZ3QzRaXMVaRftH++LxVEopL8P/8PrvV/Zpw7HEsgvrZraVLzkSTb2DYu7Fi9/P+LcXqy3TimIpdvz8r9Iz/vpwlZeeMLtVBuD8uaghtU6tNOtjjprq6zFXo7BjQqnfZc25VUW5dx9MGwCl/Kn/+KiAu2KDjo8FbtedW9egyTq9cso/SkMwX+lRsWrEmHWDyDfD9P6KXJ6fdFZwd5jnfedVdNHIZLlNdaVBTNdbI9+Hi8TDssdJjx5xmPfe6rPRY695wvWPAttsXdffzyu43aAM3Toc65VwRoKIOuXSZriqC9XaMREEMJmUOxLcKNRI7llWwm34VcGIFSteVpd1A9+Ma1FSNldIE+oyEJMcSNaM/tgZzdzmzbNr2jiGCpgQ6nFL2fP/r4GpH5wfvf5xAg0Abt4dOQ8qfdwj8nxbce4cpVV6BxtgFMjzAwOvyaHN8+a4LtJ6iBjVVqyQkhR7I7SmB0b6TCsdZpbyh9lizMx+y0wZoa4tPhssnwGVvQ8vjypfXUN2xI+U2I4uqmhJSoN5RXA84JYKgdvId0OnUwOd9q/NDSW7gfvy2X4JXi9b1XeDEVsu69GtQU6EZj/UfrVkX/3OD74G710L/a639QFMxxSdZXxS9LoE0n0lhzngovHxEY5Z5p5EfRPd+VU2D1qHTHE03zoC/bICMGyK/9or3/H9YxVK9MCdBv+AlOO0+aNkT+rkuGAK3Rjirh1tQy7jBmgLvD3Ogo0sAHXgbtArwY1FLakr58HYUCTTQtb7jF3TAoOboaOvbyaTLWfDXHaHzcXiP/7G+V4a+LpDkhqHTVGeN2weukjra6jazfszUbQo9LwqdPrGez37d6LTXnvdMeOnCnTu131XW7CUAF/7Lv/TfvJv13ON3/tcOuNn9nm5/l+c/a/3/S6oHp9xpHYtLhFtmw59Xw1njrKm73NSyLv0a1FRoR9rJwpiGKlCbWplpnhxBrdVxkNoDEuuEvveId/yPVWSJWN85GSNx4h8q8MJH0VnjrF/xoZxwU3ReL9i0VV7hBCff4FCnYcWri3tfYf2thSMlwOQCZYj/D71T7WE0DdNg7Cor6ABc+Ir193vd/0rTNvJfER6wmgSc+vn8cOtyJvxpMfw1G1r3sYbneK8Z9qj1g+DKT6H3SOtYLSup6Tg1FdgJN8Hc12Gg3RMsnAmDA5XUdi4v3XbOWXfLD6X3jU8uHRxbt7k1CTNY/znPe9qqlukyzPrV/ne7zSPSnvyJdUu72FekpOY2QW0F/FTSkzUmjesSorhMUHyS9dme87g180iw1ZPPe9qaTHlDgHkhg+l+fum0XNd8AV/fBzuWlk2T7AhkjcKY3aJFj9JhEoPugpa9rPfS5WxrMurjRsCvr0aWz/YnQkIYP57aZlhV5YPGWsE00Iz/Y1f6H+t3pVUqbTcQ6juqMOs0hJ72wPnRk6yp4zKuLzvFmVfaCdCipzV92LFD3dug3ZadAjj5j3Di763JtRNToMNJ0O5E97Q1lAY1Fdh5T8OZDzuqNcIJamFM8uqcwssZKO/dALvXWV8K4wc70sSVtjMk+VRLRRrV7smEf7Sxtl3XZHPR5njocHLZWVUqMPbsxsI/80ZSaTXYkIJn2WJa4CGOYyWLU+NDj0fb3qgvrfeHmBEj0dHL1TnW8OLXrMU4vW2ZR5a7CfOzbH9y6aKmcQnQuEPpuY6DrDakv/t0tHC2EwWaYs1pyL2w/FNre+iDpX8noz+yqtOKDlnzcNZrZs0hGY4+o0PPBHLTTCugQmnnJ2dQu+5/VsmnRQ//CQzA+pvqEWKezK5nl44Vvf4bazXsBROs/cF/sfIZznytgXj/rjucbD1qGa1+VME56+nDWa/MWf041DEL/zGnlW6fNc5amfsmn5kskupZY+QataXMF2yw9g3fmU3+5Jj54fQHrBKf72vcMB1u/SnIm/AxZnLZ9o/u55fO1F4Ob9xYdmjDoJ7HMPw4K9C+XTLc9ZrpJaWda64rvIeTdoSeEcMk1CF7fz75RSUYxwwph3tcBncstNq2el4E3c6xLwgjqF071RrKceMM6zO55QcYdCd0PQeu+sxKI1L23wGg81D3+6VfWLqd5Phba+BYOsf5BS9iVVXXbQoXvWz9LfW82CqNJPm0KV3hWImh23nWdW61DYPGls2D77ypNzgmv243EDqf7h7QyqP9QKuzSeu+Vs3B4HsqFtCUltRUBPpdaS0j0z3IL1Fn9eOpd1sdOXKzys5B2bi91ZstGOd6WG7TffW8xPol3+/K0kl9B99jzWZx/nPWrPkn3w5D7oGf/wXf/LX02nYRrDDQbqBVGnBOm3XxeOuXtXPhS68uw2DtN8Hv2aRDmd3Hxpx25MvW5GXAM0/7XbI/uTXYH+1MjzU7+3WF9zAkbjGtZQ9nx1sT+272pNI+zlrt+MklKfx7oZXHdcmFxNvf50Of+Z5/je7Hsqxchvdqxbcrd3DiMc3oaCBo2fXKT6CjPV4xrX/ZXqyjfeYLbdIB/rYLDuyw1lnr6zO11R9+g9lPw7BHrPamQ7usWUiy7AmK4yNYLNXb1lpcaK0e7V1hwdk5I6WJ9excZ9Br4C3w47PWtlu3+HYnwF0rABO7gHPTTOv/jm97moqYBjUVvoG3WgGhZa/AaZxfCiLQsLX1iNTJf4SfX4IzHnTvWHDpm3DOE2V7XnpLHL5rxwX7VT30b9a8jd610Y45zZq53stbenEux5NUzyqlZC+FxY5hAW36wZiP4bXTYNtC/9e66N/We3G2h4yaWKb0IA1aWkHZZ0HXy24bBy9Z7VbT7hzMT5m7GNx1CJt2H2TDutUwz/oCH1z4At1lM8Pi5vFGyblHrj9IHRpymFyTwvb9+Vz6b2vF74emlLZ1jk1oxh0u3whTkn/HBXe/caQzjzGGnXkFfLVkO+f3bk1yQjyLtu5jcJfmiAhFJR6MgaSERKszRIbL6sup3eBSx6L1yfXhlDuspV/6X2dNEHDu0+G1gXklJJX9ewAY9RHMeaW0KrFFDyvY7VxpLR8zaqL193H7fKudtF6AsV6NAqzOEC1xcRCnAS0aNKip8MXFQVpG8DQZN1izqDurlcrjtP+D9IugbYCFzuPi/L/Amh7jnjbQcbBKgYPusmdtX2iVwl4d5OjYYgc1Z7WViDVEYehfywY1b+miTT//oPaXDe5tSW5r1/W72rre287yf9utL/kxn0BSPbq1akC3VlYb1bEt6kPbErALOBsfPw9jDD9l7ubrL5bTtVUDfly7i+vz7+GxxLe4v+hG/9ezvVx8EftNPaZ5TqCzbGe1J41smkE+3PG372iUkkh664b8sr60FD3uyxVHtu8d3p2ze7bkpgnzEBGm3nEqifHC96tz6NaqAW0auy+HdLiwhIR4ITH9QqtE5B1fN6AcPTJPvgPytlvtUgDdhlsPL5HSaknvit5grSKtagQx4dSjV3MZGRlm3rwA6y6p6m/PBmvF5lYBSpDGwLcPQZNO7qUGX/u2WKtjg9UT7qbvrM4Jj9pB9OH97mk7DbZ6/+XnWtVZa78t7QX4sM+8jTMetUqE1051r3LyeGCcXWX20L7gPU/zsuGZbu6v46OoxMO/ZmTSpWV9Mjo05csl2yjxGPKLPLzz8wZSEuPZtj92XcBbN6rDdad05I0fNnDXWV05pXNzhj0/i/wiD9ec1IG1Ow/w0qh+JCXEUVRiyNp7mK6t6pOcULEljeas302rhnXo2Ny3o5EKRkTmG2NC/JKtWmIa1ERkOPACVlX9G8aYx33OJwMTgP7AbuAKY8xGEekIrAS8CyfNMcbcal/TH3gHSAGmAn8yId6EBjUVsXcvsLq3D3vMapsD2LUW4hP9u1N72+xumlG2ZOkpgR+fs+a8bHdC5HlY9KHVk6335cHT5e+Hx+0xXCGCWijGGGas2snOvAKy9h5mzInteXlmJrsPFNKrbSOS4uPYtOcg/124jbyCKM/wEkBqg2Ry8qyhHp/cZvXmS4gTurZswOGiEg4WFNOuaV227TvMd6t20ietEakNkmndyCoZTvhlIw/+1yp5b3z8PNfXUO40qDlvLBIPrAHOArYCc4FRxpgVjjS/B3obY24VkZHAxcaYK+yg9qUxxu+nt4j8BtwB/IoV1F40xnwdLC8a1FTECg9C1nxrIudwuv6XFJedNeVom/e21XOv94ij/9Ib99CxeT1mrc6hc4v6pLduyPhZ65izYTe/rNuNpxIqg5IS4nhpVD9e/G4ty7flHjm+/h/nkrXvMDvzCkhOiKOg2EP/Dk3I3HmAH9bmMKxnK+onJZBfXEJq/WQKij2kJFn//lv3HiI5IZ7UBv6dWNbuyOPB/y7nnuHdOL59k6P2PmNNg5rzxiInAQ8bY8629+8HMMb805Fmmp3mFxFJALKBVKADLkFNRFoDM40x3e39UcBpxphbguVFg5pSla/EY9h9sIAf1+6iWf1krnnrN45tUZ8Hz09n8ZZ9fLJgK1v3Hqa4MqJgEC+M7MvO3AIem7qSpPg4bh9qtb+1bZzCmT1asm7XAa57ey77DxfRIDmBJQ8PY8HmfWzafZBLjg8wa0g1oUHNeWORy4Dhxpgb7f2rgIHGmNsdaZbZabba++uAgUB9YDlWZIAsPwAADQBJREFUSS8XeMAY84OIZACPG2POtNOfCtxrjPHrYy4iNwM3A7Rv377/pk1RWtNKKXVU5BeVsP9wEdn78+nTrjFZ+w6zOjuXgiIPPds04mBhMa/OWsfq7DxWZedxapfm/LB2V2VnmxM6NmHuRmvGmRsHdSLeHkvRq00jzkpvyd5DhcxanUPjuomc0aMl2fvzefPHDdwy5Bj+79OldGhWj4cv6Mm+Q4Vs2XOY49Ks3r/7DhWSlBBH3aSjVyNQHYNaVe39uB1ob4zZbbehfS4iPSO5gTHmNeA1sEpqMcijUiqG6iTGUycxnpYNrW79bRun0NanB+ULI/uV2T9cWMKO3HzW5Rwgv8hD3/aNWbU9l47N63G4sITkhDh25hXw0JTlZO48QPdWDViVbQ3nGNw1ldlrciqcb29AA3jjx/CXS3rn5432Vo5j29+oAe0Z3KU5rRunsCxrP2f3bEVSQhzTlmWTnZtPm8YpdGlRn+6tG1BUYqifXFW/5mMjlu82C3BO8pZmH3NLs9WufmwE7LY7fhQAGGPm2yW4rnZ6Z3ne7Z5KqVoqJSmejs3rlenl6BsIu7RswLdjS2d12ZmXT0piPA3qJJK17zCzVudwQd82JCfE8d3KHRSVGOZv2svIAe34ZP5WFm7ex+ndW/DUNKsfW0KccMuQY/h43lZ25sV+RvwPf9vMh79tPrL/wOeBp1VrWCeBj245iR6ta/iKFA6xrH5MwKo+PAMr8MwFRhtjljvS/AE4ztFR5BJjzOUikgrsMcaUiMgxwA92uj0uHUVeMsZMDZYXbVNTSh0Nuw4UMGPVToZ2b8HsNTmszs6jef1kmjdIYvqKHUxdms3ZPVvywHnp7D9cxPkv/eh3jyZ1E+nRuiE/r9vt8gqR+9MZXbjrrK7lurY6Vj/Gukv/ucDzWF363zLGPCYi44B5xpgpIlIHeA/oB+wBRhpj1ovIpcA4oAjwAA8ZY76w75lBaZf+r4E/apd+pVRNtGXPIRZu2Ufvto0wwKzV1goX5/dpw/PfrmHWmhym/GEQuw8W8sJ3a1m38wAFxSWsyzlIUkIcdw/rys2DQ6xsH4QGtSpKg5pSqjbxeAwlxpAYX7G5KqtjUKtdLYhKKVULxMUJcRVaQbf60jUOlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1Rga1JRSStUYGtSUUkrVGBrUlFJK1Rga1JRSStUYMQ1qIjJcRFaLSKaI3OdyPllEPrLP/yoiHe3jZ4nIfBFZaj8PdVzzvX3PRfajRSzfg1JKqeojIVY3FpF44GXgLGArMFdEphhjVjiS3QDsNcYcKyIjgSeAK4BdwO+MMdtEpBcwDWjruG6MMWZerPKulFKqeoplSW0AkGmMWW+MKQQmAhf6pLkQeNfengycISJijFlojNlmH18OpIhIcgzzqpRSqgaIZVBrC2xx7G+lbGmrTBpjTDGwH2jmk+ZSYIExpsBx7G276vFvIiLRzbZSSqnqqkp3FBGRnlhVkrc4Do8xxhwHnGo/rgpw7c0iMk9E5uXk5MQ+s0oppSpdLINaFtDOsZ9mH3NNIyIJQCNgt72fBnwGXG2MWee9wBiTZT/nAR9gVXP6Mca8ZozJMMZkpKamRuUNKaWUqtpiGdTmAl1EpJOIJAEjgSk+aaYA19jblwEzjDFGRBoDXwH3GWN+8iYWkQQRaW5vJwLnA8ti+B6UUkpVIzELanYb2e1YPRdXApOMMctFZJyIXGAnexNoJiKZwFjA2+3/duBY4EGfrvvJwDQRWQIswirpvR6r96CUUqp6EWNMZech5jIyMsy8eToCQCmlIiEi840xGZWdj0hU6Y4iSimlVCQ0qCmllKoxNKgppZSqMTSoKaWUqjE0qCmllKoxNKgppZSqMTSoKaWUqjE0qCmllKoxNKgppZSqMTSoKaWUqjE0qCmllKoxNKgppZSqMTSoKaWUqjE0qCmllKoxNKgppZSqMTSoKaWUqjE0qCmllKoxNKgppZSqMWIa1ERkuIisFpFMEbnP5XyyiHxkn/9VRDo6zt1vH18tImeHe0+llFK1V8yCmojEAy8D5wDpwCgRSfdJdgOw1xhzLPAc8IR9bTowEugJDAdeEZH4MO+plFKqloplSW0AkGmMWW+MKQQmAhf6pLkQeNfengycISJiH59ojCkwxmwAMu37hXNPpZRStVQsg1pbYItjf6t9zDWNMaYY2A80C3JtOPdUSilVSyVUdgZiRURuBm62dw+IyOpy3qo5sCs6uao29D3XDvqea4eKvOcO0czI0RDLoJYFtHPsp9nH3NJsFZEEoBGwO8S1oe4JgDHmNeC18mbeS0TmGWMyKnqf6kTfc+2g77l2qG3vOZbVj3OBLiLSSUSSsDp+TPFJMwW4xt6+DJhhjDH28ZF278hOQBfgtzDvqZRSqpaKWUnNGFMsIrcD04B44C1jzHIRGQfMM8ZMAd4E3hORTGAPVpDCTjcJWAEUA38wxpQAuN0zVu9BKaVU9fL/7d1biBdlGMfx7w/X1BR010C2LFZJik4eENLqIqwMJLrpwkRITBAkyiI6SBcSdFNEByvEig6EeJFZwV5otkoEhZFkHlJTU8rQVEijCDF7unjf1VH3X67t+vf/7u8Dw868M+zO838Wnp13Zp9RujCyWiTNzVOZfYZj7hscc9/Q12J2UTMzs2K4TZaZmRXDRe1flNiSS9LlktZK+k7SFknz83iLpNWSduSvzXlckhblz2CjpAn1jeDc5a4030hqz9ujcnu2nbld20V5vGb7tkYiaZik5ZK2SdoqaXLpeZb0SP693ixpmaSBpeVZ0luSDkjaXBnrdl4lzcrH75A0q6uf1Yhc1GoouCXXX8CjEXENMAl4IMf1JNAREWOAjrwNKf4xeZkLLD7/p9xj5gNbK9vPAi/mNm2/ktq2QY32bQ3oZWBlRFwNjCXFXmyeJV0GPARMjIjrSA+T3Ut5eX6H1D6wqlt5ldQCLARuJHVqWthZCBteRHjpYgEmA6sq2wuABfU+r16I82PgDmA70JrHWoHteX0JMKNy/InjGmkh/U9jBzAFaAdE+ofUptPzTXq6dnJeb8rHqd4xdDPeocDu08+75DxzsuNQS85bO3BniXkG2oDN55pXYAawpDJ+ynGNvPhKrbbiW3Ll6ZbxwDpgRETsy7v2AyPyeimfw0vA48DfeXs4cDhSezY4Na5a7dsaySjgIPB2nnJ9U9JgCs5zRPwMPA/8COwj5W09Zee5U3fz2vD5rsVFrY+SNAT4AHg4In6r7ov0p1sxj8VKugs4EBHr630u51ETMAFYHBHjgT84OSUFFJnnZlKD81HApcBgzpymK15pee0uF7XazqbNV0OS1J9U0JZGxIo8/Iuk1ry/FTiQx0v4HG4G7pa0h/Rmhymk+03DlNqzwalxnYhZp7ZvayR7gb0RsS5vLycVuZLzfDuwOyIORsQxYAUp9yXnuVN381pCvrvkolZbkS25JInUyWVrRLxQ2VVtWTaLdK+tc/y+/BTVJOBIZZqjIUTEgogYGRFtpDyuiYiZwFpSezY4M+au2rc1jIjYD/wk6ao8dBupQ0+xeSZNO06SdHH+Pe+Mudg8V3Q3r6uAqZKa8xXu1DzW+Op9U+9CXoBpwPfALuCpep9PD8V0C2lqYiOwIS/TSPcSOoAdwKdASz5epKdAdwGbSE+W1T2O/xH/rUB7Xh9N6im6E3gfGJDHB+btnXn/6Hqf9znGOg74Ouf6I6C59DwDTwPbgM3Ae8CA0vIMLCPdMzxGuiKfcy55Be7Pse8EZtc7rp5a3FHEzMyK4elHMzMrhouamZkVw0XNzMyK4aJmZmbFcFEzM7NiuKiZ9QBJxyVtqCw99lYHSW3VjuxmVlvTfx9iZmfhz4gYV++TMOvrfKVm1osk7ZH0nKRNkr6SdGUeb5O0Jr/jqkPSFXl8hKQPJX2bl5vyt+on6Y38rrBPJA2qW1BmFzAXNbOeMei06cfplX1HIuJ64FXS2wIAXgHejYgbgKXAojy+CPgsIsaSejVuyeNjgNci4lrgMHBPL8dj1pDcUcSsB0j6PSKGdDG+B5gSET/kRtL7I2K4pEOk918dy+P7IuISSQeBkRFxtPI92oDVkV4AiaQngP4R8UzvR2bWWHylZtb7osZ6dxytrB/H98PNuuSiZtb7ple+fpnXvyC9MQBgJvB5Xu8A5gFI6idp6Pk6SbMS+K89s54xSNKGyvbKiOh8rL9Z0kbS1daMPPYg6a3Uj5HeUD07j88HXpc0h3RFNo/Ukd3MzoLvqZn1onxPbWJEHKr3uZj1BZ5+NDOzYvhKzczMiuErNTMzK4aLmpmZFcNFzczMiuGiZmZmxXBRMzOzYriomZlZMf4BVIzE/l2odbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 966us/step\n",
      "xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,)\n",
      "\n",
      "classification_report_Fold=4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.98      0.99      8725\n",
      " Anomalous 1       0.59      0.74      0.65       287\n",
      "\n",
      "    accuracy                           0.98      9012\n",
      "   macro avg       0.79      0.86      0.82      9012\n",
      "weighted avg       0.98      0.98      0.98      9012\n",
      "\n",
      "confusion_matrix_Fold=4:\n",
      "\n",
      "True Negatives:  8574\n",
      "False Positives:  151\n",
      "False Negatives:  74\n",
      "True Positives:  213\n",
      "accuracy_score_Fold=4:\n",
      " 8787 \n",
      "\n",
      "End running time Fold=4: 210214_104438 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=5: 210214_104438 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 1.168529122596383e-05, min: 3.0825261039666358e-06, max: 0.00021248865851291224\n",
      "End running time Data Augmentation_Fold=5: 210214_110402 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=5: 210214_110402 ,-------------------------- \n",
      "\n",
      "\n",
      " Number of Final yXtrain_Fold=5 labeled 0: 69796\n",
      "Number of Final yXtrain_Fold=5 labeled 1: 16072\n",
      "Number of Final yXtrain_Fold=5 labeled 2: 0 \n",
      "\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.5271 - accuracy: 0.7450 - val_loss: 0.2290 - val_accuracy: 0.9665\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4443 - accuracy: 0.8175 - val_loss: 0.2374 - val_accuracy: 0.9600\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8224 - val_loss: 0.2449 - val_accuracy: 0.9563\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.8259 - val_loss: 0.2175 - val_accuracy: 0.9568\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8278 - val_loss: 0.2327 - val_accuracy: 0.9552\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4173 - accuracy: 0.8244 - val_loss: 0.2265 - val_accuracy: 0.9534\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8258 - val_loss: 0.2107 - val_accuracy: 0.9545\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8295 - val_loss: 0.2189 - val_accuracy: 0.9498\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8311 - val_loss: 0.2305 - val_accuracy: 0.9461\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8308 - val_loss: 0.2422 - val_accuracy: 0.9443\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.8331 - val_loss: 0.2131 - val_accuracy: 0.9485\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8306 - val_loss: 0.2186 - val_accuracy: 0.9447\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.8333 - val_loss: 0.2144 - val_accuracy: 0.9444\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8340 - val_loss: 0.2276 - val_accuracy: 0.9415\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8360 - val_loss: 0.2063 - val_accuracy: 0.9454\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8369 - val_loss: 0.2091 - val_accuracy: 0.9466\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.8365 - val_loss: 0.2007 - val_accuracy: 0.9485\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8390 - val_loss: 0.2034 - val_accuracy: 0.9454\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.8367 - val_loss: 0.1743 - val_accuracy: 0.9518\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3541 - accuracy: 0.8409 - val_loss: 0.2288 - val_accuracy: 0.9349\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8430 - val_loss: 0.1836 - val_accuracy: 0.9482\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8445 - val_loss: 0.1802 - val_accuracy: 0.9490\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8449 - val_loss: 0.1952 - val_accuracy: 0.9466\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8466 - val_loss: 0.1897 - val_accuracy: 0.9450\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8499 - val_loss: 0.1972 - val_accuracy: 0.9431\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8515 - val_loss: 0.1808 - val_accuracy: 0.9480\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8538 - val_loss: 0.2048 - val_accuracy: 0.9401\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8556 - val_loss: 0.1914 - val_accuracy: 0.9424\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8558 - val_loss: 0.1743 - val_accuracy: 0.9505\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8589 - val_loss: 0.2084 - val_accuracy: 0.9321\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8588 - val_loss: 0.1871 - val_accuracy: 0.9428\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3051 - accuracy: 0.8602 - val_loss: 0.1737 - val_accuracy: 0.9455\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8623 - val_loss: 0.1939 - val_accuracy: 0.9392\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8647 - val_loss: 0.2030 - val_accuracy: 0.9296\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8654 - val_loss: 0.1580 - val_accuracy: 0.9533\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8670 - val_loss: 0.1804 - val_accuracy: 0.9431\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2898 - accuracy: 0.8667 - val_loss: 0.1629 - val_accuracy: 0.9497\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8703 - val_loss: 0.1861 - val_accuracy: 0.9380\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8682 - val_loss: 0.1669 - val_accuracy: 0.9448\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.8715 - val_loss: 0.1754 - val_accuracy: 0.9397\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2757 - accuracy: 0.8733 - val_loss: 0.1657 - val_accuracy: 0.9463\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.8754 - val_loss: 0.1977 - val_accuracy: 0.9309\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8764 - val_loss: 0.1649 - val_accuracy: 0.9428\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2649 - accuracy: 0.8777 - val_loss: 0.1801 - val_accuracy: 0.9360\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8806 - val_loss: 0.1592 - val_accuracy: 0.9444\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8795 - val_loss: 0.1679 - val_accuracy: 0.9394\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8812 - val_loss: 0.1653 - val_accuracy: 0.9417\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2536 - accuracy: 0.8839 - val_loss: 0.1788 - val_accuracy: 0.9347\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8851 - val_loss: 0.1495 - val_accuracy: 0.9485\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.8877 - val_loss: 0.1417 - val_accuracy: 0.9522\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8887 - val_loss: 0.1664 - val_accuracy: 0.9392\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8879 - val_loss: 0.1670 - val_accuracy: 0.9387\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.8922 - val_loss: 0.1600 - val_accuracy: 0.9428\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.8940 - val_loss: 0.1711 - val_accuracy: 0.9362\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.8944 - val_loss: 0.1766 - val_accuracy: 0.9318\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2343 - accuracy: 0.8960 - val_loss: 0.1566 - val_accuracy: 0.9417\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.8956 - val_loss: 0.1384 - val_accuracy: 0.9497\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8984 - val_loss: 0.1306 - val_accuracy: 0.9538\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.8999 - val_loss: 0.1356 - val_accuracy: 0.9534\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9019 - val_loss: 0.1473 - val_accuracy: 0.9460\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2192 - accuracy: 0.9023 - val_loss: 0.1541 - val_accuracy: 0.9426\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9078 - val_loss: 0.1496 - val_accuracy: 0.9442\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9060 - val_loss: 0.1429 - val_accuracy: 0.9447\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9087 - val_loss: 0.1336 - val_accuracy: 0.9517\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9094 - val_loss: 0.1303 - val_accuracy: 0.9507\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9099 - val_loss: 0.1325 - val_accuracy: 0.9501\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9111 - val_loss: 0.1247 - val_accuracy: 0.9529\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2023 - accuracy: 0.9126 - val_loss: 0.1368 - val_accuracy: 0.9475\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.9123 - val_loss: 0.1540 - val_accuracy: 0.9400\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1995 - accuracy: 0.9143 - val_loss: 0.1327 - val_accuracy: 0.9503\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1974 - accuracy: 0.9162 - val_loss: 0.1258 - val_accuracy: 0.9519\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1940 - accuracy: 0.9177 - val_loss: 0.1557 - val_accuracy: 0.9381\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9163 - val_loss: 0.1370 - val_accuracy: 0.9478\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9176 - val_loss: 0.1295 - val_accuracy: 0.9498\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9200 - val_loss: 0.1286 - val_accuracy: 0.9496\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9218 - val_loss: 0.1162 - val_accuracy: 0.9559\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9223 - val_loss: 0.1099 - val_accuracy: 0.9588\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9213 - val_loss: 0.1208 - val_accuracy: 0.9541\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9226 - val_loss: 0.1048 - val_accuracy: 0.9605\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9239 - val_loss: 0.1198 - val_accuracy: 0.9532\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.9233 - val_loss: 0.1001 - val_accuracy: 0.9610\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9259 - val_loss: 0.1132 - val_accuracy: 0.9549\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1744 - accuracy: 0.9273 - val_loss: 0.0941 - val_accuracy: 0.9634\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9247 - val_loss: 0.1024 - val_accuracy: 0.9593\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9264 - val_loss: 0.0940 - val_accuracy: 0.9634\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9280 - val_loss: 0.0953 - val_accuracy: 0.9633\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9288 - val_loss: 0.1098 - val_accuracy: 0.9562\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9323 - val_loss: 0.1106 - val_accuracy: 0.9555\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9301 - val_loss: 0.1307 - val_accuracy: 0.9466\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9347 - val_loss: 0.1001 - val_accuracy: 0.9606\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9325 - val_loss: 0.1025 - val_accuracy: 0.9585\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9328 - val_loss: 0.1096 - val_accuracy: 0.9536\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1586 - accuracy: 0.9353 - val_loss: 0.1155 - val_accuracy: 0.9517\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.9340 - val_loss: 0.1164 - val_accuracy: 0.9524\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9373 - val_loss: 0.1066 - val_accuracy: 0.9572\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9381 - val_loss: 0.1122 - val_accuracy: 0.9531\n",
      "Epoch 97/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.9363 - val_loss: 0.1022 - val_accuracy: 0.9596\n",
      "Epoch 98/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9370 - val_loss: 0.0906 - val_accuracy: 0.9623\n",
      "Epoch 99/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9376 - val_loss: 0.0994 - val_accuracy: 0.9583\n",
      "Epoch 100/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9381 - val_loss: 0.1058 - val_accuracy: 0.9557\n",
      "Epoch 101/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9382 - val_loss: 0.1179 - val_accuracy: 0.9519\n",
      "Epoch 102/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9410 - val_loss: 0.0866 - val_accuracy: 0.9650\n",
      "Epoch 103/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9401 - val_loss: 0.1102 - val_accuracy: 0.9543\n",
      "Epoch 104/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9413 - val_loss: 0.0997 - val_accuracy: 0.9588\n",
      "Epoch 105/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9417 - val_loss: 0.1017 - val_accuracy: 0.9579\n",
      "Epoch 106/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9408 - val_loss: 0.0910 - val_accuracy: 0.9623\n",
      "Epoch 107/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.9426 - val_loss: 0.0964 - val_accuracy: 0.9612\n",
      "Epoch 108/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9444 - val_loss: 0.1101 - val_accuracy: 0.9539\n",
      "Epoch 109/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9428 - val_loss: 0.0962 - val_accuracy: 0.9597\n",
      "Epoch 110/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9440 - val_loss: 0.1128 - val_accuracy: 0.9524\n",
      "Epoch 111/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9450 - val_loss: 0.0846 - val_accuracy: 0.9646\n",
      "Epoch 112/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9441 - val_loss: 0.0974 - val_accuracy: 0.9583\n",
      "Epoch 113/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9459 - val_loss: 0.1011 - val_accuracy: 0.9571\n",
      "Epoch 114/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9443 - val_loss: 0.0979 - val_accuracy: 0.9603\n",
      "Epoch 115/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9467 - val_loss: 0.0856 - val_accuracy: 0.9632\n",
      "Epoch 116/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9472 - val_loss: 0.1149 - val_accuracy: 0.9504\n",
      "Epoch 117/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9457 - val_loss: 0.0960 - val_accuracy: 0.9605\n",
      "Epoch 118/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9461 - val_loss: 0.0800 - val_accuracy: 0.9672\n",
      "Epoch 119/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9482 - val_loss: 0.0906 - val_accuracy: 0.9599\n",
      "Epoch 120/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9466 - val_loss: 0.0980 - val_accuracy: 0.9578\n",
      "Epoch 121/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9486 - val_loss: 0.1031 - val_accuracy: 0.9572\n",
      "Epoch 122/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1308 - accuracy: 0.9492 - val_loss: 0.0864 - val_accuracy: 0.9629\n",
      "Epoch 123/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1324 - accuracy: 0.9473 - val_loss: 0.0790 - val_accuracy: 0.9670\n",
      "Epoch 124/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9476 - val_loss: 0.1013 - val_accuracy: 0.9584\n",
      "Epoch 125/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.9482 - val_loss: 0.0845 - val_accuracy: 0.9632\n",
      "Epoch 126/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9490 - val_loss: 0.0840 - val_accuracy: 0.9646\n",
      "Epoch 127/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9501 - val_loss: 0.0891 - val_accuracy: 0.9629\n",
      "Epoch 128/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9488 - val_loss: 0.0798 - val_accuracy: 0.9659\n",
      "Epoch 129/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9507 - val_loss: 0.0895 - val_accuracy: 0.9622\n",
      "Epoch 130/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9514 - val_loss: 0.0663 - val_accuracy: 0.9713\n",
      "Epoch 131/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9492 - val_loss: 0.0699 - val_accuracy: 0.9700\n",
      "Epoch 132/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9496 - val_loss: 0.0814 - val_accuracy: 0.9664\n",
      "Epoch 133/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1244 - accuracy: 0.9508 - val_loss: 0.0994 - val_accuracy: 0.9582\n",
      "Epoch 134/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1248 - accuracy: 0.9510 - val_loss: 0.1064 - val_accuracy: 0.9537\n",
      "Epoch 135/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9537 - val_loss: 0.0797 - val_accuracy: 0.9668\n",
      "Epoch 136/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9527 - val_loss: 0.0931 - val_accuracy: 0.9610\n",
      "Epoch 137/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9519 - val_loss: 0.1126 - val_accuracy: 0.9525\n",
      "Epoch 138/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9537 - val_loss: 0.0856 - val_accuracy: 0.9626\n",
      "Epoch 139/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9521 - val_loss: 0.0751 - val_accuracy: 0.9684\n",
      "Epoch 140/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.9525 - val_loss: 0.0809 - val_accuracy: 0.9652\n",
      "Epoch 141/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1202 - accuracy: 0.9531 - val_loss: 0.0751 - val_accuracy: 0.9688\n",
      "Epoch 142/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9554 - val_loss: 0.0737 - val_accuracy: 0.9686\n",
      "Epoch 143/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1218 - accuracy: 0.9515 - val_loss: 0.0831 - val_accuracy: 0.9652\n",
      "Epoch 144/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9560 - val_loss: 0.0828 - val_accuracy: 0.9634\n",
      "Epoch 145/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1154 - accuracy: 0.9551 - val_loss: 0.0912 - val_accuracy: 0.9610\n",
      "Epoch 146/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9552 - val_loss: 0.0788 - val_accuracy: 0.9677\n",
      "Epoch 147/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9541 - val_loss: 0.0726 - val_accuracy: 0.9693\n",
      "Epoch 148/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9541 - val_loss: 0.0800 - val_accuracy: 0.9656\n",
      "Epoch 149/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1151 - accuracy: 0.9556 - val_loss: 0.0738 - val_accuracy: 0.9696\n",
      "Epoch 150/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1119 - accuracy: 0.9572 - val_loss: 0.0778 - val_accuracy: 0.9663\n",
      "Epoch 151/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9567 - val_loss: 0.0754 - val_accuracy: 0.9677\n",
      "Epoch 152/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9568 - val_loss: 0.0948 - val_accuracy: 0.9594\n",
      "Epoch 153/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.9569 - val_loss: 0.0729 - val_accuracy: 0.9696\n",
      "Epoch 154/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9563 - val_loss: 0.0978 - val_accuracy: 0.9572\n",
      "Epoch 155/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9564 - val_loss: 0.0927 - val_accuracy: 0.9617\n",
      "Epoch 156/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.9581 - val_loss: 0.0807 - val_accuracy: 0.9662\n",
      "Epoch 157/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9587 - val_loss: 0.0901 - val_accuracy: 0.9622\n",
      "Epoch 158/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9553 - val_loss: 0.0713 - val_accuracy: 0.9694\n",
      "Epoch 159/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9577 - val_loss: 0.0738 - val_accuracy: 0.9679\n",
      "Epoch 160/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9575 - val_loss: 0.0679 - val_accuracy: 0.9705\n",
      "Epoch 161/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.9578 - val_loss: 0.0908 - val_accuracy: 0.9628\n",
      "Epoch 162/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1074 - accuracy: 0.9592 - val_loss: 0.0927 - val_accuracy: 0.9612\n",
      "Epoch 163/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.9570 - val_loss: 0.0864 - val_accuracy: 0.9644\n",
      "Epoch 164/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9604 - val_loss: 0.0754 - val_accuracy: 0.9683\n",
      "Epoch 165/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9597 - val_loss: 0.0692 - val_accuracy: 0.9717\n",
      "Epoch 166/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9594 - val_loss: 0.0741 - val_accuracy: 0.9705\n",
      "Epoch 167/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.9611 - val_loss: 0.0922 - val_accuracy: 0.9613\n",
      "Epoch 168/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1056 - accuracy: 0.9600 - val_loss: 0.0881 - val_accuracy: 0.9643\n",
      "Epoch 169/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9599 - val_loss: 0.0745 - val_accuracy: 0.9706\n",
      "Epoch 170/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9593 - val_loss: 0.0955 - val_accuracy: 0.9607\n",
      "Epoch 171/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9606 - val_loss: 0.0646 - val_accuracy: 0.9726\n",
      "Epoch 172/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9606 - val_loss: 0.0670 - val_accuracy: 0.9719\n",
      "Epoch 173/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9603 - val_loss: 0.0704 - val_accuracy: 0.9706\n",
      "Epoch 174/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9609 - val_loss: 0.0786 - val_accuracy: 0.9672\n",
      "Epoch 175/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9625 - val_loss: 0.0938 - val_accuracy: 0.9598\n",
      "Epoch 176/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9602 - val_loss: 0.0676 - val_accuracy: 0.9709\n",
      "Epoch 177/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.9605 - val_loss: 0.0727 - val_accuracy: 0.9700\n",
      "Epoch 178/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9619 - val_loss: 0.0819 - val_accuracy: 0.9662\n",
      "Epoch 179/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.9624 - val_loss: 0.0759 - val_accuracy: 0.9689\n",
      "Epoch 180/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.9617 - val_loss: 0.0715 - val_accuracy: 0.9685\n",
      "Epoch 181/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9615 - val_loss: 0.0640 - val_accuracy: 0.9726\n",
      "Epoch 182/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9606 - val_loss: 0.0649 - val_accuracy: 0.9718\n",
      "Epoch 183/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9609 - val_loss: 0.0874 - val_accuracy: 0.9649\n",
      "Epoch 184/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9620 - val_loss: 0.0790 - val_accuracy: 0.9675\n",
      "Epoch 185/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9622 - val_loss: 0.0896 - val_accuracy: 0.9627\n",
      "Epoch 186/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.9614 - val_loss: 0.0752 - val_accuracy: 0.9684\n",
      "Epoch 187/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9628 - val_loss: 0.0778 - val_accuracy: 0.9678\n",
      "Epoch 188/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9633 - val_loss: 0.0738 - val_accuracy: 0.9697\n",
      "Epoch 189/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9630 - val_loss: 0.0773 - val_accuracy: 0.9672\n",
      "Epoch 190/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9635 - val_loss: 0.0776 - val_accuracy: 0.9683\n",
      "Epoch 191/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9627 - val_loss: 0.0669 - val_accuracy: 0.9719\n",
      "Epoch 192/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9648 - val_loss: 0.0731 - val_accuracy: 0.9699\n",
      "Epoch 193/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9629 - val_loss: 0.0753 - val_accuracy: 0.9694\n",
      "Epoch 194/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.1003 - accuracy: 0.9620 - val_loss: 0.0645 - val_accuracy: 0.9725\n",
      "Epoch 195/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9629 - val_loss: 0.0710 - val_accuracy: 0.9710\n",
      "Epoch 196/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9652 - val_loss: 0.0651 - val_accuracy: 0.9731\n",
      "Epoch 197/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9626 - val_loss: 0.0789 - val_accuracy: 0.9677\n",
      "Epoch 198/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9638 - val_loss: 0.0775 - val_accuracy: 0.9682\n",
      "Epoch 199/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9635 - val_loss: 0.0699 - val_accuracy: 0.9721\n",
      "Epoch 200/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9647 - val_loss: 0.0694 - val_accuracy: 0.9711\n",
      "Epoch 201/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.9649 - val_loss: 0.0700 - val_accuracy: 0.9711\n",
      "Epoch 202/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9644 - val_loss: 0.0641 - val_accuracy: 0.9737\n",
      "Epoch 203/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9643 - val_loss: 0.0740 - val_accuracy: 0.9707\n",
      "Epoch 204/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9655 - val_loss: 0.0649 - val_accuracy: 0.9725\n",
      "Epoch 205/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9656 - val_loss: 0.0646 - val_accuracy: 0.9733\n",
      "Epoch 206/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9653 - val_loss: 0.0822 - val_accuracy: 0.9657\n",
      "Epoch 207/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9661 - val_loss: 0.0662 - val_accuracy: 0.9726\n",
      "Epoch 208/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9659 - val_loss: 0.0692 - val_accuracy: 0.9716\n",
      "Epoch 209/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0930 - accuracy: 0.9652 - val_loss: 0.0651 - val_accuracy: 0.9730\n",
      "Epoch 210/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0929 - accuracy: 0.9658 - val_loss: 0.0718 - val_accuracy: 0.9695\n",
      "Epoch 211/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0915 - accuracy: 0.9665 - val_loss: 0.0549 - val_accuracy: 0.9760\n",
      "Epoch 212/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0948 - accuracy: 0.9643 - val_loss: 0.0662 - val_accuracy: 0.9719\n",
      "Epoch 213/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0931 - accuracy: 0.9653 - val_loss: 0.0742 - val_accuracy: 0.9698\n",
      "Epoch 214/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0912 - accuracy: 0.9659 - val_loss: 0.0724 - val_accuracy: 0.9708\n",
      "Epoch 215/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0913 - accuracy: 0.9664 - val_loss: 0.0777 - val_accuracy: 0.9691\n",
      "Epoch 216/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0934 - accuracy: 0.9642 - val_loss: 0.0663 - val_accuracy: 0.9733\n",
      "Epoch 217/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0907 - accuracy: 0.9665 - val_loss: 0.0777 - val_accuracy: 0.9680\n",
      "Epoch 218/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0903 - accuracy: 0.9662 - val_loss: 0.0628 - val_accuracy: 0.9735\n",
      "Epoch 219/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0909 - accuracy: 0.9667 - val_loss: 0.0819 - val_accuracy: 0.9678\n",
      "Epoch 220/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0900 - accuracy: 0.9676 - val_loss: 0.0716 - val_accuracy: 0.9704\n",
      "Epoch 221/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0910 - accuracy: 0.9663 - val_loss: 0.0767 - val_accuracy: 0.9693\n",
      "Epoch 222/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0930 - accuracy: 0.9646 - val_loss: 0.0717 - val_accuracy: 0.9707\n",
      "Epoch 223/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0923 - accuracy: 0.9644 - val_loss: 0.0624 - val_accuracy: 0.9737\n",
      "Epoch 224/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0899 - accuracy: 0.9673 - val_loss: 0.0809 - val_accuracy: 0.9678\n",
      "Epoch 225/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0916 - accuracy: 0.9656 - val_loss: 0.0731 - val_accuracy: 0.9693\n",
      "Epoch 226/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0884 - accuracy: 0.9677 - val_loss: 0.0637 - val_accuracy: 0.9730\n",
      "Epoch 227/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9672 - val_loss: 0.0662 - val_accuracy: 0.9726\n",
      "Epoch 228/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.9663 - val_loss: 0.0649 - val_accuracy: 0.9730\n",
      "Epoch 229/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9684 - val_loss: 0.0687 - val_accuracy: 0.9711\n",
      "Epoch 230/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9673 - val_loss: 0.0664 - val_accuracy: 0.9733\n",
      "Epoch 231/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9675 - val_loss: 0.0635 - val_accuracy: 0.9739\n",
      "Epoch 232/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9684 - val_loss: 0.0677 - val_accuracy: 0.9717\n",
      "Epoch 233/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9685 - val_loss: 0.0688 - val_accuracy: 0.9714\n",
      "Epoch 234/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9682 - val_loss: 0.0728 - val_accuracy: 0.9706\n",
      "Epoch 235/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9685 - val_loss: 0.0605 - val_accuracy: 0.9740\n",
      "Epoch 236/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9687 - val_loss: 0.0587 - val_accuracy: 0.9746\n",
      "Epoch 237/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9673 - val_loss: 0.0677 - val_accuracy: 0.9713\n",
      "Epoch 238/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9676 - val_loss: 0.0749 - val_accuracy: 0.9689\n",
      "Epoch 239/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9690 - val_loss: 0.0675 - val_accuracy: 0.9725\n",
      "Epoch 240/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9682 - val_loss: 0.0690 - val_accuracy: 0.9715\n",
      "Epoch 241/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9678 - val_loss: 0.0550 - val_accuracy: 0.9764\n",
      "Epoch 242/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9662 - val_loss: 0.0820 - val_accuracy: 0.9670\n",
      "Epoch 243/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9694 - val_loss: 0.0777 - val_accuracy: 0.9687\n",
      "Epoch 244/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9683 - val_loss: 0.0702 - val_accuracy: 0.9709\n",
      "Epoch 245/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9695 - val_loss: 0.0715 - val_accuracy: 0.9706\n",
      "Epoch 246/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9690 - val_loss: 0.0654 - val_accuracy: 0.9729\n",
      "Epoch 247/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9698 - val_loss: 0.0680 - val_accuracy: 0.9725\n",
      "Epoch 248/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9696 - val_loss: 0.0688 - val_accuracy: 0.9715\n",
      "Epoch 249/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9696 - val_loss: 0.0707 - val_accuracy: 0.9705\n",
      "Epoch 250/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0817 - accuracy: 0.9703 - val_loss: 0.0664 - val_accuracy: 0.9730\n",
      "Epoch 251/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9691 - val_loss: 0.0548 - val_accuracy: 0.9764\n",
      "Epoch 252/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9693 - val_loss: 0.0673 - val_accuracy: 0.9727\n",
      "Epoch 253/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9706 - val_loss: 0.0582 - val_accuracy: 0.9766\n",
      "Epoch 254/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9690 - val_loss: 0.0639 - val_accuracy: 0.9730\n",
      "Epoch 255/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9701 - val_loss: 0.0671 - val_accuracy: 0.9725\n",
      "Epoch 256/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.9713 - val_loss: 0.0613 - val_accuracy: 0.9747\n",
      "Epoch 257/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9699 - val_loss: 0.0807 - val_accuracy: 0.9680\n",
      "Epoch 258/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9697 - val_loss: 0.0636 - val_accuracy: 0.9739\n",
      "Epoch 259/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9705 - val_loss: 0.0778 - val_accuracy: 0.9688\n",
      "Epoch 260/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9693 - val_loss: 0.0624 - val_accuracy: 0.9745\n",
      "Epoch 261/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9707 - val_loss: 0.0624 - val_accuracy: 0.9735\n",
      "Epoch 262/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9700 - val_loss: 0.0605 - val_accuracy: 0.9753\n",
      "Epoch 263/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9701 - val_loss: 0.0576 - val_accuracy: 0.9749\n",
      "Epoch 264/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9709 - val_loss: 0.0687 - val_accuracy: 0.9726\n",
      "Epoch 265/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9704 - val_loss: 0.0548 - val_accuracy: 0.9768\n",
      "Epoch 266/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0823 - accuracy: 0.9696 - val_loss: 0.0559 - val_accuracy: 0.9749\n",
      "Epoch 267/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9713 - val_loss: 0.0654 - val_accuracy: 0.9730\n",
      "Epoch 268/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9707 - val_loss: 0.0612 - val_accuracy: 0.9744\n",
      "Epoch 269/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9716 - val_loss: 0.0508 - val_accuracy: 0.9768\n",
      "Epoch 270/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9699 - val_loss: 0.0601 - val_accuracy: 0.9747\n",
      "Epoch 271/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9707 - val_loss: 0.0577 - val_accuracy: 0.9750\n",
      "Epoch 272/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9708 - val_loss: 0.0563 - val_accuracy: 0.9766\n",
      "Epoch 273/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9702 - val_loss: 0.0603 - val_accuracy: 0.9740\n",
      "Epoch 274/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9711 - val_loss: 0.0579 - val_accuracy: 0.9756\n",
      "Epoch 275/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9720 - val_loss: 0.0569 - val_accuracy: 0.9758\n",
      "Epoch 276/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9720 - val_loss: 0.0604 - val_accuracy: 0.9753\n",
      "Epoch 277/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9723 - val_loss: 0.0529 - val_accuracy: 0.9768\n",
      "Epoch 278/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9712 - val_loss: 0.0620 - val_accuracy: 0.9750\n",
      "Epoch 279/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9725\n",
      "Epoch 280/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9717 - val_loss: 0.0808 - val_accuracy: 0.9675\n",
      "Epoch 281/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9710 - val_loss: 0.0532 - val_accuracy: 0.9769\n",
      "Epoch 282/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9709 - val_loss: 0.0652 - val_accuracy: 0.9735\n",
      "Epoch 283/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9719 - val_loss: 0.0627 - val_accuracy: 0.9750\n",
      "Epoch 284/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 0.0662 - val_accuracy: 0.9728\n",
      "Epoch 285/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9725 - val_loss: 0.0813 - val_accuracy: 0.9683\n",
      "Epoch 286/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9725 - val_loss: 0.0669 - val_accuracy: 0.9724\n",
      "Epoch 287/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.0558 - val_accuracy: 0.9766\n",
      "Epoch 288/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9721 - val_loss: 0.0702 - val_accuracy: 0.9726\n",
      "Epoch 289/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9717 - val_loss: 0.0515 - val_accuracy: 0.9763\n",
      "Epoch 290/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9717 - val_loss: 0.0601 - val_accuracy: 0.9743\n",
      "Epoch 291/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9730 - val_loss: 0.0652 - val_accuracy: 0.9737\n",
      "Epoch 292/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9721 - val_loss: 0.0619 - val_accuracy: 0.9746\n",
      "Epoch 293/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9731 - val_loss: 0.0594 - val_accuracy: 0.9745\n",
      "Epoch 294/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9724 - val_loss: 0.0635 - val_accuracy: 0.9734\n",
      "Epoch 295/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.9724 - val_loss: 0.0643 - val_accuracy: 0.9726\n",
      "Epoch 296/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9727 - val_loss: 0.0567 - val_accuracy: 0.9757\n",
      "Epoch 297/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9737 - val_loss: 0.0670 - val_accuracy: 0.9730\n",
      "Epoch 298/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9725 - val_loss: 0.0594 - val_accuracy: 0.9755\n",
      "Epoch 299/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9736 - val_loss: 0.0651 - val_accuracy: 0.9734\n",
      "Epoch 300/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9730 - val_loss: 0.0626 - val_accuracy: 0.9744\n",
      "Epoch 301/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9728 - val_loss: 0.0680 - val_accuracy: 0.9723\n",
      "Epoch 302/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9726 - val_loss: 0.0780 - val_accuracy: 0.9690\n",
      "Epoch 303/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9728 - val_loss: 0.0565 - val_accuracy: 0.9758\n",
      "Epoch 304/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9725 - val_loss: 0.0544 - val_accuracy: 0.9770\n",
      "Epoch 305/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9728 - val_loss: 0.0576 - val_accuracy: 0.9747\n",
      "Epoch 306/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.0568 - val_accuracy: 0.9758\n",
      "Epoch 307/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9731 - val_loss: 0.0618 - val_accuracy: 0.9741\n",
      "Epoch 308/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9744 - val_loss: 0.0577 - val_accuracy: 0.9751\n",
      "Epoch 309/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9723 - val_loss: 0.0554 - val_accuracy: 0.9771\n",
      "Epoch 310/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9743 - val_loss: 0.0640 - val_accuracy: 0.9741\n",
      "Epoch 311/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.0762 - val_accuracy: 0.9700\n",
      "Epoch 312/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9740 - val_loss: 0.0578 - val_accuracy: 0.9758\n",
      "Epoch 313/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9748 - val_loss: 0.0642 - val_accuracy: 0.9733\n",
      "Epoch 314/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.0577 - val_accuracy: 0.9751\n",
      "Epoch 315/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9734 - val_loss: 0.0623 - val_accuracy: 0.9740\n",
      "Epoch 316/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9736 - val_loss: 0.0492 - val_accuracy: 0.9780\n",
      "Epoch 317/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9739 - val_loss: 0.0602 - val_accuracy: 0.9750\n",
      "Epoch 318/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9750 - val_loss: 0.0625 - val_accuracy: 0.9741\n",
      "Epoch 319/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9740 - val_loss: 0.0596 - val_accuracy: 0.9750\n",
      "Epoch 320/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9745 - val_loss: 0.0551 - val_accuracy: 0.9757\n",
      "Epoch 321/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9727 - val_loss: 0.0636 - val_accuracy: 0.9739\n",
      "Epoch 322/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9728 - val_loss: 0.0588 - val_accuracy: 0.9748\n",
      "Epoch 323/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9750 - val_loss: 0.0591 - val_accuracy: 0.9753\n",
      "Epoch 324/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9747 - val_loss: 0.0637 - val_accuracy: 0.9740\n",
      "Epoch 325/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9734 - val_loss: 0.0533 - val_accuracy: 0.9765\n",
      "Epoch 326/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9749 - val_loss: 0.0552 - val_accuracy: 0.9759\n",
      "Epoch 327/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9744 - val_loss: 0.0601 - val_accuracy: 0.9753\n",
      "Epoch 328/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9748 - val_loss: 0.0579 - val_accuracy: 0.9757\n",
      "Epoch 329/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9766 - val_loss: 0.0581 - val_accuracy: 0.9755\n",
      "Epoch 330/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9739 - val_loss: 0.0707 - val_accuracy: 0.9718\n",
      "Epoch 331/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9742 - val_loss: 0.0660 - val_accuracy: 0.9744\n",
      "Epoch 332/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9748 - val_loss: 0.0553 - val_accuracy: 0.9767\n",
      "Epoch 333/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9746 - val_loss: 0.0539 - val_accuracy: 0.9776\n",
      "Epoch 334/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9751 - val_loss: 0.0592 - val_accuracy: 0.9756\n",
      "Epoch 335/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9757 - val_loss: 0.0575 - val_accuracy: 0.9757\n",
      "Epoch 336/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9746 - val_loss: 0.0517 - val_accuracy: 0.9767\n",
      "Epoch 337/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9748 - val_loss: 0.0554 - val_accuracy: 0.9766\n",
      "Epoch 338/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9742 - val_loss: 0.0571 - val_accuracy: 0.9760\n",
      "Epoch 339/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9749 - val_loss: 0.0627 - val_accuracy: 0.9746\n",
      "Epoch 340/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9745 - val_loss: 0.0590 - val_accuracy: 0.9749\n",
      "Epoch 341/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9750 - val_loss: 0.0645 - val_accuracy: 0.9734\n",
      "Epoch 342/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 0.0616 - val_accuracy: 0.9745\n",
      "Epoch 343/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9753 - val_loss: 0.0580 - val_accuracy: 0.9761\n",
      "Epoch 344/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9756 - val_loss: 0.0615 - val_accuracy: 0.9753\n",
      "Epoch 345/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9755 - val_loss: 0.0653 - val_accuracy: 0.9738\n",
      "Epoch 346/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9751 - val_loss: 0.0575 - val_accuracy: 0.9757\n",
      "Epoch 347/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9757 - val_loss: 0.0696 - val_accuracy: 0.9726\n",
      "Epoch 348/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9761 - val_loss: 0.0531 - val_accuracy: 0.9771\n",
      "Epoch 349/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9748 - val_loss: 0.0774 - val_accuracy: 0.9701\n",
      "Epoch 350/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9756 - val_loss: 0.0589 - val_accuracy: 0.9756\n",
      "Epoch 351/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0697 - accuracy: 0.9752 - val_loss: 0.0504 - val_accuracy: 0.9778\n",
      "Epoch 352/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.0653 - val_accuracy: 0.9740\n",
      "Epoch 353/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 0.0622 - val_accuracy: 0.9746\n",
      "Epoch 354/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.0593 - val_accuracy: 0.9748\n",
      "Epoch 355/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9752 - val_loss: 0.0560 - val_accuracy: 0.9758\n",
      "Epoch 356/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9752 - val_loss: 0.0602 - val_accuracy: 0.9758\n",
      "Epoch 357/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 0.0719 - val_accuracy: 0.9717\n",
      "Epoch 358/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9759 - val_loss: 0.0544 - val_accuracy: 0.9771\n",
      "Epoch 359/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9762 - val_loss: 0.0620 - val_accuracy: 0.9749\n",
      "Epoch 360/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9765 - val_loss: 0.0612 - val_accuracy: 0.9748\n",
      "Epoch 361/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9763 - val_loss: 0.0660 - val_accuracy: 0.9741\n",
      "Epoch 362/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9762 - val_loss: 0.0529 - val_accuracy: 0.9775\n",
      "Epoch 363/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9758 - val_loss: 0.0584 - val_accuracy: 0.9750\n",
      "Epoch 364/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9762 - val_loss: 0.0587 - val_accuracy: 0.9747\n",
      "Epoch 365/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9765 - val_loss: 0.0558 - val_accuracy: 0.9757\n",
      "Epoch 366/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9767 - val_loss: 0.0607 - val_accuracy: 0.9756\n",
      "Epoch 367/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9756 - val_loss: 0.0587 - val_accuracy: 0.9751\n",
      "Epoch 368/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9758 - val_loss: 0.0669 - val_accuracy: 0.9745\n",
      "Epoch 369/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9770 - val_loss: 0.0708 - val_accuracy: 0.9718\n",
      "Epoch 370/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9763 - val_loss: 0.0509 - val_accuracy: 0.9782\n",
      "Epoch 371/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9760 - val_loss: 0.0639 - val_accuracy: 0.9745\n",
      "Epoch 372/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9770 - val_loss: 0.0817 - val_accuracy: 0.9686\n",
      "Epoch 373/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9762 - val_loss: 0.0769 - val_accuracy: 0.9700\n",
      "Epoch 374/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9776 - val_loss: 0.0598 - val_accuracy: 0.9753\n",
      "Epoch 375/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9769 - val_loss: 0.0571 - val_accuracy: 0.9767\n",
      "Epoch 376/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9773 - val_loss: 0.0561 - val_accuracy: 0.9768\n",
      "Epoch 377/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9768 - val_loss: 0.0689 - val_accuracy: 0.9736\n",
      "Epoch 378/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9759 - val_loss: 0.0565 - val_accuracy: 0.9761\n",
      "Epoch 379/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9756 - val_loss: 0.0640 - val_accuracy: 0.9745\n",
      "Epoch 380/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9776 - val_loss: 0.0536 - val_accuracy: 0.9781\n",
      "Epoch 381/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9761 - val_loss: 0.0599 - val_accuracy: 0.9755\n",
      "Epoch 382/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9767 - val_loss: 0.0478 - val_accuracy: 0.9788\n",
      "Epoch 383/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9769 - val_loss: 0.0537 - val_accuracy: 0.9779\n",
      "Epoch 384/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9765 - val_loss: 0.0655 - val_accuracy: 0.9749\n",
      "Epoch 385/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9768 - val_loss: 0.0550 - val_accuracy: 0.9770\n",
      "Epoch 386/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9761 - val_loss: 0.0674 - val_accuracy: 0.9748\n",
      "Epoch 387/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9774 - val_loss: 0.0621 - val_accuracy: 0.9748\n",
      "Epoch 388/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 0.0614 - val_accuracy: 0.9753\n",
      "Epoch 389/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9777 - val_loss: 0.0680 - val_accuracy: 0.9731\n",
      "Epoch 390/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9773 - val_loss: 0.0566 - val_accuracy: 0.9765\n",
      "Epoch 391/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9767 - val_loss: 0.0529 - val_accuracy: 0.9776\n",
      "Epoch 392/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9769 - val_loss: 0.0635 - val_accuracy: 0.9751\n",
      "Epoch 393/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9766 - val_loss: 0.0531 - val_accuracy: 0.9778\n",
      "Epoch 394/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9779 - val_loss: 0.0622 - val_accuracy: 0.9744\n",
      "Epoch 395/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9775 - val_loss: 0.0563 - val_accuracy: 0.9766\n",
      "Epoch 396/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0656 - accuracy: 0.9773 - val_loss: 0.0542 - val_accuracy: 0.9767\n",
      "Epoch 397/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9779 - val_loss: 0.0648 - val_accuracy: 0.9738\n",
      "Epoch 398/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9764 - val_loss: 0.0556 - val_accuracy: 0.9773\n",
      "Epoch 399/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9775 - val_loss: 0.0480 - val_accuracy: 0.9786\n",
      "Epoch 400/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9758 - val_loss: 0.0555 - val_accuracy: 0.9770\n",
      "Epoch 401/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9777 - val_loss: 0.0635 - val_accuracy: 0.9751\n",
      "Epoch 402/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.0552 - val_accuracy: 0.9766\n",
      "Epoch 403/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9785 - val_loss: 0.0553 - val_accuracy: 0.9764\n",
      "Epoch 404/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9790 - val_loss: 0.0621 - val_accuracy: 0.9744\n",
      "Epoch 405/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9783 - val_loss: 0.0580 - val_accuracy: 0.9766\n",
      "Epoch 406/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9777 - val_loss: 0.0599 - val_accuracy: 0.9751\n",
      "Epoch 407/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9769 - val_loss: 0.0594 - val_accuracy: 0.9756\n",
      "Epoch 408/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9780 - val_loss: 0.0604 - val_accuracy: 0.9759\n",
      "Epoch 409/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9778 - val_loss: 0.0611 - val_accuracy: 0.9751\n",
      "Epoch 410/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9773 - val_loss: 0.0552 - val_accuracy: 0.9771\n",
      "Epoch 411/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9775 - val_loss: 0.0620 - val_accuracy: 0.9754\n",
      "Epoch 412/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9767 - val_loss: 0.0647 - val_accuracy: 0.9743\n",
      "Epoch 413/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9779 - val_loss: 0.0530 - val_accuracy: 0.9777\n",
      "Epoch 414/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9783 - val_loss: 0.0508 - val_accuracy: 0.9775\n",
      "Epoch 415/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9788 - val_loss: 0.0496 - val_accuracy: 0.9778\n",
      "Epoch 416/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 0.0584 - val_accuracy: 0.9768\n",
      "Epoch 417/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9785 - val_loss: 0.0603 - val_accuracy: 0.9753\n",
      "Epoch 418/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9781 - val_loss: 0.0685 - val_accuracy: 0.9726\n",
      "Epoch 419/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9780 - val_loss: 0.0710 - val_accuracy: 0.9740\n",
      "Epoch 420/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9783 - val_loss: 0.0641 - val_accuracy: 0.9759\n",
      "Epoch 421/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9776 - val_loss: 0.0535 - val_accuracy: 0.9774\n",
      "Epoch 422/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9786 - val_loss: 0.0590 - val_accuracy: 0.9767\n",
      "Epoch 423/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9790 - val_loss: 0.0500 - val_accuracy: 0.9778\n",
      "Epoch 424/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9787 - val_loss: 0.0526 - val_accuracy: 0.9775\n",
      "Epoch 425/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9782 - val_loss: 0.0609 - val_accuracy: 0.9765\n",
      "Epoch 426/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9777 - val_loss: 0.0569 - val_accuracy: 0.9765\n",
      "Epoch 427/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9786 - val_loss: 0.0590 - val_accuracy: 0.9758\n",
      "Epoch 428/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.0485 - val_accuracy: 0.9779\n",
      "Epoch 429/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9782 - val_loss: 0.0731 - val_accuracy: 0.9727\n",
      "Epoch 430/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9777 - val_loss: 0.0524 - val_accuracy: 0.9776\n",
      "Epoch 431/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9783 - val_loss: 0.0619 - val_accuracy: 0.9753\n",
      "Epoch 432/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0672 - val_accuracy: 0.9743\n",
      "Epoch 433/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.0554 - val_accuracy: 0.9774\n",
      "Epoch 434/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9788 - val_loss: 0.0552 - val_accuracy: 0.9770\n",
      "Epoch 435/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9790 - val_loss: 0.0566 - val_accuracy: 0.9763\n",
      "Epoch 436/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9775 - val_loss: 0.0664 - val_accuracy: 0.9745\n",
      "Epoch 437/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9778 - val_loss: 0.0650 - val_accuracy: 0.9751\n",
      "Epoch 438/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9784 - val_loss: 0.0524 - val_accuracy: 0.9778\n",
      "Epoch 439/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9782 - val_loss: 0.0499 - val_accuracy: 0.9773\n",
      "Epoch 440/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9777 - val_loss: 0.0542 - val_accuracy: 0.9770\n",
      "Epoch 441/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9794 - val_loss: 0.0555 - val_accuracy: 0.9760\n",
      "Epoch 442/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9782 - val_loss: 0.0620 - val_accuracy: 0.9754\n",
      "Epoch 443/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9774 - val_loss: 0.0558 - val_accuracy: 0.9771\n",
      "Epoch 444/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9782 - val_loss: 0.0476 - val_accuracy: 0.9787\n",
      "Epoch 445/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9788 - val_loss: 0.0523 - val_accuracy: 0.9779\n",
      "Epoch 446/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9775 - val_loss: 0.0574 - val_accuracy: 0.9773\n",
      "Epoch 447/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9777 - val_loss: 0.0548 - val_accuracy: 0.9770\n",
      "Epoch 448/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9794 - val_loss: 0.0709 - val_accuracy: 0.9730\n",
      "Epoch 449/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9781 - val_loss: 0.0576 - val_accuracy: 0.9765\n",
      "Epoch 450/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9795 - val_loss: 0.0476 - val_accuracy: 0.9789\n",
      "Epoch 451/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9788 - val_loss: 0.0575 - val_accuracy: 0.9777\n",
      "Epoch 452/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0611 - accuracy: 0.9790 - val_loss: 0.0512 - val_accuracy: 0.9776\n",
      "Epoch 453/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9781 - val_loss: 0.0634 - val_accuracy: 0.9750\n",
      "Epoch 454/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9776 - val_loss: 0.0539 - val_accuracy: 0.9779\n",
      "Epoch 455/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.0520 - val_accuracy: 0.9774\n",
      "Epoch 456/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9787 - val_loss: 0.0479 - val_accuracy: 0.9789\n",
      "Epoch 457/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9787 - val_loss: 0.0478 - val_accuracy: 0.9784\n",
      "Epoch 458/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9789 - val_loss: 0.0503 - val_accuracy: 0.9782\n",
      "Epoch 459/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9790 - val_loss: 0.0495 - val_accuracy: 0.9775\n",
      "Epoch 460/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0607 - val_accuracy: 0.9754\n",
      "Epoch 461/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9782 - val_loss: 0.0523 - val_accuracy: 0.9778\n",
      "Epoch 462/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9788 - val_loss: 0.0562 - val_accuracy: 0.9765\n",
      "Epoch 463/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9792 - val_loss: 0.0612 - val_accuracy: 0.9750\n",
      "Epoch 464/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9797 - val_loss: 0.0543 - val_accuracy: 0.9766\n",
      "Epoch 465/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9788 - val_loss: 0.0535 - val_accuracy: 0.9767\n",
      "Epoch 466/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9784 - val_loss: 0.0569 - val_accuracy: 0.9765\n",
      "Epoch 467/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9788 - val_loss: 0.0602 - val_accuracy: 0.9764\n",
      "Epoch 468/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9793 - val_loss: 0.0564 - val_accuracy: 0.9763\n",
      "Epoch 469/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9791 - val_loss: 0.0588 - val_accuracy: 0.9751\n",
      "Epoch 470/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9789 - val_loss: 0.0550 - val_accuracy: 0.9771\n",
      "Epoch 471/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.0603 - val_accuracy: 0.9757\n",
      "Epoch 472/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9800 - val_loss: 0.0491 - val_accuracy: 0.9779\n",
      "Epoch 473/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9793 - val_loss: 0.0568 - val_accuracy: 0.9769\n",
      "Epoch 474/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9788 - val_loss: 0.0570 - val_accuracy: 0.9775\n",
      "Epoch 475/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9786 - val_loss: 0.0497 - val_accuracy: 0.9780\n",
      "Epoch 476/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9796 - val_loss: 0.0584 - val_accuracy: 0.9761\n",
      "Epoch 477/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 0.0501 - val_accuracy: 0.9782\n",
      "Epoch 478/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.0553 - val_accuracy: 0.9769\n",
      "Epoch 479/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 0.0545 - val_accuracy: 0.9770\n",
      "Epoch 480/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9789 - val_loss: 0.0564 - val_accuracy: 0.9774\n",
      "Epoch 481/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9788 - val_loss: 0.0516 - val_accuracy: 0.9780\n",
      "Epoch 482/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9789 - val_loss: 0.0541 - val_accuracy: 0.9771\n",
      "Epoch 483/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9790 - val_loss: 0.0543 - val_accuracy: 0.9774\n",
      "Epoch 484/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9800 - val_loss: 0.0661 - val_accuracy: 0.9758\n",
      "Epoch 485/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9778 - val_loss: 0.0507 - val_accuracy: 0.9780\n",
      "Epoch 486/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9786 - val_loss: 0.0651 - val_accuracy: 0.9750\n",
      "Epoch 487/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9791 - val_loss: 0.0579 - val_accuracy: 0.9765\n",
      "Epoch 488/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.0500 - val_accuracy: 0.9774\n",
      "Epoch 489/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9797 - val_loss: 0.0471 - val_accuracy: 0.9795\n",
      "Epoch 490/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9791 - val_loss: 0.0598 - val_accuracy: 0.9758\n",
      "Epoch 491/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9793 - val_loss: 0.0590 - val_accuracy: 0.9767\n",
      "Epoch 492/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.0514 - val_accuracy: 0.9777\n",
      "Epoch 493/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9796 - val_loss: 0.0552 - val_accuracy: 0.9770\n",
      "Epoch 494/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9803 - val_loss: 0.0640 - val_accuracy: 0.9756\n",
      "Epoch 495/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9790 - val_loss: 0.0496 - val_accuracy: 0.9771\n",
      "Epoch 496/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9807 - val_loss: 0.0502 - val_accuracy: 0.9769\n",
      "Epoch 497/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0594 - accuracy: 0.9798 - val_loss: 0.0550 - val_accuracy: 0.9769\n",
      "Epoch 498/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9791 - val_loss: 0.0571 - val_accuracy: 0.9759\n",
      "Epoch 499/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.0461 - val_accuracy: 0.9798\n",
      "Epoch 500/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9791 - val_loss: 0.0504 - val_accuracy: 0.9777\n",
      "Epoch 501/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.0592 - val_accuracy: 0.9759\n",
      "Epoch 502/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 0.0631 - val_accuracy: 0.9755\n",
      "Epoch 503/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9793 - val_loss: 0.0591 - val_accuracy: 0.9760\n",
      "Epoch 504/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.0654 - val_accuracy: 0.9746\n",
      "Epoch 505/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9793 - val_loss: 0.0600 - val_accuracy: 0.9766\n",
      "Epoch 506/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9779 - val_loss: 0.0623 - val_accuracy: 0.9756\n",
      "Epoch 507/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9798 - val_loss: 0.0621 - val_accuracy: 0.9757\n",
      "Epoch 508/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9805 - val_loss: 0.0510 - val_accuracy: 0.9779\n",
      "Epoch 509/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9792 - val_loss: 0.0551 - val_accuracy: 0.9773\n",
      "Epoch 510/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9805 - val_loss: 0.0546 - val_accuracy: 0.9766\n",
      "Epoch 511/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9800 - val_loss: 0.0581 - val_accuracy: 0.9765\n",
      "Epoch 512/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9802 - val_loss: 0.0571 - val_accuracy: 0.9765\n",
      "Epoch 513/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9795 - val_loss: 0.0563 - val_accuracy: 0.9767\n",
      "Epoch 514/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9804 - val_loss: 0.0678 - val_accuracy: 0.9746\n",
      "Epoch 515/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9785 - val_loss: 0.0540 - val_accuracy: 0.9773\n",
      "Epoch 516/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9797 - val_loss: 0.0591 - val_accuracy: 0.9765\n",
      "Epoch 517/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9798 - val_loss: 0.0552 - val_accuracy: 0.9776\n",
      "Epoch 518/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9805 - val_loss: 0.0606 - val_accuracy: 0.9757\n",
      "Epoch 519/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9795 - val_loss: 0.0572 - val_accuracy: 0.9759\n",
      "Epoch 520/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.0602 - val_accuracy: 0.9763\n",
      "Epoch 521/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9792 - val_loss: 0.0527 - val_accuracy: 0.9778\n",
      "Epoch 522/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9792 - val_loss: 0.0584 - val_accuracy: 0.9768\n",
      "Epoch 523/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9787 - val_loss: 0.0563 - val_accuracy: 0.9774\n",
      "Epoch 524/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9797 - val_loss: 0.0545 - val_accuracy: 0.9774\n",
      "Epoch 525/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9795 - val_loss: 0.0493 - val_accuracy: 0.9785\n",
      "Epoch 526/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9795 - val_loss: 0.0580 - val_accuracy: 0.9768\n",
      "Epoch 527/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9792 - val_loss: 0.0551 - val_accuracy: 0.9773\n",
      "Epoch 528/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9798 - val_loss: 0.0502 - val_accuracy: 0.9776\n",
      "Epoch 529/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 0.0620 - val_accuracy: 0.9764\n",
      "Epoch 530/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.0570 - val_accuracy: 0.9765\n",
      "Epoch 531/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9797 - val_loss: 0.0520 - val_accuracy: 0.9769\n",
      "Epoch 532/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9808 - val_loss: 0.0515 - val_accuracy: 0.9769\n",
      "Epoch 533/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9795 - val_loss: 0.0597 - val_accuracy: 0.9766\n",
      "Epoch 534/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9796 - val_loss: 0.0482 - val_accuracy: 0.9780\n",
      "Epoch 535/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.0595 - val_accuracy: 0.9767\n",
      "Epoch 536/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9797 - val_loss: 0.0527 - val_accuracy: 0.9776\n",
      "Epoch 537/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9795 - val_loss: 0.0555 - val_accuracy: 0.9773\n",
      "Epoch 538/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9807 - val_loss: 0.0542 - val_accuracy: 0.9777\n",
      "Epoch 539/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9796 - val_loss: 0.0571 - val_accuracy: 0.9775\n",
      "Epoch 540/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9795 - val_loss: 0.0519 - val_accuracy: 0.9782\n",
      "Epoch 541/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9797 - val_loss: 0.0625 - val_accuracy: 0.9759\n",
      "Epoch 542/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9798 - val_loss: 0.0572 - val_accuracy: 0.9781\n",
      "Epoch 543/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9792 - val_loss: 0.0540 - val_accuracy: 0.9778\n",
      "Epoch 544/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9800 - val_loss: 0.0636 - val_accuracy: 0.9760\n",
      "Epoch 545/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9799 - val_loss: 0.0615 - val_accuracy: 0.9761\n",
      "Epoch 546/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9797 - val_loss: 0.0576 - val_accuracy: 0.9769\n",
      "Epoch 547/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9802 - val_loss: 0.0524 - val_accuracy: 0.9765\n",
      "Epoch 548/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9812 - val_loss: 0.0541 - val_accuracy: 0.9769\n",
      "Epoch 549/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9797 - val_loss: 0.0559 - val_accuracy: 0.9764\n",
      "Epoch 550/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 0.0620 - val_accuracy: 0.9755\n",
      "Epoch 551/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9814 - val_loss: 0.0531 - val_accuracy: 0.9771\n",
      "Epoch 552/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9805 - val_loss: 0.0490 - val_accuracy: 0.9785\n",
      "Epoch 553/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9804 - val_loss: 0.0596 - val_accuracy: 0.9758\n",
      "Epoch 554/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9798 - val_loss: 0.0640 - val_accuracy: 0.9754\n",
      "Epoch 555/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.0622 - val_accuracy: 0.9754\n",
      "Epoch 556/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9799 - val_loss: 0.0499 - val_accuracy: 0.9794\n",
      "Epoch 557/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9802 - val_loss: 0.0598 - val_accuracy: 0.9759\n",
      "Epoch 558/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9802 - val_loss: 0.0566 - val_accuracy: 0.9776\n",
      "Epoch 559/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9793 - val_loss: 0.0786 - val_accuracy: 0.9719\n",
      "Epoch 560/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9802 - val_loss: 0.0672 - val_accuracy: 0.9754\n",
      "Epoch 561/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9803 - val_loss: 0.0562 - val_accuracy: 0.9770\n",
      "Epoch 562/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9806 - val_loss: 0.0509 - val_accuracy: 0.9791\n",
      "Epoch 563/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.0568 - val_accuracy: 0.9766\n",
      "Epoch 564/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9801 - val_loss: 0.0618 - val_accuracy: 0.9756\n",
      "Epoch 565/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9793 - val_loss: 0.0595 - val_accuracy: 0.9757\n",
      "Epoch 566/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9805 - val_loss: 0.0657 - val_accuracy: 0.9754\n",
      "Epoch 567/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9794 - val_loss: 0.0505 - val_accuracy: 0.9784\n",
      "Epoch 568/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9805 - val_loss: 0.0633 - val_accuracy: 0.9754\n",
      "Epoch 569/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9792 - val_loss: 0.0539 - val_accuracy: 0.9778\n",
      "Epoch 570/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9802 - val_loss: 0.0595 - val_accuracy: 0.9765\n",
      "Epoch 571/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9808 - val_loss: 0.0579 - val_accuracy: 0.9768\n",
      "Epoch 572/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9798 - val_loss: 0.0578 - val_accuracy: 0.9766\n",
      "Epoch 573/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9806 - val_loss: 0.0511 - val_accuracy: 0.9776\n",
      "Epoch 574/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9807 - val_loss: 0.0629 - val_accuracy: 0.9754\n",
      "Epoch 575/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9793 - val_loss: 0.0508 - val_accuracy: 0.9780\n",
      "Epoch 576/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.0629 - val_accuracy: 0.9747\n",
      "Epoch 577/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9799 - val_loss: 0.0544 - val_accuracy: 0.9774\n",
      "Epoch 578/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9803 - val_loss: 0.0541 - val_accuracy: 0.9767\n",
      "Epoch 579/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9811 - val_loss: 0.0565 - val_accuracy: 0.9770\n",
      "Epoch 580/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.0661 - val_accuracy: 0.9743\n",
      "Epoch 581/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9803 - val_loss: 0.0620 - val_accuracy: 0.9760\n",
      "Epoch 582/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.0549 - val_accuracy: 0.9770\n",
      "Epoch 583/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9799 - val_loss: 0.0469 - val_accuracy: 0.9798\n",
      "Epoch 584/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9806 - val_loss: 0.0599 - val_accuracy: 0.9761\n",
      "Epoch 585/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9809 - val_loss: 0.0588 - val_accuracy: 0.9765\n",
      "Epoch 586/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9804 - val_loss: 0.0596 - val_accuracy: 0.9760\n",
      "Epoch 587/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9807 - val_loss: 0.0548 - val_accuracy: 0.9779\n",
      "Epoch 588/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9811 - val_loss: 0.0519 - val_accuracy: 0.9778\n",
      "Epoch 589/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0511 - val_accuracy: 0.9773\n",
      "Epoch 590/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9807 - val_loss: 0.0687 - val_accuracy: 0.9755\n",
      "Epoch 591/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9813 - val_loss: 0.0512 - val_accuracy: 0.9778\n",
      "Epoch 592/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9799 - val_loss: 0.0576 - val_accuracy: 0.9764\n",
      "Epoch 593/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9810 - val_loss: 0.0535 - val_accuracy: 0.9770\n",
      "Epoch 594/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9796 - val_loss: 0.0544 - val_accuracy: 0.9773\n",
      "Epoch 595/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9816 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 596/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9805 - val_loss: 0.0511 - val_accuracy: 0.9785\n",
      "Epoch 597/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9809 - val_loss: 0.0604 - val_accuracy: 0.9766\n",
      "Epoch 598/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9818 - val_loss: 0.0543 - val_accuracy: 0.9773\n",
      "Epoch 599/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9809 - val_loss: 0.0597 - val_accuracy: 0.9760\n",
      "Epoch 600/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9807 - val_loss: 0.0487 - val_accuracy: 0.9781\n",
      "Epoch 601/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9803 - val_loss: 0.0517 - val_accuracy: 0.9774\n",
      "Epoch 602/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9812 - val_loss: 0.0493 - val_accuracy: 0.9792\n",
      "Epoch 603/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9802 - val_loss: 0.0557 - val_accuracy: 0.9769\n",
      "Epoch 604/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9804 - val_loss: 0.0520 - val_accuracy: 0.9785\n",
      "Epoch 605/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9808 - val_loss: 0.0518 - val_accuracy: 0.9767\n",
      "Epoch 606/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9807 - val_loss: 0.0619 - val_accuracy: 0.9764\n",
      "Epoch 607/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0609 - val_accuracy: 0.9768\n",
      "Epoch 608/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9807 - val_loss: 0.0532 - val_accuracy: 0.9776\n",
      "Epoch 609/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 0.0500 - val_accuracy: 0.9795\n",
      "Epoch 610/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9807 - val_loss: 0.0510 - val_accuracy: 0.9790\n",
      "Epoch 611/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9802 - val_loss: 0.0518 - val_accuracy: 0.9778\n",
      "Epoch 612/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9803 - val_loss: 0.0549 - val_accuracy: 0.9774\n",
      "Epoch 613/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9803 - val_loss: 0.0597 - val_accuracy: 0.9761\n",
      "Epoch 614/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9798 - val_loss: 0.0594 - val_accuracy: 0.9765\n",
      "Epoch 615/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.0527 - val_accuracy: 0.9779\n",
      "Epoch 616/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9804 - val_loss: 0.0608 - val_accuracy: 0.9763\n",
      "Epoch 617/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9810 - val_loss: 0.0577 - val_accuracy: 0.9766\n",
      "Epoch 618/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9814 - val_loss: 0.0547 - val_accuracy: 0.9775\n",
      "Epoch 619/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9808 - val_loss: 0.0569 - val_accuracy: 0.9767\n",
      "Epoch 620/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9806 - val_loss: 0.0506 - val_accuracy: 0.9778\n",
      "Epoch 621/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9819 - val_loss: 0.0604 - val_accuracy: 0.9760\n",
      "Epoch 622/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9807 - val_loss: 0.0563 - val_accuracy: 0.9767\n",
      "Epoch 623/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9811 - val_loss: 0.0519 - val_accuracy: 0.9787\n",
      "Epoch 624/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9808 - val_loss: 0.0568 - val_accuracy: 0.9774\n",
      "Epoch 625/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9804 - val_loss: 0.0537 - val_accuracy: 0.9775\n",
      "Epoch 626/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9810 - val_loss: 0.0554 - val_accuracy: 0.9778\n",
      "Epoch 627/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9811 - val_loss: 0.0537 - val_accuracy: 0.9780\n",
      "Epoch 628/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9814 - val_loss: 0.0456 - val_accuracy: 0.9794\n",
      "Epoch 629/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.0567 - val_accuracy: 0.9773\n",
      "Epoch 630/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9805 - val_loss: 0.0498 - val_accuracy: 0.9788\n",
      "Epoch 631/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9796 - val_loss: 0.0625 - val_accuracy: 0.9757\n",
      "Epoch 632/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9806 - val_loss: 0.0579 - val_accuracy: 0.9758\n",
      "Epoch 633/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9802 - val_loss: 0.0533 - val_accuracy: 0.9775\n",
      "Epoch 634/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9805 - val_loss: 0.0505 - val_accuracy: 0.9781\n",
      "Epoch 635/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9800 - val_loss: 0.0617 - val_accuracy: 0.9764\n",
      "Epoch 636/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.0614 - val_accuracy: 0.9760\n",
      "Epoch 637/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.0542 - val_accuracy: 0.9766\n",
      "Epoch 638/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.0498 - val_accuracy: 0.9777\n",
      "Epoch 639/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.0577 - val_accuracy: 0.9769\n",
      "Epoch 640/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9808 - val_loss: 0.0607 - val_accuracy: 0.9766\n",
      "Epoch 641/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9806 - val_loss: 0.0592 - val_accuracy: 0.9771\n",
      "Epoch 642/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9806 - val_loss: 0.0486 - val_accuracy: 0.9785\n",
      "Epoch 643/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9807 - val_loss: 0.0634 - val_accuracy: 0.9763\n",
      "Epoch 644/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.0505 - val_accuracy: 0.9790\n",
      "Epoch 645/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9813 - val_loss: 0.0526 - val_accuracy: 0.9778\n",
      "Epoch 646/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9803 - val_loss: 0.0550 - val_accuracy: 0.9778\n",
      "Epoch 647/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9812 - val_loss: 0.0590 - val_accuracy: 0.9765\n",
      "Epoch 648/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9813 - val_loss: 0.0646 - val_accuracy: 0.9753\n",
      "Epoch 649/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.0574 - val_accuracy: 0.9771\n",
      "Epoch 650/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9810 - val_loss: 0.0533 - val_accuracy: 0.9779\n",
      "Epoch 651/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.0539 - val_accuracy: 0.9781\n",
      "Epoch 652/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9813 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 653/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9816 - val_loss: 0.0593 - val_accuracy: 0.9765\n",
      "Epoch 654/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.0526 - val_accuracy: 0.9785\n",
      "Epoch 655/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9819 - val_loss: 0.0577 - val_accuracy: 0.9768\n",
      "Epoch 656/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9821 - val_loss: 0.0543 - val_accuracy: 0.9782\n",
      "Epoch 657/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 0.0621 - val_accuracy: 0.9766\n",
      "Epoch 658/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9814 - val_loss: 0.0591 - val_accuracy: 0.9765\n",
      "Epoch 659/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.0528 - val_accuracy: 0.9788\n",
      "Epoch 660/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9800 - val_loss: 0.0574 - val_accuracy: 0.9767\n",
      "Epoch 661/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9812 - val_loss: 0.0582 - val_accuracy: 0.9765\n",
      "Epoch 662/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0559 - val_accuracy: 0.9769\n",
      "Epoch 663/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9809 - val_loss: 0.0584 - val_accuracy: 0.9770\n",
      "Epoch 664/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9799 - val_loss: 0.0582 - val_accuracy: 0.9773\n",
      "Epoch 665/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9808 - val_loss: 0.0501 - val_accuracy: 0.9792\n",
      "Epoch 666/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9815 - val_loss: 0.0560 - val_accuracy: 0.9773\n",
      "Epoch 667/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9813 - val_loss: 0.0558 - val_accuracy: 0.9777\n",
      "Epoch 668/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9809 - val_loss: 0.0595 - val_accuracy: 0.9771\n",
      "Epoch 669/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9816 - val_loss: 0.0537 - val_accuracy: 0.9792\n",
      "Epoch 670/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9810 - val_loss: 0.0543 - val_accuracy: 0.9780\n",
      "Epoch 671/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9821 - val_loss: 0.0590 - val_accuracy: 0.9764\n",
      "Epoch 672/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9812 - val_loss: 0.0654 - val_accuracy: 0.9756\n",
      "Epoch 673/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.0590 - val_accuracy: 0.9769\n",
      "Epoch 674/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9810 - val_loss: 0.0544 - val_accuracy: 0.9773\n",
      "Epoch 675/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9821 - val_loss: 0.0570 - val_accuracy: 0.9765\n",
      "Epoch 676/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9807 - val_loss: 0.0543 - val_accuracy: 0.9787\n",
      "Epoch 677/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9803 - val_loss: 0.0580 - val_accuracy: 0.9773\n",
      "Epoch 678/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0524 - val_accuracy: 0.9775\n",
      "Epoch 679/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9816 - val_loss: 0.0535 - val_accuracy: 0.9776\n",
      "Epoch 680/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9818 - val_loss: 0.0603 - val_accuracy: 0.9770\n",
      "Epoch 681/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.0563 - val_accuracy: 0.9767\n",
      "Epoch 682/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9809 - val_loss: 0.0558 - val_accuracy: 0.9770\n",
      "Epoch 683/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 0.0495 - val_accuracy: 0.9788\n",
      "Epoch 684/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9805 - val_loss: 0.0585 - val_accuracy: 0.9782\n",
      "Epoch 685/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 0.0534 - val_accuracy: 0.9774\n",
      "Epoch 686/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9813 - val_loss: 0.0558 - val_accuracy: 0.9777\n",
      "Epoch 687/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.0525 - val_accuracy: 0.9782\n",
      "Epoch 688/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9812 - val_loss: 0.0508 - val_accuracy: 0.9787\n",
      "Epoch 689/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9805 - val_loss: 0.0467 - val_accuracy: 0.9800\n",
      "Epoch 690/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9817 - val_loss: 0.0582 - val_accuracy: 0.9771\n",
      "Epoch 691/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9811 - val_loss: 0.0540 - val_accuracy: 0.9778\n",
      "Epoch 692/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9809 - val_loss: 0.0543 - val_accuracy: 0.9774\n",
      "Epoch 693/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9801 - val_loss: 0.0565 - val_accuracy: 0.9768\n",
      "Epoch 694/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9803 - val_loss: 0.0615 - val_accuracy: 0.9757\n",
      "Epoch 695/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9812 - val_loss: 0.0501 - val_accuracy: 0.9790\n",
      "Epoch 696/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0534 - val_accuracy: 0.9774\n",
      "Epoch 697/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9814 - val_loss: 0.0478 - val_accuracy: 0.9786\n",
      "Epoch 698/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9811 - val_loss: 0.0575 - val_accuracy: 0.9773\n",
      "Epoch 699/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0539 - accuracy: 0.9806 - val_loss: 0.0588 - val_accuracy: 0.9776\n",
      "Epoch 700/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9811 - val_loss: 0.0532 - val_accuracy: 0.9781\n",
      "Epoch 701/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9819 - val_loss: 0.0555 - val_accuracy: 0.9774\n",
      "Epoch 702/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9810 - val_loss: 0.0574 - val_accuracy: 0.9770\n",
      "Epoch 703/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9819 - val_loss: 0.0523 - val_accuracy: 0.9790\n",
      "Epoch 704/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9803 - val_loss: 0.0552 - val_accuracy: 0.9778\n",
      "Epoch 705/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.0626 - val_accuracy: 0.9761\n",
      "Epoch 706/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0471 - val_accuracy: 0.9788\n",
      "Epoch 707/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9812 - val_loss: 0.0519 - val_accuracy: 0.9796\n",
      "Epoch 708/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9822 - val_loss: 0.0487 - val_accuracy: 0.9801\n",
      "Epoch 709/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9813 - val_loss: 0.0516 - val_accuracy: 0.9781\n",
      "Epoch 710/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9817 - val_loss: 0.0604 - val_accuracy: 0.9760\n",
      "Epoch 711/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9814 - val_loss: 0.0533 - val_accuracy: 0.9786\n",
      "Epoch 712/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9815 - val_loss: 0.0515 - val_accuracy: 0.9778\n",
      "Epoch 713/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9812 - val_loss: 0.0488 - val_accuracy: 0.9790\n",
      "Epoch 714/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9814 - val_loss: 0.0505 - val_accuracy: 0.9789\n",
      "Epoch 715/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9809 - val_loss: 0.0538 - val_accuracy: 0.9784\n",
      "Epoch 716/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0527 - val_accuracy: 0.9784\n",
      "Epoch 717/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9805 - val_loss: 0.0486 - val_accuracy: 0.9792\n",
      "Epoch 718/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9813 - val_loss: 0.0506 - val_accuracy: 0.9782\n",
      "Epoch 719/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9814 - val_loss: 0.0524 - val_accuracy: 0.9778\n",
      "Epoch 720/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.0547 - val_accuracy: 0.9777\n",
      "Epoch 721/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0574 - val_accuracy: 0.9768\n",
      "Epoch 722/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0512 - val_accuracy: 0.9784\n",
      "Epoch 723/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 0.0569 - val_accuracy: 0.9771\n",
      "Epoch 724/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9806 - val_loss: 0.0545 - val_accuracy: 0.9779\n",
      "Epoch 725/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9812 - val_loss: 0.0531 - val_accuracy: 0.9776\n",
      "Epoch 726/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9821 - val_loss: 0.0555 - val_accuracy: 0.9781\n",
      "Epoch 727/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9826 - val_loss: 0.0580 - val_accuracy: 0.9769\n",
      "Epoch 728/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9807 - val_loss: 0.0547 - val_accuracy: 0.9779\n",
      "Epoch 729/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9813 - val_loss: 0.0606 - val_accuracy: 0.9766\n",
      "Epoch 730/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9817 - val_loss: 0.0534 - val_accuracy: 0.9786\n",
      "Epoch 731/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9809 - val_loss: 0.0644 - val_accuracy: 0.9761\n",
      "Epoch 732/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9801 - val_loss: 0.0500 - val_accuracy: 0.9787\n",
      "Epoch 733/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9817 - val_loss: 0.0601 - val_accuracy: 0.9767\n",
      "Epoch 734/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9810 - val_loss: 0.0593 - val_accuracy: 0.9766\n",
      "Epoch 735/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9825 - val_loss: 0.0518 - val_accuracy: 0.9776\n",
      "Epoch 736/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.0607 - val_accuracy: 0.9768\n",
      "Epoch 737/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9810 - val_loss: 0.0569 - val_accuracy: 0.9765\n",
      "Epoch 738/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9814 - val_loss: 0.0648 - val_accuracy: 0.9751\n",
      "Epoch 739/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.0505 - val_accuracy: 0.9785\n",
      "Epoch 740/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9811 - val_loss: 0.0585 - val_accuracy: 0.9770\n",
      "Epoch 741/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0542 - val_accuracy: 0.9780\n",
      "Epoch 742/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9813 - val_loss: 0.0503 - val_accuracy: 0.9791\n",
      "Epoch 743/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9815 - val_loss: 0.0515 - val_accuracy: 0.9795\n",
      "Epoch 744/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9826 - val_loss: 0.0519 - val_accuracy: 0.9788\n",
      "Epoch 745/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9816 - val_loss: 0.0618 - val_accuracy: 0.9765\n",
      "Epoch 746/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9804 - val_loss: 0.0567 - val_accuracy: 0.9774\n",
      "Epoch 747/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.0546 - val_accuracy: 0.9780\n",
      "Epoch 748/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9814 - val_loss: 0.0598 - val_accuracy: 0.9773\n",
      "Epoch 749/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.0534 - val_accuracy: 0.9779\n",
      "Epoch 750/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9816 - val_loss: 0.0548 - val_accuracy: 0.9773\n",
      "Epoch 751/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9814 - val_loss: 0.0568 - val_accuracy: 0.9774\n",
      "Epoch 752/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0529 - val_accuracy: 0.9787\n",
      "Epoch 753/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9823 - val_loss: 0.0593 - val_accuracy: 0.9773\n",
      "Epoch 754/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9815 - val_loss: 0.0473 - val_accuracy: 0.9802\n",
      "Epoch 755/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0526 - accuracy: 0.9811 - val_loss: 0.0527 - val_accuracy: 0.9782\n",
      "Epoch 756/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9815 - val_loss: 0.0543 - val_accuracy: 0.9774\n",
      "Epoch 757/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9809 - val_loss: 0.0573 - val_accuracy: 0.9773\n",
      "Epoch 758/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9821 - val_loss: 0.0611 - val_accuracy: 0.9766\n",
      "Epoch 759/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9816 - val_loss: 0.0575 - val_accuracy: 0.9775\n",
      "Epoch 760/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0544 - val_accuracy: 0.9776\n",
      "Epoch 761/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9806 - val_loss: 0.0557 - val_accuracy: 0.9785\n",
      "Epoch 762/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0486 - val_accuracy: 0.9789\n",
      "Epoch 763/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9812 - val_loss: 0.0585 - val_accuracy: 0.9769\n",
      "Epoch 764/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9810 - val_loss: 0.0506 - val_accuracy: 0.9794\n",
      "Epoch 765/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9817 - val_loss: 0.0538 - val_accuracy: 0.9785\n",
      "Epoch 766/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9818 - val_loss: 0.0588 - val_accuracy: 0.9773\n",
      "Epoch 767/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9819 - val_loss: 0.0634 - val_accuracy: 0.9767\n",
      "Epoch 768/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 0.0552 - val_accuracy: 0.9789\n",
      "Epoch 769/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9813 - val_loss: 0.0611 - val_accuracy: 0.9766\n",
      "Epoch 770/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9819 - val_loss: 0.0565 - val_accuracy: 0.9771\n",
      "Epoch 771/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9813 - val_loss: 0.0508 - val_accuracy: 0.9788\n",
      "Epoch 772/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9810 - val_loss: 0.0526 - val_accuracy: 0.9789\n",
      "Epoch 773/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9813 - val_loss: 0.0550 - val_accuracy: 0.9774\n",
      "Epoch 774/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0527 - val_accuracy: 0.9778\n",
      "Epoch 775/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9814 - val_loss: 0.0539 - val_accuracy: 0.9786\n",
      "Epoch 776/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 0.0559 - val_accuracy: 0.9780\n",
      "Epoch 777/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.0527 - val_accuracy: 0.9789\n",
      "Epoch 778/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0561 - val_accuracy: 0.9774\n",
      "Epoch 779/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9818 - val_loss: 0.0523 - val_accuracy: 0.9782\n",
      "Epoch 780/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9823 - val_loss: 0.0593 - val_accuracy: 0.9771\n",
      "Epoch 781/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9827 - val_loss: 0.0640 - val_accuracy: 0.9758\n",
      "Epoch 782/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9820 - val_loss: 0.0525 - val_accuracy: 0.9788\n",
      "Epoch 783/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9817 - val_loss: 0.0569 - val_accuracy: 0.9775\n",
      "Epoch 784/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9814 - val_loss: 0.0556 - val_accuracy: 0.9770\n",
      "Epoch 785/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0588 - val_accuracy: 0.9773\n",
      "Epoch 786/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9818 - val_loss: 0.0563 - val_accuracy: 0.9782\n",
      "Epoch 787/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 0.0579 - val_accuracy: 0.9771\n",
      "Epoch 788/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0602 - val_accuracy: 0.9775\n",
      "Epoch 789/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.0591 - val_accuracy: 0.9774\n",
      "Epoch 790/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.0531 - val_accuracy: 0.9787\n",
      "Epoch 791/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9808 - val_loss: 0.0548 - val_accuracy: 0.9781\n",
      "Epoch 792/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9831 - val_loss: 0.0531 - val_accuracy: 0.9780\n",
      "Epoch 793/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9819 - val_loss: 0.0567 - val_accuracy: 0.9777\n",
      "Epoch 794/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.0623 - val_accuracy: 0.9760\n",
      "Epoch 795/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9822 - val_loss: 0.0563 - val_accuracy: 0.9774\n",
      "Epoch 796/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9825 - val_loss: 0.0572 - val_accuracy: 0.9773\n",
      "Epoch 797/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9818 - val_loss: 0.0637 - val_accuracy: 0.9764\n",
      "Epoch 798/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9833 - val_loss: 0.0605 - val_accuracy: 0.9765\n",
      "Epoch 799/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9820 - val_loss: 0.0499 - val_accuracy: 0.9790\n",
      "Epoch 800/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0504 - accuracy: 0.9816 - val_loss: 0.0603 - val_accuracy: 0.9767\n",
      "Epoch 801/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9816 - val_loss: 0.0566 - val_accuracy: 0.9774\n",
      "Epoch 802/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0490 - val_accuracy: 0.9791\n",
      "Epoch 803/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0530 - val_accuracy: 0.9781\n",
      "Epoch 804/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9817 - val_loss: 0.0634 - val_accuracy: 0.9768\n",
      "Epoch 805/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9816 - val_loss: 0.0555 - val_accuracy: 0.9776\n",
      "Epoch 806/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.0577 - val_accuracy: 0.9775\n",
      "Epoch 807/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0595 - val_accuracy: 0.9764\n",
      "Epoch 808/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9826 - val_loss: 0.0542 - val_accuracy: 0.9787\n",
      "Epoch 809/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.0530 - val_accuracy: 0.9786\n",
      "Epoch 810/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.0558 - val_accuracy: 0.9778\n",
      "Epoch 811/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.0496 - val_accuracy: 0.9790\n",
      "Epoch 812/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9820 - val_loss: 0.0528 - val_accuracy: 0.9779\n",
      "Epoch 813/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0561 - val_accuracy: 0.9775\n",
      "Epoch 814/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9822 - val_loss: 0.0574 - val_accuracy: 0.9777\n",
      "Epoch 815/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.0553 - val_accuracy: 0.9775\n",
      "Epoch 816/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9812 - val_loss: 0.0608 - val_accuracy: 0.9770\n",
      "Epoch 817/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9809 - val_loss: 0.0529 - val_accuracy: 0.9784\n",
      "Epoch 818/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9819 - val_loss: 0.0542 - val_accuracy: 0.9782\n",
      "Epoch 819/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9817 - val_loss: 0.0560 - val_accuracy: 0.9774\n",
      "Epoch 820/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9819 - val_loss: 0.0575 - val_accuracy: 0.9776\n",
      "Epoch 821/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9817 - val_loss: 0.0643 - val_accuracy: 0.9763\n",
      "Epoch 822/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9823 - val_loss: 0.0636 - val_accuracy: 0.9767\n",
      "Epoch 823/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9807 - val_loss: 0.0508 - val_accuracy: 0.9786\n",
      "Epoch 824/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9811 - val_loss: 0.0523 - val_accuracy: 0.9791\n",
      "Epoch 825/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9825 - val_loss: 0.0497 - val_accuracy: 0.9795\n",
      "Epoch 826/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9817 - val_loss: 0.0534 - val_accuracy: 0.9784\n",
      "Epoch 827/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9807 - val_loss: 0.0546 - val_accuracy: 0.9774\n",
      "Epoch 828/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9829 - val_loss: 0.0631 - val_accuracy: 0.9769\n",
      "Epoch 829/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9831 - val_loss: 0.0551 - val_accuracy: 0.9775\n",
      "Epoch 830/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9829 - val_loss: 0.0522 - val_accuracy: 0.9781\n",
      "Epoch 831/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.0595 - val_accuracy: 0.9770\n",
      "Epoch 832/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9819 - val_loss: 0.0499 - val_accuracy: 0.9791\n",
      "Epoch 833/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.0540 - val_accuracy: 0.9779\n",
      "Epoch 834/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9829 - val_loss: 0.0481 - val_accuracy: 0.9792\n",
      "Epoch 835/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0590 - val_accuracy: 0.9768\n",
      "Epoch 836/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9824 - val_loss: 0.0501 - val_accuracy: 0.9787\n",
      "Epoch 837/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9821 - val_loss: 0.0523 - val_accuracy: 0.9794\n",
      "Epoch 838/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9828 - val_loss: 0.0684 - val_accuracy: 0.9760\n",
      "Epoch 839/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9827 - val_loss: 0.0514 - val_accuracy: 0.9787\n",
      "Epoch 840/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.0546 - val_accuracy: 0.9785\n",
      "Epoch 841/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9830 - val_loss: 0.0481 - val_accuracy: 0.9792\n",
      "Epoch 842/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.0502 - val_accuracy: 0.9794\n",
      "Epoch 843/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9820 - val_loss: 0.0544 - val_accuracy: 0.9787\n",
      "Epoch 844/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9825 - val_loss: 0.0620 - val_accuracy: 0.9766\n",
      "Epoch 845/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9822 - val_loss: 0.0502 - val_accuracy: 0.9785\n",
      "Epoch 846/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9823 - val_loss: 0.0600 - val_accuracy: 0.9775\n",
      "Epoch 847/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9824 - val_loss: 0.0566 - val_accuracy: 0.9776\n",
      "Epoch 848/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9825 - val_loss: 0.0583 - val_accuracy: 0.9776\n",
      "Epoch 849/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9829 - val_loss: 0.0492 - val_accuracy: 0.9787\n",
      "Epoch 850/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0571 - val_accuracy: 0.9780\n",
      "Epoch 851/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.0600 - val_accuracy: 0.9773\n",
      "Epoch 852/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.0501 - val_accuracy: 0.9798\n",
      "Epoch 853/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.0527 - val_accuracy: 0.9780\n",
      "Epoch 854/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.0504 - val_accuracy: 0.9796\n",
      "Epoch 855/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.0632 - val_accuracy: 0.9760\n",
      "Epoch 856/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0497 - accuracy: 0.9817 - val_loss: 0.0586 - val_accuracy: 0.9774\n",
      "Epoch 857/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9823 - val_loss: 0.0590 - val_accuracy: 0.9764\n",
      "Epoch 858/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9828 - val_loss: 0.0615 - val_accuracy: 0.9769\n",
      "Epoch 859/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9814 - val_loss: 0.0515 - val_accuracy: 0.9791\n",
      "Epoch 860/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9824 - val_loss: 0.0546 - val_accuracy: 0.9779\n",
      "Epoch 861/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.0572 - val_accuracy: 0.9776\n",
      "Epoch 862/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9830 - val_loss: 0.0523 - val_accuracy: 0.9787\n",
      "Epoch 863/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.0533 - val_accuracy: 0.9789\n",
      "Epoch 864/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9821 - val_loss: 0.0607 - val_accuracy: 0.9778\n",
      "Epoch 865/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0524 - val_accuracy: 0.9784\n",
      "Epoch 866/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.0516 - val_accuracy: 0.9799\n",
      "Epoch 867/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9823 - val_loss: 0.0573 - val_accuracy: 0.9781\n",
      "Epoch 868/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9823 - val_loss: 0.0521 - val_accuracy: 0.9787\n",
      "Epoch 869/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0554 - val_accuracy: 0.9775\n",
      "Epoch 870/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9826 - val_loss: 0.0552 - val_accuracy: 0.9785\n",
      "Epoch 871/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0511 - val_accuracy: 0.9798\n",
      "Epoch 872/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.0740 - val_accuracy: 0.9750\n",
      "Epoch 873/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.0522 - val_accuracy: 0.9780\n",
      "Epoch 874/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.0660 - val_accuracy: 0.9760\n",
      "Epoch 875/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9822 - val_loss: 0.0578 - val_accuracy: 0.9781\n",
      "Epoch 876/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0488 - accuracy: 0.9819 - val_loss: 0.0574 - val_accuracy: 0.9780\n",
      "Epoch 877/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9828 - val_loss: 0.0544 - val_accuracy: 0.9779\n",
      "Epoch 878/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0495 - accuracy: 0.9820 - val_loss: 0.0588 - val_accuracy: 0.9770\n",
      "Epoch 879/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9817 - val_loss: 0.0571 - val_accuracy: 0.9771\n",
      "Epoch 880/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0478 - accuracy: 0.9823 - val_loss: 0.0548 - val_accuracy: 0.9782\n",
      "Epoch 881/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0496 - accuracy: 0.9822 - val_loss: 0.0508 - val_accuracy: 0.9786\n",
      "Epoch 882/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9825 - val_loss: 0.0611 - val_accuracy: 0.9763\n",
      "Epoch 883/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0482 - accuracy: 0.9822 - val_loss: 0.0529 - val_accuracy: 0.9784\n",
      "Epoch 884/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.0519 - val_accuracy: 0.9792\n",
      "Epoch 885/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9824 - val_loss: 0.0596 - val_accuracy: 0.9774\n",
      "Epoch 886/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0462 - accuracy: 0.9834 - val_loss: 0.0517 - val_accuracy: 0.9781\n",
      "Epoch 887/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0482 - accuracy: 0.9828 - val_loss: 0.0583 - val_accuracy: 0.9774\n",
      "Epoch 888/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9823 - val_loss: 0.0559 - val_accuracy: 0.9778\n",
      "Epoch 889/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0483 - accuracy: 0.9822 - val_loss: 0.0529 - val_accuracy: 0.9781\n",
      "Epoch 890/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0546 - val_accuracy: 0.9782\n",
      "Epoch 891/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9827 - val_loss: 0.0557 - val_accuracy: 0.9781\n",
      "Epoch 892/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9828 - val_loss: 0.0540 - val_accuracy: 0.9779\n",
      "Epoch 893/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9831 - val_loss: 0.0509 - val_accuracy: 0.9791\n",
      "Epoch 894/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9811 - val_loss: 0.0538 - val_accuracy: 0.9784\n",
      "Epoch 895/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.0654 - val_accuracy: 0.9764\n",
      "Epoch 896/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9828 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 897/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0561 - val_accuracy: 0.9773\n",
      "Epoch 898/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 0.0585 - val_accuracy: 0.9771\n",
      "Epoch 899/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0559 - val_accuracy: 0.9773\n",
      "Epoch 900/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9823 - val_loss: 0.0576 - val_accuracy: 0.9778\n",
      "Epoch 901/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9829 - val_loss: 0.0510 - val_accuracy: 0.9786\n",
      "Epoch 902/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9829 - val_loss: 0.0607 - val_accuracy: 0.9768\n",
      "Epoch 903/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9828 - val_loss: 0.0588 - val_accuracy: 0.9771\n",
      "Epoch 904/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9828 - val_loss: 0.0531 - val_accuracy: 0.9779\n",
      "Epoch 905/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9825 - val_loss: 0.0508 - val_accuracy: 0.9787\n",
      "Epoch 906/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9781\n",
      "Epoch 907/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9831 - val_loss: 0.0481 - val_accuracy: 0.9797\n",
      "Epoch 908/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0533 - val_accuracy: 0.9791\n",
      "Epoch 909/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9831 - val_loss: 0.0539 - val_accuracy: 0.9778\n",
      "Epoch 910/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9826 - val_loss: 0.0502 - val_accuracy: 0.9792\n",
      "Epoch 911/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9826 - val_loss: 0.0583 - val_accuracy: 0.9775\n",
      "Epoch 912/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9819 - val_loss: 0.0535 - val_accuracy: 0.9782\n",
      "Epoch 913/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9829 - val_loss: 0.0575 - val_accuracy: 0.9777\n",
      "Epoch 914/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0549 - val_accuracy: 0.9786\n",
      "Epoch 915/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9823 - val_loss: 0.0655 - val_accuracy: 0.9760\n",
      "Epoch 916/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9816 - val_loss: 0.0617 - val_accuracy: 0.9766\n",
      "Epoch 917/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9830 - val_loss: 0.0495 - val_accuracy: 0.9787\n",
      "Epoch 918/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.0522 - val_accuracy: 0.9784\n",
      "Epoch 919/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0526 - val_accuracy: 0.9782\n",
      "Epoch 920/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0547 - val_accuracy: 0.9779\n",
      "Epoch 921/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9831 - val_loss: 0.0534 - val_accuracy: 0.9790\n",
      "Epoch 922/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9828 - val_loss: 0.0599 - val_accuracy: 0.9776\n",
      "Epoch 923/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9833 - val_loss: 0.0605 - val_accuracy: 0.9769\n",
      "Epoch 924/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9819 - val_loss: 0.0706 - val_accuracy: 0.9754\n",
      "Epoch 925/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9819 - val_loss: 0.0517 - val_accuracy: 0.9779\n",
      "Epoch 926/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9826 - val_loss: 0.0562 - val_accuracy: 0.9770\n",
      "Epoch 927/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.0705 - val_accuracy: 0.9756\n",
      "Epoch 928/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0600 - val_accuracy: 0.9773\n",
      "Epoch 929/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9824 - val_loss: 0.0513 - val_accuracy: 0.9791\n",
      "Epoch 930/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9822 - val_loss: 0.0544 - val_accuracy: 0.9785\n",
      "Epoch 931/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9826 - val_loss: 0.0604 - val_accuracy: 0.9771\n",
      "Epoch 932/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9824 - val_loss: 0.0582 - val_accuracy: 0.9779\n",
      "Epoch 933/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.0581 - val_accuracy: 0.9774\n",
      "Epoch 934/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 0.0685 - val_accuracy: 0.9763\n",
      "Epoch 935/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9831 - val_loss: 0.0569 - val_accuracy: 0.9780\n",
      "Epoch 936/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.0511 - val_accuracy: 0.9795\n",
      "Epoch 937/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9827 - val_loss: 0.0577 - val_accuracy: 0.9779\n",
      "Epoch 938/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9825 - val_loss: 0.0555 - val_accuracy: 0.9776\n",
      "Epoch 939/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9830 - val_loss: 0.0581 - val_accuracy: 0.9761\n",
      "Epoch 940/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9835 - val_loss: 0.0531 - val_accuracy: 0.9788\n",
      "Epoch 941/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9829 - val_loss: 0.0554 - val_accuracy: 0.9784\n",
      "Epoch 942/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9829 - val_loss: 0.0564 - val_accuracy: 0.9781\n",
      "Epoch 943/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.0529 - val_accuracy: 0.9788\n",
      "Epoch 944/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9834 - val_loss: 0.0525 - val_accuracy: 0.9776\n",
      "Epoch 945/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9832 - val_loss: 0.0607 - val_accuracy: 0.9767\n",
      "Epoch 946/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9827 - val_loss: 0.0587 - val_accuracy: 0.9777\n",
      "Epoch 947/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9838 - val_loss: 0.0621 - val_accuracy: 0.9773\n",
      "Epoch 948/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9823 - val_loss: 0.0590 - val_accuracy: 0.9769\n",
      "Epoch 949/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 0.0544 - val_accuracy: 0.9784\n",
      "Epoch 950/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9824 - val_loss: 0.0552 - val_accuracy: 0.9785\n",
      "Epoch 951/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: 0.0558 - val_accuracy: 0.9778\n",
      "Epoch 952/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9831 - val_loss: 0.0603 - val_accuracy: 0.9768\n",
      "Epoch 953/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9834 - val_loss: 0.0704 - val_accuracy: 0.9754\n",
      "Epoch 954/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9814 - val_loss: 0.0555 - val_accuracy: 0.9784\n",
      "Epoch 955/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9828 - val_loss: 0.0651 - val_accuracy: 0.9768\n",
      "Epoch 956/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9830 - val_loss: 0.0530 - val_accuracy: 0.9790\n",
      "Epoch 957/1000\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9827 - val_loss: 0.0525 - val_accuracy: 0.9798\n",
      "Epoch 958/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9832 - val_loss: 0.0538 - val_accuracy: 0.9779\n",
      "Epoch 959/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9825 - val_loss: 0.0562 - val_accuracy: 0.9781\n",
      "Epoch 960/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9825 - val_loss: 0.0565 - val_accuracy: 0.9785\n",
      "Epoch 961/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9827 - val_loss: 0.0514 - val_accuracy: 0.9785\n",
      "Epoch 962/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9827 - val_loss: 0.0623 - val_accuracy: 0.9766\n",
      "Epoch 963/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0539 - val_accuracy: 0.9790\n",
      "Epoch 964/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9835 - val_loss: 0.0523 - val_accuracy: 0.9790\n",
      "Epoch 965/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9824 - val_loss: 0.0559 - val_accuracy: 0.9778\n",
      "Epoch 966/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9823 - val_loss: 0.0530 - val_accuracy: 0.9790\n",
      "Epoch 967/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9828 - val_loss: 0.0575 - val_accuracy: 0.9777\n",
      "Epoch 968/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9827 - val_loss: 0.0503 - val_accuracy: 0.9785\n",
      "Epoch 969/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9832 - val_loss: 0.0550 - val_accuracy: 0.9778\n",
      "Epoch 970/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9826 - val_loss: 0.0641 - val_accuracy: 0.9771\n",
      "Epoch 971/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9831 - val_loss: 0.0666 - val_accuracy: 0.9764\n",
      "Epoch 972/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.0534 - val_accuracy: 0.9785\n",
      "Epoch 973/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9824 - val_loss: 0.0526 - val_accuracy: 0.9788\n",
      "Epoch 974/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9828 - val_loss: 0.0719 - val_accuracy: 0.9758\n",
      "Epoch 975/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9821 - val_loss: 0.0518 - val_accuracy: 0.9791\n",
      "Epoch 976/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9831 - val_loss: 0.0583 - val_accuracy: 0.9776\n",
      "Epoch 977/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9826 - val_loss: 0.0666 - val_accuracy: 0.9761\n",
      "Epoch 978/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9825 - val_loss: 0.0506 - val_accuracy: 0.9799\n",
      "Epoch 979/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9828 - val_loss: 0.0566 - val_accuracy: 0.9778\n",
      "Epoch 980/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9832 - val_loss: 0.0595 - val_accuracy: 0.9774\n",
      "Epoch 981/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9824 - val_loss: 0.0551 - val_accuracy: 0.9787\n",
      "Epoch 982/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9828 - val_loss: 0.0525 - val_accuracy: 0.9775\n",
      "Epoch 983/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9818 - val_loss: 0.0589 - val_accuracy: 0.9770\n",
      "Epoch 984/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9825 - val_loss: 0.0585 - val_accuracy: 0.9778\n",
      "Epoch 985/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9832 - val_loss: 0.0505 - val_accuracy: 0.9797\n",
      "Epoch 986/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9823 - val_loss: 0.0593 - val_accuracy: 0.9769\n",
      "Epoch 987/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9832 - val_loss: 0.0537 - val_accuracy: 0.9780\n",
      "Epoch 988/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9829 - val_loss: 0.0525 - val_accuracy: 0.9795\n",
      "Epoch 989/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9821 - val_loss: 0.0561 - val_accuracy: 0.9778\n",
      "Epoch 990/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.0501 - val_accuracy: 0.9799\n",
      "Epoch 991/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9827 - val_loss: 0.0568 - val_accuracy: 0.9778\n",
      "Epoch 992/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9829 - val_loss: 0.0547 - val_accuracy: 0.9786\n",
      "Epoch 993/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9828 - val_loss: 0.0591 - val_accuracy: 0.9775\n",
      "Epoch 994/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9828 - val_loss: 0.0543 - val_accuracy: 0.9781\n",
      "Epoch 995/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9834 - val_loss: 0.0495 - val_accuracy: 0.9797\n",
      "Epoch 996/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9829 - val_loss: 0.0576 - val_accuracy: 0.9780\n",
      "Epoch 997/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9832 - val_loss: 0.0631 - val_accuracy: 0.9770\n",
      "Epoch 998/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.0561 - val_accuracy: 0.9780\n",
      "Epoch 999/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9829 - val_loss: 0.0574 - val_accuracy: 0.9776\n",
      "Epoch 1000/1000\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9819 - val_loss: 0.0541 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 962us/step\n",
      "xtrain: (85868, 59), ytrain: (85868,)\n",
      "xvalid: (9011, 59), yvalid: (9011,)\n",
      "xtest: (9012, 59), ytest: (9012,)\n",
      "\n",
      "classification_report_Fold=5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.98      0.99      8725\n",
      " Anomalous 1       0.58      0.82      0.68       287\n",
      "\n",
      "    accuracy                           0.98      9012\n",
      "   macro avg       0.79      0.90      0.83      9012\n",
      "weighted avg       0.98      0.98      0.98      9012\n",
      "\n",
      "confusion_matrix_Fold=5:\n",
      "\n",
      "True Negatives:  8558\n",
      "False Positives:  167\n",
      "False Negatives:  53\n",
      "True Positives:  234\n",
      "accuracy_score_Fold=5:\n",
      " 8792 \n",
      "\n",
      "End running time Fold=5: 210214_111150 ,-------------------------- \n",
      "\n",
      "\n",
      "classification_report_AllFolds:\n",
      "            Normal 0  Anomalous 1\n",
      "precision      0.99         0.59\n",
      "recall         0.98         0.83\n",
      "f1-score       0.99         0.69\n",
      "End running time: 210214_111150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "\n",
    "###Start sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit,cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "### End sklearn\n",
    "\n",
    "###***Start tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.random.set_seed(1234)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "###**** End tensorflow.keras\n",
    "#sys.path.append(\"..\")\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "pathAug = \"/content/drive/MyDrive/MasterThesis_Files/mainCodes/augmentation/\"\n",
    "#pathData =\"/content/drive/MyDrive/MasterThesis_Files/mainCodes/data/forCrossValidation/forAugShifted5/\"\n",
    "pathData =\"/content/drive/MyDrive/MasterThesis_Files/mainCodes/data/forCrossValidation/Shifted5_Rol_Lbl1/\"\n",
    "\n",
    "\n",
    "sys.path.insert(0,pathAug)\n",
    "sys.path.insert(1,pathData)\n",
    "\n",
    "import preprocessRollingLabel2_NN as aug\n",
    "\n",
    "\n",
    "#####End Import Libraries\n",
    "\n",
    "\n",
    "############ Start Running codes\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"Main Start running time : {datestr}\")\n",
    "\n",
    "accPerFold = []\n",
    "lossPerFold = []\n",
    "dfPrReF1=pd.DataFrame()\n",
    "\n",
    "\n",
    "# dfActual = pd.read_csv(pathData+\"dfpShifted5_ForAug_201201_202734_AllTested_Correct_NT_NH.csv\",header=None)\n",
    "#dfActual = pd.read_csv(pathData+\"dfpShifted5ForAug_1To5_FromAllTrainTest_201204_192153.csv\",header=None)\n",
    "dfActual = pd.read_csv(pathData+\"dfpShifted5_Rolling_210110_191921.csv\",header=None)\n",
    "\n",
    "\n",
    "\n",
    "#dfActual=dfActual[:5000]\n",
    "\n",
    "yX=dfActual.values\n",
    "X = yX[:, 1:]  # converts the df to a numpy array\n",
    "y = yX[:, 0]\n",
    "\n",
    "print(f\"\\n Number of Actual labeled 0: {len(y[np.where(y==0)])}\")\n",
    "print(f\"Number of Actual labeled 1: {len(y[np.where(y==1)])}\")\n",
    "print(f\"Number of Actual labeled 2: {len(y[np.where(y==2)])} \\n\")\n",
    "\n",
    "\n",
    "dataSplitPCT=.3\n",
    "dataSplitValTestPCT=.5\n",
    "\n",
    "train_test_split_Shuffle=True\n",
    "flagFitShuffle = True#False\n",
    "flagSeed=True\n",
    "\n",
    "p1=\"\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)#5\n",
    "# skf = KFold(n_splits=5,shuffle=False)#5\n",
    "model=0\n",
    "\n",
    "for foldNum, (trainIndex, testIndex) in enumerate(skf.split(X,y),start=1):\n",
    "\n",
    "    #print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n",
    "\n",
    "    yXtrain, yXtest = yX[trainIndex], yX[testIndex]\n",
    "    #ytrain, ytest = y[trainIndex], y[testIndex]\n",
    "\n",
    "    AugedNN=aug.GenerateAug_NN_Rolling(yXtrain,foldNum,flagLbl2=False,jitterNum4Lbl1=7,jitterNum4Lbl2=3)###***Generate synthetic data\n",
    "\n",
    "    datestrfoldNum = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"\\n Start running time Fold={foldNum}: {datestrfoldNum} ,-------------------------- \\n\")\n",
    "\n",
    "    Actual_AugedNN=np.concatenate((yXtrain,AugedNN),axis=0)\n",
    "    #yXtrain=Actual_AugedNN\n",
    "\n",
    "    yXtrain1, yXtrain2 = train_test_split(Actual_AugedNN, shuffle=train_test_split_Shuffle,\n",
    "                                                          test_size=dataSplitPCT, random_state=42,\n",
    "                                                          stratify=Actual_AugedNN[:,0])  # stratify=input_y\n",
    "\n",
    "    yXtrain = np.concatenate((yXtrain1, yXtrain2), axis=0)\n",
    "\n",
    "    yXvalid, yXtest = train_test_split(yXtest, shuffle=train_test_split_Shuffle,\n",
    "                                          test_size=dataSplitValTestPCT, random_state=42,\n",
    "                                          stratify=yXtest[:, 0])  # stratify=input_y\n",
    "\n",
    "    print(f\"\\n Number of Final yXtrain_Fold={foldNum} labeled 0: {len(yXtrain[np.where(yXtrain[:, 0] == 0)])}\")\n",
    "    print(f\"Number of Final yXtrain_Fold={foldNum} labeled 1: {len(yXtrain[np.where(yXtrain[:, 0] == 1)])}\")\n",
    "    print(f\"Number of Final yXtrain_Fold={foldNum} labeled 2: {len(yXtrain[np.where(yXtrain[:, 0] == 2)])} \\n\")\n",
    "\n",
    "    ytrain = yXtrain  [:,0]\n",
    "    yvalid = yXvalid  [:,0]\n",
    "    ytest  = yXtest   [:,0]\n",
    "\n",
    "    xtrain = yXtrain  [:,1:]\n",
    "    xvalid = yXvalid  [:,1:]\n",
    "    xtest  = yXtest   [:,1:]\n",
    "\n",
    "\n",
    "    neurons = xtrain.shape[1]\n",
    "\n",
    "    epochs = 1000#50#40#60#30#30# 150  # 0  # 100#300#60#300#10#200#00#150\n",
    "    batch = 512#32\n",
    "    lr = 0.0001\n",
    "\n",
    "    #flagR1 = True\n",
    "    flagR1=False\n",
    "    r1 = .1\n",
    "    r2 = .1\n",
    "    d1 = .2\n",
    "\n",
    "    if foldNum==1:\n",
    "        print(\"\\n Hyperparameters:\")\n",
    "        print(f\"epochs: {epochs}, batch: {batch}, lr: {lr}, neurons: {neurons}, flagFitShuffle: {flagFitShuffle} , train_test_split_Shuffle: {train_test_split_Shuffle}, flagSeed: {flagSeed}\\n \")\n",
    "\n",
    "        print(f\"\\n xtrain: {np.shape(xtrain)}, ytrain: {np.shape(ytrain)}\")\n",
    "        print(f\"xvalid: {np.shape(xvalid)}, yvalid: {np.shape(yvalid)}\")\n",
    "        print(f\"xtest: {np.shape(xtest)}, ytest: {np.shape(ytest)} \\n\")\n",
    "\n",
    "        #p1 = os.path.join(str(pathCurrent.parent.parent), \"Results\", \"Results_001_class_oppys\", \"bestModels\", \"\")\n",
    "\n",
    "        # pathSavingPlotsPerRunning = pathSavingPlots + datestr #+ \"_\" + modelname\n",
    "        # if not os.path.exists(pathSavingPlotsPerRunning):\n",
    "        #     os.makedirs(pathSavingPlotsPerRunning)\n",
    "\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(177, activation='tanh', input_dim=xtrain.shape[1]\n",
    "                    #,kernel_regularizer = l1(r1) if flagR1 else l2(r2),\n",
    "                    ))  # , input_dim=xtrain.shape[1]\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(150, activation='tanh',\n",
    "                   #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "                   #kernel_regularizer=l1_l2(l1=r1, l2=r2),\n",
    "                    # bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "                    # activity_regularizer=l1(r2)\n",
    "                    ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(90, activation='tanh',\n",
    "                    #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "                    #kernel_regularizer=l1_l2(l1=r1, l2=r2)\n",
    "                    # bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer = l1(r1) if flagR1 else l2(r2)\n",
    "                    # activity_regularizer=l2(r2)\n",
    "                    ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    # model.add(Dense(32, activation='tanh',\n",
    "    #                #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "    #                 # bias_regularizer=l1(r2),\n",
    "    #                 #activity_regularizer = l1(r1) if flagR1 else l2(r2)\n",
    "    #                 # activity_regularizer=l2(r2)\n",
    "    #                 ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    # model.add(Dense(16, activation='tanh',\n",
    "    #                kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "    #                #kernel_regularizer=l1_l2(l1=r1, l2=r2),\n",
    "    # #                 # bias_regularizer=l1(r2),\n",
    "    #                 #activity_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "    # #                 # activity_regularizer=l2(r2)\n",
    "    #                 ))\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr)#lr\n",
    "    # cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",save_best_only=True,verbose=0)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "    if foldNum == 1:\n",
    "        print(\"\\n Final test model.summary(): \\n\")\n",
    "        print(model.summary())\n",
    "\n",
    "        print(f\"\\n model.get_config: {str(model.get_config())}\")\n",
    "\n",
    "    # fit model\n",
    "    history1 = model.fit(xtrain, ytrain, batch_size=batch, epochs=epochs\n",
    "                         , validation_data=(xvalid, yvalid)\n",
    "                         , verbose=1, use_multiprocessing=True,\n",
    "                         shuffle=flagFitShuffle).history  # ,shuffle=True#,callbacks=[es]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history1['loss'], linewidth=2, label='Train')  # OR accuracy\n",
    "    plt.plot(history1['val_loss'], linewidth=2, label='Validation')  # OR val_accuracy\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.13, 1.13))\n",
    "    plt.title(f'Model loss Fold={foldNum}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,.2)\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"loss&val_loss_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    yPred = model.predict(xtest, verbose=1)\n",
    "\n",
    "    l = []\n",
    "    for i in yPred:\n",
    "        if i < .5:\n",
    "            l.append(0)\n",
    "        else:\n",
    "            l.append(1)\n",
    "\n",
    "    yPred = l\n",
    "\n",
    "    print(f\"xtrain: {np.shape(xtrain)}, ytrain: {np.shape(ytrain)}\")\n",
    "    print(f\"xvalid: {np.shape(xvalid)}, yvalid: {np.shape(yvalid)}\")\n",
    "    print(f\"xtest: {np.shape(xtest)}, ytest: {np.shape(ytest)}\")\n",
    "\n",
    "\n",
    "    target_names = ['Normal 0', 'Anomalous 1']\n",
    "    print(f\"\\nclassification_report_Fold={foldNum}:\")\n",
    "    print(classification_report(ytest, yPred, target_names=target_names))\n",
    "\n",
    "    cr = pd.DataFrame(classification_report(ytest, yPred, target_names=target_names,output_dict=True))\n",
    "    dfPrReF1 = dfPrReF1.append(cr.iloc[:3, :2])\n",
    "\n",
    "    print(f\"confusion_matrix_Fold={foldNum}:\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(ytest, yPred, labels=[0, 1]).ravel()\n",
    "    print(\"True Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "\n",
    "\n",
    "    print(f\"accuracy_score_Fold={foldNum}:\\n {accuracy_score(ytest, yPred, normalize=False)} \\n\")\n",
    "\n",
    "    # Predicting test images\n",
    "    # preds = np.where(yPred < 0.5, 0, 1)\n",
    "\n",
    "    # mlbClasses = [0, 1, 2]\n",
    "    # # Plot confusion matrix\n",
    "    # plt.figure(figsize=(14, 8))\n",
    "    # for j, (label, matrix) in enumerate(zip(mlbClasses, mlbConfusion)):\n",
    "    #     plt.subplot(f'23{j + 1}')\n",
    "    #     labels = [f'Not_{label}', label]\n",
    "    #     sns.heatmap(matrix, annot=True, square=True, fmt='d', cbar=False, cmap='Blues',\n",
    "    #                 cbar_kws={'label': 'My Colorbar'},  # , fmt = 'd'\n",
    "    #                 xticklabels=labels, yticklabels=labels, linecolor='black', linewidth=1)\n",
    "    #\n",
    "    #     plt.ylabel('Actual class')\n",
    "    #     plt.xlabel(f'Predicted class_Fold={foldNum}')\n",
    "    #     plt.title(labels[0])\n",
    "    #\n",
    "    # plt.tight_layout()\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"ConfusionMatrix_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    #plt.show()\n",
    "\n",
    "    datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"End running time Fold={foldNum}: {datestr} ,-------------------------- \\n\")\n",
    "\n",
    "dfPrReF1=pd.DataFrame([np.round(dfPrReF1[dfPrReF1.index=='precision'].mean(),2),\n",
    "                       np.round(dfPrReF1[dfPrReF1.index=='recall'].mean(),2),np.round(dfPrReF1[dfPrReF1.index=='f1-score'].mean(),2)],\n",
    "                      index=['precision','recall','f1-score'])\n",
    "\n",
    "print(f\"\\nclassification_report_AllFolds:\\n {dfPrReF1}\")\n",
    "\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"End running time: {datestr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytjrnQfmjjrg"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etm5OKtgw-8L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89vp3n4lw-_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glnQYPpkMPIW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCzD224rMX-E"
   },
   "source": [
    "# **Do Rolling dataset Shifted5 In Cross-validation, so Final test fold just has shifted5 data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qenRZXXYMPMz",
    "outputId": "c7a8c732-9d1d-4bb4-b477-804e6148ff88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Start running time : 210217_003405\n",
      "Main Start running time : 210217_003407\n",
      "\n",
      "   Number of Actual labeled 0: 17453\n",
      "   Number of Actual labeled 1: 545\n",
      "   Number of Actual labeled 2: 0 \n",
      "\n",
      "Start running time Data Augmentation_Fold=1: 210217_003408 ,-------------------------- \n",
      "\n",
      "\n",
      " Data Augmentation Hyperparameters:\n",
      "epochs: 2000, batch: 16, lr: 0.0001, neurons1: 177, neurons2: 150, flagFitShuffle: True \n",
      " \n",
      "\n",
      " Hyperparameters:\n",
      "\n",
      " Data Augmentation model.summary(): \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 177)               10620     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               26700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                13590     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 59)                5369      \n",
      "=================================================================\n",
      "Total params: 56,279\n",
      "Trainable params: 56,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Data Augmentation MSE Label1, iter2==> mean: 9.357488890509633e-06, min: 2.4755230307849147e-06, max: 0.0003564895520509285\n",
      "End running time Data Augmentation_Fold=1: 210217_010429 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=1: 210217_010429 ,-------------------------- \n",
      "\n",
      "\n",
      "   Number of Final yXtrain_Fold=1 labeled 0: 69790\n",
      "   Number of Final yXtrain_Fold=1 labeled 1: 15260\n",
      "   Number of Final yXtrain_Fold=1 labeled 2: 0 \n",
      "\n",
      "\n",
      " Hyperparameters:\n",
      "epochs: 100, batch: 256, lr: 0.001, neurons: 59, flagFitShuffle: True , train_test_split_Shuffle: True, flagSeed: True\n",
      " \n",
      "\n",
      "  xtrain: (85050, 59), ytrain: (85050,)\n",
      "    xvalid: (1800, 59), yvalid: (1800,)\n",
      "    xtest:  (1800, 59),  ytest:  (1800,) \n",
      "\n",
      "\n",
      " Final test model.summary(): \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 177)               10620     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               26700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                13590     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 91        \n",
      "=================================================================\n",
      "Total params: 51,001\n",
      "Trainable params: 51,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " model.get_config: {'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 59), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_input'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 59), 'dtype': 'float32', 'units': 177, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 150, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 90, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
      "Epoch 1/100\n",
      "333/333 [==============================] - 1s 3ms/step - loss: 0.4634 - accuracy: 0.8084 - val_loss: 0.1835 - val_accuracy: 0.9617\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3818 - accuracy: 0.8383 - val_loss: 0.2357 - val_accuracy: 0.9394\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8509 - val_loss: 0.1825 - val_accuracy: 0.9439\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2789 - accuracy: 0.8722 - val_loss: 0.2430 - val_accuracy: 0.8911\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2379 - accuracy: 0.8911 - val_loss: 0.1816 - val_accuracy: 0.9150\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9122 - val_loss: 0.1283 - val_accuracy: 0.9439\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9245 - val_loss: 0.1455 - val_accuracy: 0.9400\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1555 - accuracy: 0.9346 - val_loss: 0.1023 - val_accuracy: 0.9533\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1461 - accuracy: 0.9388 - val_loss: 0.1057 - val_accuracy: 0.9511\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1352 - accuracy: 0.9438 - val_loss: 0.1200 - val_accuracy: 0.9444\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9459 - val_loss: 0.1189 - val_accuracy: 0.9500\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9504 - val_loss: 0.1279 - val_accuracy: 0.9417\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9513 - val_loss: 0.1063 - val_accuracy: 0.9506\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1144 - accuracy: 0.9528 - val_loss: 0.1032 - val_accuracy: 0.9528\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1124 - accuracy: 0.9554 - val_loss: 0.0893 - val_accuracy: 0.9600\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1081 - accuracy: 0.9560 - val_loss: 0.0945 - val_accuracy: 0.9583\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1076 - accuracy: 0.9578 - val_loss: 0.0843 - val_accuracy: 0.9611\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1022 - accuracy: 0.9600 - val_loss: 0.1124 - val_accuracy: 0.9522\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0994 - accuracy: 0.9613 - val_loss: 0.1188 - val_accuracy: 0.9522\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1026 - accuracy: 0.9595 - val_loss: 0.1088 - val_accuracy: 0.9506\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9629 - val_loss: 0.1038 - val_accuracy: 0.9556\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0964 - accuracy: 0.9626 - val_loss: 0.0934 - val_accuracy: 0.9572\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.9635 - val_loss: 0.0826 - val_accuracy: 0.9617\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.9637 - val_loss: 0.0928 - val_accuracy: 0.9589\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.9640 - val_loss: 0.0880 - val_accuracy: 0.9611\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0928 - accuracy: 0.9634 - val_loss: 0.1040 - val_accuracy: 0.9606\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0926 - accuracy: 0.9641 - val_loss: 0.0939 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0915 - accuracy: 0.9646 - val_loss: 0.0989 - val_accuracy: 0.9572\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0901 - accuracy: 0.9656 - val_loss: 0.1018 - val_accuracy: 0.9567\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9654 - val_loss: 0.0788 - val_accuracy: 0.9656\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9652 - val_loss: 0.0956 - val_accuracy: 0.9556\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.9645 - val_loss: 0.0839 - val_accuracy: 0.9617\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9681 - val_loss: 0.0880 - val_accuracy: 0.9622\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9660 - val_loss: 0.0902 - val_accuracy: 0.9639\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9655 - val_loss: 0.0875 - val_accuracy: 0.9611\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9676 - val_loss: 0.0790 - val_accuracy: 0.9628\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9676 - val_loss: 0.0872 - val_accuracy: 0.9606\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9654 - val_loss: 0.0964 - val_accuracy: 0.9561\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0849 - accuracy: 0.9674 - val_loss: 0.0896 - val_accuracy: 0.9633\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9684 - val_loss: 0.0838 - val_accuracy: 0.9611\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.9669 - val_loss: 0.0808 - val_accuracy: 0.9650\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0829 - accuracy: 0.9674 - val_loss: 0.0840 - val_accuracy: 0.9650\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0823 - accuracy: 0.9682 - val_loss: 0.0948 - val_accuracy: 0.9600\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0820 - accuracy: 0.9684 - val_loss: 0.0843 - val_accuracy: 0.9628\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9694 - val_loss: 0.0923 - val_accuracy: 0.9611\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.9672 - val_loss: 0.0898 - val_accuracy: 0.9633\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0811 - accuracy: 0.9695 - val_loss: 0.0852 - val_accuracy: 0.9656\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9696 - val_loss: 0.0920 - val_accuracy: 0.9622\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9697 - val_loss: 0.0927 - val_accuracy: 0.9622\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9694 - val_loss: 0.0880 - val_accuracy: 0.9617\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9699 - val_loss: 0.0870 - val_accuracy: 0.9589\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.9698 - val_loss: 0.0815 - val_accuracy: 0.9667\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0820 - accuracy: 0.9682 - val_loss: 0.0876 - val_accuracy: 0.9611\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0825 - accuracy: 0.9680 - val_loss: 0.0844 - val_accuracy: 0.9644\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.9705 - val_loss: 0.0988 - val_accuracy: 0.9583\n",
      "Epoch 56/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.9691 - val_loss: 0.0828 - val_accuracy: 0.9611\n",
      "Epoch 57/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9715 - val_loss: 0.0789 - val_accuracy: 0.9678\n",
      "Epoch 58/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.9696 - val_loss: 0.0802 - val_accuracy: 0.9611\n",
      "Epoch 59/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9699 - val_loss: 0.0781 - val_accuracy: 0.9628\n",
      "Epoch 60/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.9712 - val_loss: 0.0870 - val_accuracy: 0.9622\n",
      "Epoch 61/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0781 - accuracy: 0.9704 - val_loss: 0.0848 - val_accuracy: 0.9606\n",
      "Epoch 62/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0803 - accuracy: 0.9685 - val_loss: 0.0779 - val_accuracy: 0.9661\n",
      "Epoch 63/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.9701 - val_loss: 0.0827 - val_accuracy: 0.9678\n",
      "Epoch 64/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9693 - val_loss: 0.0962 - val_accuracy: 0.9589\n",
      "Epoch 65/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9689 - val_loss: 0.0859 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.9704 - val_loss: 0.0944 - val_accuracy: 0.9572\n",
      "Epoch 67/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9712 - val_loss: 0.0844 - val_accuracy: 0.9639\n",
      "Epoch 68/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.9706 - val_loss: 0.0942 - val_accuracy: 0.9589\n",
      "Epoch 69/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9719 - val_loss: 0.0714 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0781 - accuracy: 0.9706 - val_loss: 0.0830 - val_accuracy: 0.9661\n",
      "Epoch 71/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.9714 - val_loss: 0.0824 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.9710 - val_loss: 0.0829 - val_accuracy: 0.9644\n",
      "Epoch 73/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9716 - val_loss: 0.0819 - val_accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9705 - val_loss: 0.0759 - val_accuracy: 0.9672\n",
      "Epoch 75/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0760 - accuracy: 0.9714 - val_loss: 0.0863 - val_accuracy: 0.9622\n",
      "Epoch 76/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9718 - val_loss: 0.0875 - val_accuracy: 0.9633\n",
      "Epoch 77/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0750 - accuracy: 0.9714 - val_loss: 0.0855 - val_accuracy: 0.9644\n",
      "Epoch 78/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9718 - val_loss: 0.0897 - val_accuracy: 0.9622\n",
      "Epoch 79/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.9706 - val_loss: 0.0853 - val_accuracy: 0.9617\n",
      "Epoch 80/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9720 - val_loss: 0.0875 - val_accuracy: 0.9644\n",
      "Epoch 81/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9710 - val_loss: 0.0851 - val_accuracy: 0.9644\n",
      "Epoch 82/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9718 - val_loss: 0.0900 - val_accuracy: 0.9578\n",
      "Epoch 83/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9718 - val_loss: 0.0779 - val_accuracy: 0.9672\n",
      "Epoch 84/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9704 - val_loss: 0.0887 - val_accuracy: 0.9583\n",
      "Epoch 85/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9733 - val_loss: 0.0985 - val_accuracy: 0.9606\n",
      "Epoch 86/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0717 - accuracy: 0.9729 - val_loss: 0.0827 - val_accuracy: 0.9661\n",
      "Epoch 87/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9715 - val_loss: 0.0812 - val_accuracy: 0.9672\n",
      "Epoch 88/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9718 - val_loss: 0.0828 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9709 - val_loss: 0.0788 - val_accuracy: 0.9667\n",
      "Epoch 90/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9713 - val_loss: 0.0825 - val_accuracy: 0.9650\n",
      "Epoch 91/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0728 - accuracy: 0.9728 - val_loss: 0.0991 - val_accuracy: 0.9578\n",
      "Epoch 92/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9726 - val_loss: 0.0817 - val_accuracy: 0.9656\n",
      "Epoch 93/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9719 - val_loss: 0.0821 - val_accuracy: 0.9633\n",
      "Epoch 94/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9730 - val_loss: 0.0766 - val_accuracy: 0.9694\n",
      "Epoch 95/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9717 - val_loss: 0.0806 - val_accuracy: 0.9644\n",
      "Epoch 96/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9731 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
      "Epoch 97/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0712 - accuracy: 0.9730 - val_loss: 0.0880 - val_accuracy: 0.9622\n",
      "Epoch 98/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9727 - val_loss: 0.0970 - val_accuracy: 0.9556\n",
      "Epoch 99/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9720 - val_loss: 0.0845 - val_accuracy: 0.9639\n",
      "Epoch 100/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.9718 - val_loss: 0.0861 - val_accuracy: 0.9628\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 826us/step\n",
      "xtrain: (85050, 59), ytrain: (85050,)\n",
      "xvalid: (1800, 59), yvalid: (1800,)\n",
      "xtest: (1800, 59), ytest: (1800,)\n",
      "\n",
      "classification_report_Fold=1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.98      0.98      1746\n",
      " Anomalous 1       0.51      0.70      0.59        54\n",
      "\n",
      "    accuracy                           0.97      1800\n",
      "   macro avg       0.75      0.84      0.79      1800\n",
      "weighted avg       0.98      0.97      0.97      1800\n",
      "\n",
      "confusion_matrix_Fold=1:\n",
      "\n",
      "True Negatives:  1709\n",
      "False Positives:  37\n",
      "False Negatives:  16\n",
      "True Positives:  38\n",
      "accuracy_score_Fold=1:\n",
      " 1747 \n",
      "\n",
      "End running time Fold=1: 210217_010533 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=2: 210217_010533 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 7.1304859888372185e-06, min: 2.158160447286563e-06, max: 0.00013703806422061908\n",
      "End running time Data Augmentation_Fold=2: 210217_013609 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=2: 210217_013609 ,-------------------------- \n",
      "\n",
      "\n",
      "   Number of Final yXtrain_Fold=2 labeled 0: 69790\n",
      "   Number of Final yXtrain_Fold=2 labeled 1: 15260\n",
      "   Number of Final yXtrain_Fold=2 labeled 2: 0 \n",
      "\n",
      "Epoch 1/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.4449 - accuracy: 0.8168 - val_loss: 0.1762 - val_accuracy: 0.9667\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 0.8404 - val_loss: 0.2134 - val_accuracy: 0.9356\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3103 - accuracy: 0.8576 - val_loss: 0.1686 - val_accuracy: 0.9356\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2592 - accuracy: 0.8815 - val_loss: 0.1918 - val_accuracy: 0.9167\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2267 - accuracy: 0.8963 - val_loss: 0.1377 - val_accuracy: 0.9417\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1877 - accuracy: 0.9175 - val_loss: 0.1248 - val_accuracy: 0.9533\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.9269 - val_loss: 0.1165 - val_accuracy: 0.9544\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1557 - accuracy: 0.9344 - val_loss: 0.1094 - val_accuracy: 0.9561\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1387 - accuracy: 0.9428 - val_loss: 0.0893 - val_accuracy: 0.9656\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1342 - accuracy: 0.9438 - val_loss: 0.1101 - val_accuracy: 0.9522\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9484 - val_loss: 0.1090 - val_accuracy: 0.9494\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9529 - val_loss: 0.1146 - val_accuracy: 0.9528\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1154 - accuracy: 0.9542 - val_loss: 0.1103 - val_accuracy: 0.9522\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1157 - accuracy: 0.9532 - val_loss: 0.1081 - val_accuracy: 0.9556\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9575 - val_loss: 0.1156 - val_accuracy: 0.9500\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1092 - accuracy: 0.9568 - val_loss: 0.1007 - val_accuracy: 0.9611\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9588 - val_loss: 0.0920 - val_accuracy: 0.9583\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1023 - accuracy: 0.9594 - val_loss: 0.1007 - val_accuracy: 0.9600\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.9619 - val_loss: 0.1040 - val_accuracy: 0.9550\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0971 - accuracy: 0.9615 - val_loss: 0.0956 - val_accuracy: 0.9600\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0981 - accuracy: 0.9621 - val_loss: 0.1043 - val_accuracy: 0.9572\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0979 - accuracy: 0.9617 - val_loss: 0.1068 - val_accuracy: 0.9600\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.9623 - val_loss: 0.0930 - val_accuracy: 0.9622\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0917 - accuracy: 0.9652 - val_loss: 0.1033 - val_accuracy: 0.9611\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0936 - accuracy: 0.9637 - val_loss: 0.1059 - val_accuracy: 0.9578\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.9635 - val_loss: 0.1068 - val_accuracy: 0.9572\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0906 - accuracy: 0.9652 - val_loss: 0.1093 - val_accuracy: 0.9600\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0897 - accuracy: 0.9658 - val_loss: 0.1147 - val_accuracy: 0.9594\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0922 - accuracy: 0.9646 - val_loss: 0.1071 - val_accuracy: 0.9589\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9677 - val_loss: 0.0942 - val_accuracy: 0.9644\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0886 - accuracy: 0.9669 - val_loss: 0.0978 - val_accuracy: 0.9611\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0888 - accuracy: 0.9652 - val_loss: 0.0967 - val_accuracy: 0.9628\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9674 - val_loss: 0.1152 - val_accuracy: 0.9589\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0866 - accuracy: 0.9664 - val_loss: 0.1151 - val_accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9656 - val_loss: 0.1172 - val_accuracy: 0.9556\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.9656 - val_loss: 0.1087 - val_accuracy: 0.9583\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.9668 - val_loss: 0.1044 - val_accuracy: 0.9600\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9641 - val_loss: 0.0932 - val_accuracy: 0.9617\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.9685 - val_loss: 0.1087 - val_accuracy: 0.9606\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9701 - val_loss: 0.1006 - val_accuracy: 0.9606\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.9672 - val_loss: 0.1104 - val_accuracy: 0.9594\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0823 - accuracy: 0.9691 - val_loss: 0.0982 - val_accuracy: 0.9589\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9667 - val_loss: 0.1114 - val_accuracy: 0.9606\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0813 - accuracy: 0.9702 - val_loss: 0.1006 - val_accuracy: 0.9639\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0824 - accuracy: 0.9686 - val_loss: 0.1057 - val_accuracy: 0.9611\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0831 - accuracy: 0.9688 - val_loss: 0.0944 - val_accuracy: 0.9622\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9696 - val_loss: 0.1040 - val_accuracy: 0.9583\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.9694 - val_loss: 0.1001 - val_accuracy: 0.9628\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.9699 - val_loss: 0.0995 - val_accuracy: 0.9628\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9698 - val_loss: 0.0898 - val_accuracy: 0.9656\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9698 - val_loss: 0.1046 - val_accuracy: 0.9594\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9686 - val_loss: 0.1038 - val_accuracy: 0.9600\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9696 - val_loss: 0.0933 - val_accuracy: 0.9644\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0797 - accuracy: 0.9694 - val_loss: 0.0918 - val_accuracy: 0.9683\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0820 - accuracy: 0.9692 - val_loss: 0.0995 - val_accuracy: 0.9594\n",
      "Epoch 56/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.9707 - val_loss: 0.0976 - val_accuracy: 0.9644\n",
      "Epoch 57/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0808 - accuracy: 0.9693 - val_loss: 0.0995 - val_accuracy: 0.9633\n",
      "Epoch 58/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9696 - val_loss: 0.1005 - val_accuracy: 0.9622\n",
      "Epoch 59/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9702 - val_loss: 0.1002 - val_accuracy: 0.9628\n",
      "Epoch 60/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9703 - val_loss: 0.1043 - val_accuracy: 0.9606\n",
      "Epoch 61/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.9705 - val_loss: 0.1010 - val_accuracy: 0.9606\n",
      "Epoch 62/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.9708 - val_loss: 0.0944 - val_accuracy: 0.9656\n",
      "Epoch 63/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9695 - val_loss: 0.1034 - val_accuracy: 0.9600\n",
      "Epoch 64/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.9709 - val_loss: 0.0991 - val_accuracy: 0.9650\n",
      "Epoch 65/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0781 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9594\n",
      "Epoch 66/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.9714 - val_loss: 0.1016 - val_accuracy: 0.9656\n",
      "Epoch 67/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9711 - val_loss: 0.1033 - val_accuracy: 0.9600\n",
      "Epoch 68/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9713 - val_loss: 0.1015 - val_accuracy: 0.9600\n",
      "Epoch 69/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.9711 - val_loss: 0.1146 - val_accuracy: 0.9589\n",
      "Epoch 70/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9713 - val_loss: 0.1080 - val_accuracy: 0.9572\n",
      "Epoch 71/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9698 - val_loss: 0.1088 - val_accuracy: 0.9628\n",
      "Epoch 72/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9695 - val_loss: 0.0995 - val_accuracy: 0.9644\n",
      "Epoch 73/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9715 - val_loss: 0.1070 - val_accuracy: 0.9639\n",
      "Epoch 74/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.9705 - val_loss: 0.1088 - val_accuracy: 0.9611\n",
      "Epoch 75/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9711 - val_loss: 0.1076 - val_accuracy: 0.9633\n",
      "Epoch 76/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0750 - accuracy: 0.9717 - val_loss: 0.1094 - val_accuracy: 0.9611\n",
      "Epoch 77/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.9698 - val_loss: 0.1036 - val_accuracy: 0.9639\n",
      "Epoch 78/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.9707 - val_loss: 0.1133 - val_accuracy: 0.9611\n",
      "Epoch 79/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.9719 - val_loss: 0.1148 - val_accuracy: 0.9572\n",
      "Epoch 80/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9712 - val_loss: 0.1033 - val_accuracy: 0.9611\n",
      "Epoch 81/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9707 - val_loss: 0.1006 - val_accuracy: 0.9622\n",
      "Epoch 82/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9718 - val_loss: 0.1092 - val_accuracy: 0.9589\n",
      "Epoch 83/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9717 - val_loss: 0.1119 - val_accuracy: 0.9606\n",
      "Epoch 84/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.9708 - val_loss: 0.1037 - val_accuracy: 0.9622\n",
      "Epoch 85/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9710 - val_loss: 0.0997 - val_accuracy: 0.9644\n",
      "Epoch 86/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0758 - accuracy: 0.9713 - val_loss: 0.1036 - val_accuracy: 0.9622\n",
      "Epoch 87/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9718 - val_loss: 0.0986 - val_accuracy: 0.9644\n",
      "Epoch 88/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0750 - accuracy: 0.9713 - val_loss: 0.1024 - val_accuracy: 0.9617\n",
      "Epoch 89/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9716 - val_loss: 0.0994 - val_accuracy: 0.9617\n",
      "Epoch 90/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9707 - val_loss: 0.1059 - val_accuracy: 0.9644\n",
      "Epoch 91/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.9709 - val_loss: 0.1015 - val_accuracy: 0.9622\n",
      "Epoch 92/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0758 - accuracy: 0.9720 - val_loss: 0.1036 - val_accuracy: 0.9617\n",
      "Epoch 93/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9713 - val_loss: 0.1063 - val_accuracy: 0.9617\n",
      "Epoch 94/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0739 - accuracy: 0.9725 - val_loss: 0.1102 - val_accuracy: 0.9606\n",
      "Epoch 95/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.9725 - val_loss: 0.1004 - val_accuracy: 0.9611\n",
      "Epoch 96/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.9712 - val_loss: 0.1051 - val_accuracy: 0.9622\n",
      "Epoch 97/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9728 - val_loss: 0.1075 - val_accuracy: 0.9594\n",
      "Epoch 98/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0711 - accuracy: 0.9734 - val_loss: 0.1182 - val_accuracy: 0.9583\n",
      "Epoch 99/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.9732 - val_loss: 0.1073 - val_accuracy: 0.9594\n",
      "Epoch 100/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9727 - val_loss: 0.1117 - val_accuracy: 0.9589\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 841us/step\n",
      "xtrain: (85050, 59), ytrain: (85050,)\n",
      "xvalid: (1800, 59), yvalid: (1800,)\n",
      "xtest: (1800, 59), ytest: (1800,)\n",
      "\n",
      "classification_report_Fold=2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.96      0.98      1746\n",
      " Anomalous 1       0.37      0.70      0.48        54\n",
      "\n",
      "    accuracy                           0.95      1800\n",
      "   macro avg       0.68      0.83      0.73      1800\n",
      "weighted avg       0.97      0.95      0.96      1800\n",
      "\n",
      "confusion_matrix_Fold=2:\n",
      "\n",
      "True Negatives:  1681\n",
      "False Positives:  65\n",
      "False Negatives:  16\n",
      "True Positives:  38\n",
      "accuracy_score_Fold=2:\n",
      " 1719 \n",
      "\n",
      "End running time Fold=2: 210217_013714 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=3: 210217_013714 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 9.533063689099176e-06, min: 3.0769456637868587e-06, max: 5.88359743071446e-05\n",
      "End running time Data Augmentation_Fold=3: 210217_020753 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=3: 210217_020753 ,-------------------------- \n",
      "\n",
      "\n",
      "   Number of Final yXtrain_Fold=3 labeled 0: 69790\n",
      "   Number of Final yXtrain_Fold=3 labeled 1: 15260\n",
      "   Number of Final yXtrain_Fold=3 labeled 2: 0 \n",
      "\n",
      "Epoch 1/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.4422 - accuracy: 0.8232 - val_loss: 0.1753 - val_accuracy: 0.9667\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3721 - accuracy: 0.8394 - val_loss: 0.2376 - val_accuracy: 0.9339\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8542 - val_loss: 0.1747 - val_accuracy: 0.9472\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2752 - accuracy: 0.8728 - val_loss: 0.1967 - val_accuracy: 0.9161\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2340 - accuracy: 0.8929 - val_loss: 0.1693 - val_accuracy: 0.9294\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9130 - val_loss: 0.1355 - val_accuracy: 0.9428\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9222 - val_loss: 0.1381 - val_accuracy: 0.9350\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1585 - accuracy: 0.9306 - val_loss: 0.1067 - val_accuracy: 0.9556\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1505 - accuracy: 0.9359 - val_loss: 0.1019 - val_accuracy: 0.9550\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1371 - accuracy: 0.9427 - val_loss: 0.1238 - val_accuracy: 0.9461\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1291 - accuracy: 0.9457 - val_loss: 0.1156 - val_accuracy: 0.9456\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1202 - accuracy: 0.9515 - val_loss: 0.1158 - val_accuracy: 0.9467\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9492 - val_loss: 0.0958 - val_accuracy: 0.9533\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1159 - accuracy: 0.9536 - val_loss: 0.1146 - val_accuracy: 0.9489\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1165 - accuracy: 0.9525 - val_loss: 0.1183 - val_accuracy: 0.9522\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.9571 - val_loss: 0.1145 - val_accuracy: 0.9483\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1075 - accuracy: 0.9579 - val_loss: 0.1105 - val_accuracy: 0.9489\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1041 - accuracy: 0.9590 - val_loss: 0.1322 - val_accuracy: 0.9422\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.9591 - val_loss: 0.1207 - val_accuracy: 0.9450\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1015 - accuracy: 0.9601 - val_loss: 0.1250 - val_accuracy: 0.9461\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1019 - accuracy: 0.9606 - val_loss: 0.0991 - val_accuracy: 0.9567\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0975 - accuracy: 0.9624 - val_loss: 0.1156 - val_accuracy: 0.9461\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0932 - accuracy: 0.9638 - val_loss: 0.1092 - val_accuracy: 0.9533\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9621 - val_loss: 0.1194 - val_accuracy: 0.9439\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.9633 - val_loss: 0.1138 - val_accuracy: 0.9467\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9641 - val_loss: 0.1127 - val_accuracy: 0.9522\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.9636 - val_loss: 0.1154 - val_accuracy: 0.9478\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.1076 - val_accuracy: 0.9494\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9645 - val_loss: 0.1144 - val_accuracy: 0.9522\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0942 - accuracy: 0.9632 - val_loss: 0.1240 - val_accuracy: 0.9489\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9658 - val_loss: 0.1078 - val_accuracy: 0.9528\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9662 - val_loss: 0.1127 - val_accuracy: 0.9494\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9668 - val_loss: 0.1411 - val_accuracy: 0.9433\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.9652 - val_loss: 0.1108 - val_accuracy: 0.9517\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.9661 - val_loss: 0.1202 - val_accuracy: 0.9456\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.9672 - val_loss: 0.1175 - val_accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0865 - accuracy: 0.9672 - val_loss: 0.1070 - val_accuracy: 0.9511\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9666 - val_loss: 0.1041 - val_accuracy: 0.9528\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9657 - val_loss: 0.1166 - val_accuracy: 0.9500\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.9679 - val_loss: 0.1153 - val_accuracy: 0.9511\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9670 - val_loss: 0.1319 - val_accuracy: 0.9428\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0829 - accuracy: 0.9676 - val_loss: 0.1114 - val_accuracy: 0.9517\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9682 - val_loss: 0.1180 - val_accuracy: 0.9472\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9688 - val_loss: 0.1154 - val_accuracy: 0.9500\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0818 - accuracy: 0.9689 - val_loss: 0.1541 - val_accuracy: 0.9389\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9669 - val_loss: 0.1246 - val_accuracy: 0.9489\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9680 - val_loss: 0.1095 - val_accuracy: 0.9506\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.9675 - val_loss: 0.1134 - val_accuracy: 0.9511\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9689 - val_loss: 0.1124 - val_accuracy: 0.9494\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0813 - accuracy: 0.9686 - val_loss: 0.1060 - val_accuracy: 0.9539\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9705 - val_loss: 0.1108 - val_accuracy: 0.9539\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0815 - accuracy: 0.9688 - val_loss: 0.1075 - val_accuracy: 0.9506\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9693 - val_loss: 0.1105 - val_accuracy: 0.9528\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9668 - val_loss: 0.1055 - val_accuracy: 0.9561\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.9707 - val_loss: 0.1042 - val_accuracy: 0.9533\n",
      "Epoch 56/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0825 - accuracy: 0.9680 - val_loss: 0.1214 - val_accuracy: 0.9522\n",
      "Epoch 57/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9708 - val_loss: 0.1034 - val_accuracy: 0.9533\n",
      "Epoch 58/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9700 - val_loss: 0.1021 - val_accuracy: 0.9567\n",
      "Epoch 59/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9700 - val_loss: 0.1067 - val_accuracy: 0.9533\n",
      "Epoch 60/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.9710 - val_loss: 0.1038 - val_accuracy: 0.9550\n",
      "Epoch 61/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0803 - accuracy: 0.9697 - val_loss: 0.1064 - val_accuracy: 0.9511\n",
      "Epoch 62/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0810 - accuracy: 0.9689 - val_loss: 0.1124 - val_accuracy: 0.9511\n",
      "Epoch 63/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9692 - val_loss: 0.1248 - val_accuracy: 0.9483\n",
      "Epoch 64/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.9705 - val_loss: 0.1208 - val_accuracy: 0.9494\n",
      "Epoch 65/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.9693 - val_loss: 0.1060 - val_accuracy: 0.9561\n",
      "Epoch 66/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.9715 - val_loss: 0.1178 - val_accuracy: 0.9517\n",
      "Epoch 67/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9716 - val_loss: 0.1145 - val_accuracy: 0.9511\n",
      "Epoch 68/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9716 - val_loss: 0.1016 - val_accuracy: 0.9533\n",
      "Epoch 69/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9708 - val_loss: 0.1151 - val_accuracy: 0.9517\n",
      "Epoch 70/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.9702 - val_loss: 0.1081 - val_accuracy: 0.9544\n",
      "Epoch 71/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.9700 - val_loss: 0.1158 - val_accuracy: 0.9517\n",
      "Epoch 72/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9710 - val_loss: 0.1038 - val_accuracy: 0.9556\n",
      "Epoch 73/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0807 - accuracy: 0.9691 - val_loss: 0.1149 - val_accuracy: 0.9533\n",
      "Epoch 74/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.9725 - val_loss: 0.1066 - val_accuracy: 0.9556\n",
      "Epoch 75/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9717 - val_loss: 0.1162 - val_accuracy: 0.9506\n",
      "Epoch 76/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9719 - val_loss: 0.1194 - val_accuracy: 0.9533\n",
      "Epoch 77/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9706 - val_loss: 0.1078 - val_accuracy: 0.9539\n",
      "Epoch 78/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9718 - val_loss: 0.1088 - val_accuracy: 0.9517\n",
      "Epoch 79/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.9716 - val_loss: 0.1121 - val_accuracy: 0.9533\n",
      "Epoch 80/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.9706 - val_loss: 0.1067 - val_accuracy: 0.9517\n",
      "Epoch 81/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9730 - val_loss: 0.1036 - val_accuracy: 0.9528\n",
      "Epoch 82/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0759 - accuracy: 0.9717 - val_loss: 0.1167 - val_accuracy: 0.9533\n",
      "Epoch 83/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.9704 - val_loss: 0.1130 - val_accuracy: 0.9517\n",
      "Epoch 84/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9726 - val_loss: 0.1152 - val_accuracy: 0.9550\n",
      "Epoch 85/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0728 - accuracy: 0.9729 - val_loss: 0.1148 - val_accuracy: 0.9522\n",
      "Epoch 86/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9722 - val_loss: 0.1285 - val_accuracy: 0.9494\n",
      "Epoch 87/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9711 - val_loss: 0.0994 - val_accuracy: 0.9556\n",
      "Epoch 88/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9719 - val_loss: 0.1223 - val_accuracy: 0.9483\n",
      "Epoch 89/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.9727 - val_loss: 0.1242 - val_accuracy: 0.9478\n",
      "Epoch 90/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0811 - accuracy: 0.9694 - val_loss: 0.1156 - val_accuracy: 0.9500\n",
      "Epoch 91/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.9728 - val_loss: 0.1049 - val_accuracy: 0.9556\n",
      "Epoch 92/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.9733 - val_loss: 0.1109 - val_accuracy: 0.9517\n",
      "Epoch 93/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9711 - val_loss: 0.1133 - val_accuracy: 0.9506\n",
      "Epoch 94/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0718 - accuracy: 0.9731 - val_loss: 0.1097 - val_accuracy: 0.9544\n",
      "Epoch 95/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.9731 - val_loss: 0.1121 - val_accuracy: 0.9539\n",
      "Epoch 96/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9712 - val_loss: 0.1047 - val_accuracy: 0.9533\n",
      "Epoch 97/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.9724 - val_loss: 0.1210 - val_accuracy: 0.9550\n",
      "Epoch 98/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0749 - accuracy: 0.9715 - val_loss: 0.1045 - val_accuracy: 0.9544\n",
      "Epoch 99/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0717 - accuracy: 0.9738 - val_loss: 0.1023 - val_accuracy: 0.9567\n",
      "Epoch 100/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9712 - val_loss: 0.1147 - val_accuracy: 0.9506\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 831us/step\n",
      "xtrain: (85050, 59), ytrain: (85050,)\n",
      "xvalid: (1800, 59), yvalid: (1800,)\n",
      "xtest: (1800, 59), ytest: (1800,)\n",
      "\n",
      "classification_report_Fold=3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.97      0.98      1746\n",
      " Anomalous 1       0.42      0.67      0.51        54\n",
      "\n",
      "    accuracy                           0.96      1800\n",
      "   macro avg       0.70      0.82      0.75      1800\n",
      "weighted avg       0.97      0.96      0.97      1800\n",
      "\n",
      "confusion_matrix_Fold=3:\n",
      "\n",
      "True Negatives:  1696\n",
      "False Positives:  50\n",
      "False Negatives:  18\n",
      "True Positives:  36\n",
      "accuracy_score_Fold=3:\n",
      " 1732 \n",
      "\n",
      "End running time Fold=3: 210217_020859 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=4: 210217_020859 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 8.217186171033809e-06, min: 1.984343950375805e-06, max: 0.00011293498069292804\n",
      "End running time Data Augmentation_Fold=4: 210217_023945 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=4: 210217_023945 ,-------------------------- \n",
      "\n",
      "\n",
      "   Number of Final yXtrain_Fold=4 labeled 0: 69795\n",
      "   Number of Final yXtrain_Fold=4 labeled 1: 15260\n",
      "   Number of Final yXtrain_Fold=4 labeled 2: 0 \n",
      "\n",
      "Epoch 1/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.4320 - accuracy: 0.8274 - val_loss: 0.2191 - val_accuracy: 0.9461\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3636 - accuracy: 0.8440 - val_loss: 0.1798 - val_accuracy: 0.9405\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.8603 - val_loss: 0.1857 - val_accuracy: 0.9327\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2593 - accuracy: 0.8829 - val_loss: 0.1579 - val_accuracy: 0.9327\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9008 - val_loss: 0.1508 - val_accuracy: 0.9389\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9172 - val_loss: 0.1465 - val_accuracy: 0.9405\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9290 - val_loss: 0.1107 - val_accuracy: 0.9505\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1547 - accuracy: 0.9347 - val_loss: 0.1101 - val_accuracy: 0.9516\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1465 - accuracy: 0.9383 - val_loss: 0.1307 - val_accuracy: 0.9450\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1312 - accuracy: 0.9462 - val_loss: 0.1020 - val_accuracy: 0.9533\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1290 - accuracy: 0.9470 - val_loss: 0.1065 - val_accuracy: 0.9566\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1197 - accuracy: 0.9513 - val_loss: 0.1051 - val_accuracy: 0.9566\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1159 - accuracy: 0.9535 - val_loss: 0.1082 - val_accuracy: 0.9566\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1128 - accuracy: 0.9555 - val_loss: 0.0948 - val_accuracy: 0.9594\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1097 - accuracy: 0.9571 - val_loss: 0.1041 - val_accuracy: 0.9550\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1081 - accuracy: 0.9571 - val_loss: 0.1041 - val_accuracy: 0.9561\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.9579 - val_loss: 0.0934 - val_accuracy: 0.9600\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1036 - accuracy: 0.9603 - val_loss: 0.0943 - val_accuracy: 0.9616\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1026 - accuracy: 0.9603 - val_loss: 0.0912 - val_accuracy: 0.9616\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0989 - accuracy: 0.9620 - val_loss: 0.0832 - val_accuracy: 0.9633\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.9603 - val_loss: 0.1050 - val_accuracy: 0.9583\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.9605 - val_loss: 0.0962 - val_accuracy: 0.9605\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0978 - accuracy: 0.9622 - val_loss: 0.0823 - val_accuracy: 0.9655\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0954 - accuracy: 0.9630 - val_loss: 0.0838 - val_accuracy: 0.9655\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0934 - accuracy: 0.9639 - val_loss: 0.0877 - val_accuracy: 0.9628\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0937 - accuracy: 0.9639 - val_loss: 0.0941 - val_accuracy: 0.9611\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0945 - accuracy: 0.9640 - val_loss: 0.0849 - val_accuracy: 0.9666\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9670 - val_loss: 0.0957 - val_accuracy: 0.9611\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0912 - accuracy: 0.9653 - val_loss: 0.0887 - val_accuracy: 0.9628\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.9645 - val_loss: 0.0997 - val_accuracy: 0.9594\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0912 - accuracy: 0.9658 - val_loss: 0.0840 - val_accuracy: 0.9666\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.9648 - val_loss: 0.0968 - val_accuracy: 0.9605\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0910 - accuracy: 0.9653 - val_loss: 0.1033 - val_accuracy: 0.9600\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0850 - accuracy: 0.9675 - val_loss: 0.0870 - val_accuracy: 0.9655\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0871 - accuracy: 0.9671 - val_loss: 0.0970 - val_accuracy: 0.9622\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0875 - accuracy: 0.9670 - val_loss: 0.0812 - val_accuracy: 0.9633\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.9687 - val_loss: 0.0773 - val_accuracy: 0.9661\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9677 - val_loss: 0.0795 - val_accuracy: 0.9655\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.9670 - val_loss: 0.0855 - val_accuracy: 0.9611\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9687 - val_loss: 0.0833 - val_accuracy: 0.9622\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.9682 - val_loss: 0.0980 - val_accuracy: 0.9583\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0857 - accuracy: 0.9674 - val_loss: 0.0901 - val_accuracy: 0.9655\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9682 - val_loss: 0.0822 - val_accuracy: 0.9633\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.9677 - val_loss: 0.0868 - val_accuracy: 0.9666\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0809 - accuracy: 0.9701 - val_loss: 0.0914 - val_accuracy: 0.9628\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0815 - accuracy: 0.9699 - val_loss: 0.0891 - val_accuracy: 0.9639\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9685 - val_loss: 0.0859 - val_accuracy: 0.9644\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9692 - val_loss: 0.0883 - val_accuracy: 0.9650\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0837 - accuracy: 0.9682 - val_loss: 0.0906 - val_accuracy: 0.9633\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9697 - val_loss: 0.0797 - val_accuracy: 0.9666\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.9687 - val_loss: 0.0939 - val_accuracy: 0.9600\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.9680 - val_loss: 0.0928 - val_accuracy: 0.9622\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.9704 - val_loss: 0.0815 - val_accuracy: 0.9633\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0808 - accuracy: 0.9691 - val_loss: 0.0801 - val_accuracy: 0.9683\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9683 - val_loss: 0.0793 - val_accuracy: 0.9655\n",
      "Epoch 56/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0781 - accuracy: 0.9702 - val_loss: 0.0852 - val_accuracy: 0.9655\n",
      "Epoch 57/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9700 - val_loss: 0.0776 - val_accuracy: 0.9683\n",
      "Epoch 58/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9689 - val_loss: 0.0861 - val_accuracy: 0.9655\n",
      "Epoch 59/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.9694 - val_loss: 0.0870 - val_accuracy: 0.9639\n",
      "Epoch 60/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9701 - val_loss: 0.0944 - val_accuracy: 0.9616\n",
      "Epoch 61/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9709 - val_loss: 0.0808 - val_accuracy: 0.9672\n",
      "Epoch 62/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.9702 - val_loss: 0.0886 - val_accuracy: 0.9655\n",
      "Epoch 63/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0813 - accuracy: 0.9684 - val_loss: 0.0829 - val_accuracy: 0.9661\n",
      "Epoch 64/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.9701 - val_loss: 0.0789 - val_accuracy: 0.9678\n",
      "Epoch 65/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.9710 - val_loss: 0.0878 - val_accuracy: 0.9650\n",
      "Epoch 66/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9696 - val_loss: 0.0882 - val_accuracy: 0.9611\n",
      "Epoch 67/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9720 - val_loss: 0.0838 - val_accuracy: 0.9672\n",
      "Epoch 68/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9717 - val_loss: 0.0783 - val_accuracy: 0.9655\n",
      "Epoch 69/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.9702 - val_loss: 0.0781 - val_accuracy: 0.9678\n",
      "Epoch 70/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9717 - val_loss: 0.0876 - val_accuracy: 0.9616\n",
      "Epoch 71/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9720 - val_loss: 0.0871 - val_accuracy: 0.9639\n",
      "Epoch 72/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9707 - val_loss: 0.0807 - val_accuracy: 0.9678\n",
      "Epoch 73/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0807 - accuracy: 0.9698 - val_loss: 0.0810 - val_accuracy: 0.9666\n",
      "Epoch 74/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0797 - accuracy: 0.9696 - val_loss: 0.0899 - val_accuracy: 0.9622\n",
      "Epoch 75/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0734 - accuracy: 0.9721 - val_loss: 0.0867 - val_accuracy: 0.9644\n",
      "Epoch 76/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9713 - val_loss: 0.0870 - val_accuracy: 0.9655\n",
      "Epoch 77/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9716 - val_loss: 0.0882 - val_accuracy: 0.9644\n",
      "Epoch 78/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.9714 - val_loss: 0.0872 - val_accuracy: 0.9655\n",
      "Epoch 79/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9699 - val_loss: 0.0901 - val_accuracy: 0.9628\n",
      "Epoch 80/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.9708 - val_loss: 0.0859 - val_accuracy: 0.9644\n",
      "Epoch 81/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0740 - accuracy: 0.9719 - val_loss: 0.0794 - val_accuracy: 0.9650\n",
      "Epoch 82/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9720 - val_loss: 0.0855 - val_accuracy: 0.9644\n",
      "Epoch 83/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0739 - accuracy: 0.9722 - val_loss: 0.0812 - val_accuracy: 0.9661\n",
      "Epoch 84/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9725 - val_loss: 0.0887 - val_accuracy: 0.9650\n",
      "Epoch 85/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9725 - val_loss: 0.0882 - val_accuracy: 0.9633\n",
      "Epoch 86/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0741 - accuracy: 0.9721 - val_loss: 0.0829 - val_accuracy: 0.9639\n",
      "Epoch 87/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9699 - val_loss: 0.0852 - val_accuracy: 0.9661\n",
      "Epoch 88/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9723 - val_loss: 0.0780 - val_accuracy: 0.9661\n",
      "Epoch 89/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9718 - val_loss: 0.0858 - val_accuracy: 0.9666\n",
      "Epoch 90/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0734 - accuracy: 0.9722 - val_loss: 0.0885 - val_accuracy: 0.9644\n",
      "Epoch 91/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9715 - val_loss: 0.0823 - val_accuracy: 0.9655\n",
      "Epoch 92/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9722 - val_loss: 0.0935 - val_accuracy: 0.9616\n",
      "Epoch 93/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9722 - val_loss: 0.0951 - val_accuracy: 0.9616\n",
      "Epoch 94/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9717 - val_loss: 0.0862 - val_accuracy: 0.9655\n",
      "Epoch 95/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9717 - val_loss: 0.0829 - val_accuracy: 0.9650\n",
      "Epoch 96/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9726 - val_loss: 0.0766 - val_accuracy: 0.9661\n",
      "Epoch 97/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9725 - val_loss: 0.0905 - val_accuracy: 0.9650\n",
      "Epoch 98/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9716 - val_loss: 0.0943 - val_accuracy: 0.9622\n",
      "Epoch 99/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.9742 - val_loss: 0.0863 - val_accuracy: 0.9639\n",
      "Epoch 100/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9719 - val_loss: 0.0838 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 786us/step\n",
      "xtrain: (85055, 59), ytrain: (85055,)\n",
      "xvalid: (1799, 59), yvalid: (1799,)\n",
      "xtest: (1800, 59), ytest: (1800,)\n",
      "\n",
      "classification_report_Fold=4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.97      0.98      1745\n",
      " Anomalous 1       0.43      0.78      0.55        55\n",
      "\n",
      "    accuracy                           0.96      1800\n",
      "   macro avg       0.71      0.87      0.77      1800\n",
      "weighted avg       0.98      0.96      0.97      1800\n",
      "\n",
      "confusion_matrix_Fold=4:\n",
      "\n",
      "True Negatives:  1688\n",
      "False Positives:  57\n",
      "False Negatives:  12\n",
      "True Positives:  43\n",
      "accuracy_score_Fold=4:\n",
      " 1731 \n",
      "\n",
      "End running time Fold=4: 210217_024050 ,-------------------------- \n",
      "\n",
      "Start running time Data Augmentation_Fold=5: 210217_024050 ,-------------------------- \n",
      "\n",
      "Data Augmentation MSE Label1, iter2==> mean: 7.954899453025576e-06, min: 2.669506125611327e-06, max: 0.00015135569032788553\n",
      "End running time Data Augmentation_Fold=5: 210217_031133 ,-------------------------- \n",
      "\n",
      "\n",
      " Start running time Fold=5: 210217_031133 ,-------------------------- \n",
      "\n",
      "\n",
      "   Number of Final yXtrain_Fold=5 labeled 0: 69795\n",
      "   Number of Final yXtrain_Fold=5 labeled 1: 15260\n",
      "   Number of Final yXtrain_Fold=5 labeled 2: 0 \n",
      "\n",
      "Epoch 1/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.4534 - accuracy: 0.8112 - val_loss: 0.2293 - val_accuracy: 0.9400\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3713 - accuracy: 0.8391 - val_loss: 0.1917 - val_accuracy: 0.9411\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.3219 - accuracy: 0.8538 - val_loss: 0.1877 - val_accuracy: 0.9361\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2694 - accuracy: 0.8770 - val_loss: 0.1695 - val_accuracy: 0.9366\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2305 - accuracy: 0.8950 - val_loss: 0.1238 - val_accuracy: 0.9511\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9138 - val_loss: 0.1029 - val_accuracy: 0.9600\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.9261 - val_loss: 0.1336 - val_accuracy: 0.9416\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1572 - accuracy: 0.9334 - val_loss: 0.0804 - val_accuracy: 0.9611\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1469 - accuracy: 0.9398 - val_loss: 0.0830 - val_accuracy: 0.9683\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1320 - accuracy: 0.9464 - val_loss: 0.1079 - val_accuracy: 0.9516\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.9473 - val_loss: 0.0826 - val_accuracy: 0.9605\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1232 - accuracy: 0.9492 - val_loss: 0.0813 - val_accuracy: 0.9633\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1170 - accuracy: 0.9535 - val_loss: 0.0874 - val_accuracy: 0.9611\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.9527 - val_loss: 0.0680 - val_accuracy: 0.9717\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1120 - accuracy: 0.9553 - val_loss: 0.0781 - val_accuracy: 0.9694\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1102 - accuracy: 0.9562 - val_loss: 0.0766 - val_accuracy: 0.9678\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1062 - accuracy: 0.9584 - val_loss: 0.0976 - val_accuracy: 0.9583\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.9574 - val_loss: 0.0799 - val_accuracy: 0.9694\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.9593 - val_loss: 0.0725 - val_accuracy: 0.9650\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0997 - accuracy: 0.9616 - val_loss: 0.0761 - val_accuracy: 0.9689\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.9607 - val_loss: 0.0885 - val_accuracy: 0.9644\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9628 - val_loss: 0.0715 - val_accuracy: 0.9661\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.9616 - val_loss: 0.0679 - val_accuracy: 0.9705\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.9622 - val_loss: 0.0687 - val_accuracy: 0.9700\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.9652 - val_loss: 0.0730 - val_accuracy: 0.9678\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0945 - accuracy: 0.9638 - val_loss: 0.0706 - val_accuracy: 0.9683\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9625 - val_loss: 0.0686 - val_accuracy: 0.9711\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9654 - val_loss: 0.0737 - val_accuracy: 0.9694\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.9642 - val_loss: 0.0718 - val_accuracy: 0.9689\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.9665 - val_loss: 0.0643 - val_accuracy: 0.9694\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0926 - accuracy: 0.9642 - val_loss: 0.0745 - val_accuracy: 0.9672\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0904 - accuracy: 0.9657 - val_loss: 0.0657 - val_accuracy: 0.9728\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0897 - accuracy: 0.9653 - val_loss: 0.0722 - val_accuracy: 0.9666\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0890 - accuracy: 0.9658 - val_loss: 0.0763 - val_accuracy: 0.9694\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0896 - accuracy: 0.9658 - val_loss: 0.0808 - val_accuracy: 0.9661\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.9669 - val_loss: 0.0739 - val_accuracy: 0.9733\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.9667 - val_loss: 0.0785 - val_accuracy: 0.9689\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0864 - accuracy: 0.9669 - val_loss: 0.0675 - val_accuracy: 0.9700\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0869 - accuracy: 0.9667 - val_loss: 0.0663 - val_accuracy: 0.9733\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9684 - val_loss: 0.0847 - val_accuracy: 0.9661\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.9672 - val_loss: 0.0776 - val_accuracy: 0.9694\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0866 - accuracy: 0.9666 - val_loss: 0.0676 - val_accuracy: 0.9705\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0835 - accuracy: 0.9686 - val_loss: 0.0772 - val_accuracy: 0.9694\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.9675 - val_loss: 0.0673 - val_accuracy: 0.9711\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.9677 - val_loss: 0.0631 - val_accuracy: 0.9717\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9688 - val_loss: 0.0682 - val_accuracy: 0.9717\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0831 - accuracy: 0.9682 - val_loss: 0.0648 - val_accuracy: 0.9711\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9676 - val_loss: 0.0666 - val_accuracy: 0.9717\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.9676 - val_loss: 0.0635 - val_accuracy: 0.9767\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0839 - accuracy: 0.9681 - val_loss: 0.0683 - val_accuracy: 0.9744\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.9679 - val_loss: 0.0682 - val_accuracy: 0.9728\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.9687 - val_loss: 0.0687 - val_accuracy: 0.9722\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0793 - accuracy: 0.9706 - val_loss: 0.0695 - val_accuracy: 0.9722\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.9694 - val_loss: 0.0652 - val_accuracy: 0.9728\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0819 - accuracy: 0.9688 - val_loss: 0.0631 - val_accuracy: 0.9711\n",
      "Epoch 56/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.9697 - val_loss: 0.0734 - val_accuracy: 0.9678\n",
      "Epoch 57/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9688 - val_loss: 0.0686 - val_accuracy: 0.9739\n",
      "Epoch 58/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.9708 - val_loss: 0.0669 - val_accuracy: 0.9728\n",
      "Epoch 59/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9703 - val_loss: 0.0714 - val_accuracy: 0.9672\n",
      "Epoch 60/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9700 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 61/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.9706 - val_loss: 0.0798 - val_accuracy: 0.9722\n",
      "Epoch 62/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9706 - val_loss: 0.0663 - val_accuracy: 0.9744\n",
      "Epoch 63/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9691 - val_loss: 0.0660 - val_accuracy: 0.9755\n",
      "Epoch 64/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0805 - accuracy: 0.9694 - val_loss: 0.0578 - val_accuracy: 0.9789\n",
      "Epoch 65/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9698 - val_loss: 0.0549 - val_accuracy: 0.9755\n",
      "Epoch 66/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9697 - val_loss: 0.0727 - val_accuracy: 0.9700\n",
      "Epoch 67/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9706 - val_loss: 0.0668 - val_accuracy: 0.9750\n",
      "Epoch 68/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9723 - val_loss: 0.0674 - val_accuracy: 0.9717\n",
      "Epoch 69/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.9711 - val_loss: 0.0700 - val_accuracy: 0.9700\n",
      "Epoch 70/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9706 - val_loss: 0.0612 - val_accuracy: 0.9750\n",
      "Epoch 71/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.9712 - val_loss: 0.0624 - val_accuracy: 0.9744\n",
      "Epoch 72/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.9697 - val_loss: 0.0627 - val_accuracy: 0.9733\n",
      "Epoch 73/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.9699 - val_loss: 0.0630 - val_accuracy: 0.9767\n",
      "Epoch 74/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.9708 - val_loss: 0.0687 - val_accuracy: 0.9739\n",
      "Epoch 75/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9708 - val_loss: 0.0634 - val_accuracy: 0.9744\n",
      "Epoch 76/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9717 - val_loss: 0.0596 - val_accuracy: 0.9761\n",
      "Epoch 77/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9699 - val_loss: 0.0779 - val_accuracy: 0.9666\n",
      "Epoch 78/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.9709 - val_loss: 0.0662 - val_accuracy: 0.9728\n",
      "Epoch 79/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9717 - val_loss: 0.0824 - val_accuracy: 0.9672\n",
      "Epoch 80/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.9713 - val_loss: 0.0580 - val_accuracy: 0.9755\n",
      "Epoch 81/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9721 - val_loss: 0.0670 - val_accuracy: 0.9694\n",
      "Epoch 82/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0740 - accuracy: 0.9720 - val_loss: 0.0594 - val_accuracy: 0.9744\n",
      "Epoch 83/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9702 - val_loss: 0.0702 - val_accuracy: 0.9722\n",
      "Epoch 84/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9723 - val_loss: 0.0641 - val_accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9718 - val_loss: 0.0688 - val_accuracy: 0.9717\n",
      "Epoch 86/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9708 - val_loss: 0.0639 - val_accuracy: 0.9739\n",
      "Epoch 87/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.9727 - val_loss: 0.0615 - val_accuracy: 0.9739\n",
      "Epoch 88/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.9707 - val_loss: 0.0622 - val_accuracy: 0.9733\n",
      "Epoch 89/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 0.0604 - val_accuracy: 0.9733\n",
      "Epoch 90/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9717 - val_loss: 0.0709 - val_accuracy: 0.9700\n",
      "Epoch 91/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9721 - val_loss: 0.0647 - val_accuracy: 0.9744\n",
      "Epoch 92/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9732 - val_loss: 0.0585 - val_accuracy: 0.9744\n",
      "Epoch 93/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.9722 - val_loss: 0.0553 - val_accuracy: 0.9778\n",
      "Epoch 94/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9736 - val_loss: 0.0582 - val_accuracy: 0.9761\n",
      "Epoch 95/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9716 - val_loss: 0.0684 - val_accuracy: 0.9717\n",
      "Epoch 96/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0741 - accuracy: 0.9718 - val_loss: 0.0542 - val_accuracy: 0.9772\n",
      "Epoch 97/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9719 - val_loss: 0.0675 - val_accuracy: 0.9694\n",
      "Epoch 98/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.9732 - val_loss: 0.0609 - val_accuracy: 0.9744\n",
      "Epoch 99/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9726 - val_loss: 0.0543 - val_accuracy: 0.9778\n",
      "Epoch 100/100\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9718 - val_loss: 0.0568 - val_accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 784us/step\n",
      "xtrain: (85055, 59), ytrain: (85055,)\n",
      "xvalid: (1799, 59), yvalid: (1799,)\n",
      "xtest: (1800, 59), ytest: (1800,)\n",
      "\n",
      "classification_report_Fold=5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Normal 0       0.99      0.97      0.98      1745\n",
      " Anomalous 1       0.46      0.71      0.56        55\n",
      "\n",
      "    accuracy                           0.97      1800\n",
      "   macro avg       0.72      0.84      0.77      1800\n",
      "weighted avg       0.97      0.97      0.97      1800\n",
      "\n",
      "confusion_matrix_Fold=5:\n",
      "\n",
      "True Negatives:  1699\n",
      "False Positives:  46\n",
      "False Negatives:  16\n",
      "True Positives:  39\n",
      "accuracy_score_Fold=5:\n",
      " 1738 \n",
      "\n",
      "End running time Fold=5: 210217_031239 ,-------------------------- \n",
      "\n",
      "\n",
      "classification_report_AllFolds:\n",
      "            Normal 0  Anomalous 1\n",
      "precision      0.99         0.44\n",
      "recall         0.97         0.71\n",
      "f1-score       0.98         0.54\n",
      "End running time: 210217_031239\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "\n",
    "###Start sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit,cross_val_score,KFold,cross_validate,GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "### End sklearn\n",
    "\n",
    "###***Start tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.random.set_seed(1234)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "###**** End tensorflow.keras\n",
    "#sys.path.append(\"..\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "pathAug = \"/content/drive/MyDrive/MasterThesis_Files/mainCodes/augmentation/\"\n",
    "pathData =\"/content/drive/MyDrive/MasterThesis_Files/mainCodes/data/forCrossValidation/forAugShifted5/\"\n",
    "#pathData =\"/content/drive/MyDrive/MasterThesis_Files/mainCodes/data/forCrossValidation/Shifted5_Rol_Lbl1/\"\n",
    "\n",
    "\n",
    "sys.path.insert(0,pathAug)\n",
    "sys.path.insert(1,pathData)\n",
    "\n",
    "import preprocessRollingLabel2_NN as aug\n",
    "\n",
    "\n",
    "#####End Import Libraries\n",
    "\n",
    "\n",
    "############ Start Running codes\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"Main Start running time : {datestr}\")\n",
    "\n",
    "accPerFold = []\n",
    "lossPerFold = []\n",
    "dfPrReF1=pd.DataFrame()\n",
    "\n",
    "\n",
    "# dfActual = pd.read_csv(pathData+\"dfpShifted5_ForAug_201201_202734_AllTested_Correct_NT_NH.csv\",header=None)\n",
    "#dfActual = pd.read_csv(pathData+\"dfpShifted5ForAug_1To5_FromAllTrainTest_201204_192153.csv\",header=None)\n",
    "#dfActual = pd.read_csv(pathData+\"dfpShifted5_Rolling_210110_191921.csv\",header=None)\n",
    "\n",
    "dfActual = pd.read_csv(pathData+\"dfpShifted5_NoLess5_210216_182303.csv\",header=None)\n",
    "\n",
    "#####End Import Libraries\n",
    "\n",
    "\n",
    "############ Start Running codes\n",
    "\n",
    "def temporalize(X, y, lookback):\n",
    "    output_X = []\n",
    "    output_y = []\n",
    "\n",
    "    for i in range(-2, len(X) - 5 - 1):\n",
    "        t = []\n",
    "        for j in range(1, 5 + 1):\n",
    "            t.append(X[i + j + 1, :])\n",
    "\n",
    "        lookback = 4### because i start from zero and 4th(0,1,2,3,4) value is first label for first block\n",
    "        i+=2\n",
    "        output_X.append(t)\n",
    "        output_y.append(y[i + lookback ])\n",
    "    y2=np.repeat(np.array(output_y),5)\n",
    "    y2=y2.reshape(y2.shape[0],1)\n",
    "    X2=np.array(output_X).reshape(len(output_X)*5,59)\n",
    "    y2X2=np.concatenate((y2, X2), axis=1)\n",
    "    return y2X2\n",
    "\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"Main Start running time : {datestr}\")\n",
    "\n",
    "accPerFold = []\n",
    "lossPerFold = []\n",
    "dfPrReF1=pd.DataFrame()\n",
    "\n",
    "\n",
    "#dfActual=dfActual[:5000]\n",
    "\n",
    "yX=dfActual.values\n",
    "X = yX[:, 1:]  # converts the df to a numpy array\n",
    "y = yX[:, 0]\n",
    "\n",
    "# neg, pos = np.bincount(y.astype(int))\n",
    "# total = neg + pos\n",
    "# print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "#     total, pos, 100 * pos / total))\n",
    "# init_bias = np.log([pos / neg])\n",
    "\n",
    "\n",
    "print(f\"\\n   Number of Actual labeled 0: {len(y[np.where(y==0)])}\")\n",
    "print(f\"   Number of Actual labeled 1: {len(y[np.where(y==1)])}\")\n",
    "print(f\"   Number of Actual labeled 2: {len(y[np.where(y==2)])} \\n\")\n",
    "\n",
    "\n",
    "dataSplitPCT=.3\n",
    "dataSplitValTestPCT=.5\n",
    "\n",
    "train_test_split_Shuffle=True\n",
    "flagFitShuffle =True# True\n",
    "flagSeed=True\n",
    "\n",
    "p1=\"\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)#5\n",
    "#skf = KFold(n_splits=5,shuffle=False)#5\n",
    "model=0\n",
    "\n",
    "for foldNum, (trainIndex, testIndex) in enumerate(skf.split(X,y),start=1):\n",
    "\n",
    "    #print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n",
    "\n",
    "    yXtrain, yXtest = yX[trainIndex], yX[testIndex]\n",
    "    #ytrain, ytest = y[trainIndex], y[testIndex]\n",
    "\n",
    "    lookback = 5  # Equivalent to 10 min of past data.\n",
    "\n",
    "    # Temporalize the data\n",
    "    yXtrain = temporalize(X=yXtrain[:, 1:], y=yXtrain[:, 0], lookback=lookback)\n",
    "\n",
    "    AugedNN=aug.GenerateAug_NN_Rolling(yXtrain,foldNum,flagLbl2=False,jitterNum4Lbl1=7,jitterNum4Lbl2=10)###***Generate synthetic data\n",
    "\n",
    "\n",
    "    datestrfoldNum = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"\\n Start running time Fold={foldNum}: {datestrfoldNum} ,-------------------------- \\n\")\n",
    "\n",
    "    Actual_AugedNN=np.concatenate((yXtrain,AugedNN),axis=0)\n",
    "    #yXtrain=Actual_AugedNN\n",
    "\n",
    "    yXtrain1, yXtrain2 = train_test_split(Actual_AugedNN, shuffle=train_test_split_Shuffle,\n",
    "                                                          test_size=dataSplitPCT, random_state=42,\n",
    "                                                          stratify=Actual_AugedNN[:,0])  # stratify=input_y\n",
    "\n",
    "    yXtrain = np.concatenate((yXtrain1, yXtrain2), axis=0)\n",
    "\n",
    "    yXvalid, yXtest = train_test_split(yXtest, shuffle=train_test_split_Shuffle,\n",
    "                                          test_size=dataSplitValTestPCT, random_state=42,\n",
    "                                          stratify=yXtest[:, 0])  # stratify=input_y\n",
    "\n",
    "    print(f\"\\n   Number of Final yXtrain_Fold={foldNum} labeled 0: {len(yXtrain[np.where(yXtrain[:, 0] == 0)])}\")\n",
    "    print(f\"   Number of Final yXtrain_Fold={foldNum} labeled 1: {len(yXtrain[np.where(yXtrain[:, 0] == 1)])}\")\n",
    "    print(f\"   Number of Final yXtrain_Fold={foldNum} labeled 2: {len(yXtrain[np.where(yXtrain[:, 0] == 2)])} \\n\")\n",
    "\n",
    "    ytrain = yXtrain  [:,0]\n",
    "    yvalid = yXvalid  [:,0]\n",
    "    ytest  = yXtest   [:,0]\n",
    "\n",
    "\n",
    "    xtrain = yXtrain  [:,1:]\n",
    "    xvalid = yXvalid  [:,1:]\n",
    "    xtest  = yXtest   [:,1:]\n",
    "\n",
    "\n",
    "    neurons = xtrain.shape[1]\n",
    "\n",
    "    epochs = 100#100#0#60#400#60#30#30# 150  # 0  # 100#300#60#300#10#200#00#150\n",
    "    batch = 256\n",
    "    lr = 0.001\n",
    "\n",
    "    #flagR1 = True\n",
    "    flagR1=False\n",
    "    r1 = .1\n",
    "    r2 = .1\n",
    "    d1 = .2\n",
    "\n",
    "    if foldNum==1:\n",
    "        print(\"\\n Hyperparameters:\")\n",
    "        print(f\"epochs: {epochs}, batch: {batch}, lr: {lr}, neurons: {neurons}, flagFitShuffle: {flagFitShuffle} , train_test_split_Shuffle: {train_test_split_Shuffle}, flagSeed: {flagSeed}\\n \")\n",
    "\n",
    "        print(f\"\\n  xtrain: {np.shape(xtrain)}, ytrain: {np.shape(ytrain)}\")\n",
    "        print(f\"    xvalid: {np.shape(xvalid)}, yvalid: {np.shape(yvalid)}\")\n",
    "        print(f\"    xtest:  {np.shape(xtest)},  ytest:  {np.shape(ytest)} \\n\")\n",
    "\n",
    "        #p1 = os.path.join(str(pathCurrent.parent.parent), \"Results\", \"Results_001_class_oppys\", \"bestModels\", \"\")\n",
    "\n",
    "        # pathSavingPlotsPerRunning = pathSavingPlotsShifted5 + datestr #+ \"_\" + modelname\n",
    "        # if not os.path.exists(pathSavingPlotsPerRunning):\n",
    "        #     os.makedirs(pathSavingPlotsPerRunning)\n",
    "\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(177, activation='tanh', input_dim=xtrain.shape[1]#,initial_bias=init_bias,#180\n",
    "                    #,kernel_regularizer = l1(r1) if flagR1 else l2(r2),\n",
    "                    ))  # , input_dim=xtrain.shape[1]\n",
    "    #model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(150, activation='tanh',#,initial_bias=init_bias,#150\n",
    "                    #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "                    # ,# kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                    #bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "                    #activity_regularizer=l1(r2)\n",
    "                    ))\n",
    "    # model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(90, activation='tanh'#,initial_bias=init_bias,#90\n",
    "                    #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "                    # bias_regularizer=l1(r2),\n",
    "                    #activity_regularizer = l1(r1) if flagR1 else l2(r2)\n",
    "                    # activity_regularizer=l2(r2)\n",
    "                    ))\n",
    "    # model.add(Dropout(d1))\n",
    "\n",
    "    # model.add(Dense(295, activation='tanh'#,initial_bias=init_bias,\n",
    "    #                 #kernel_regularizer=l1(r1) if flagR1 else l2(r2),\n",
    "    # #                 # bias_regularizer=l1(r2),\n",
    "    # #                 activity_regularizer=l1(r1) if flagR1 else l2(r2)\n",
    "    # #                 # activity_regularizer=l2(r2)\n",
    "    #                 ))\n",
    "    # model.add(Dropout(d1))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr)#lr\n",
    "    # cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",save_best_only=True,verbose=0)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "    if foldNum == 1:\n",
    "        print(\"\\n Final test model.summary(): \\n\")\n",
    "        print(model.summary())\n",
    "\n",
    "        print(f\"\\n model.get_config: {str(model.get_config())}\")\n",
    "\n",
    "    # fit model\n",
    "    history1 = model.fit(xtrain, ytrain, batch_size=batch, epochs=epochs\n",
    "                         , validation_data=(xvalid, yvalid)\n",
    "                         , verbose=1, #use_multiprocessing=True,\n",
    "                         shuffle=flagFitShuffle).history  ### ,shuffle=True#,callbacks=[es]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history1['loss'], linewidth=2, label='Train')  # OR accuracy\n",
    "    plt.plot(history1['val_loss'], linewidth=2, label='Validation')  # OR val_accuracy\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.13, 1.13))\n",
    "    plt.title(f'Model loss Fold={foldNum}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,.2)\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"loss&val_loss_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    yPred = model.predict(xtest, verbose=1)\n",
    "\n",
    "    l = []\n",
    "    for i in yPred:\n",
    "        if i < .5:\n",
    "            l.append(0)\n",
    "        else:\n",
    "            l.append(1)\n",
    "\n",
    "    yPred = l\n",
    "\n",
    "    print(f\"xtrain: {np.shape(xtrain)}, ytrain: {np.shape(ytrain)}\")\n",
    "    print(f\"xvalid: {np.shape(xvalid)}, yvalid: {np.shape(yvalid)}\")\n",
    "    print(f\"xtest: {np.shape(xtest)}, ytest: {np.shape(ytest)}\")\n",
    "\n",
    "\n",
    "    target_names = ['Normal 0', 'Anomalous 1']\n",
    "    print(f\"\\nclassification_report_Fold={foldNum}:\")\n",
    "    print(classification_report(ytest, yPred, target_names=target_names))\n",
    "\n",
    "    cr = pd.DataFrame(classification_report(ytest, yPred, target_names=target_names,output_dict=True))\n",
    "    dfPrReF1 = dfPrReF1.append(cr.iloc[:3, :2])\n",
    "\n",
    "    print(f\"confusion_matrix_Fold={foldNum}:\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(ytest, yPred, labels=[0, 1]).ravel()\n",
    "    print(\"True Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "\n",
    "\n",
    "    print(f\"accuracy_score_Fold={foldNum}:\\n {accuracy_score(ytest, yPred, normalize=False)} \\n\")\n",
    "\n",
    "    # Predicting test images\n",
    "    # preds = np.where(yPred < 0.5, 0, 1)\n",
    "\n",
    "    # mlbClasses = [0, 1, 2]\n",
    "    # # Plot confusion matrix\n",
    "    # plt.figure(figsize=(14, 8))\n",
    "    # for j, (label, matrix) in enumerate(zip(mlbClasses, mlbConfusion)):\n",
    "    #     plt.subplot(f'23{j + 1}')\n",
    "    #     labels = [f'Not_{label}', label]\n",
    "    #     sns.heatmap(matrix, annot=True, square=True, fmt='d', cbar=False, cmap='Blues',\n",
    "    #                 cbar_kws={'label': 'My Colorbar'},  # , fmt = 'd'\n",
    "    #                 xticklabels=labels, yticklabels=labels, linecolor='black', linewidth=1)\n",
    "    #\n",
    "    #     plt.ylabel('Actual class')\n",
    "    #     plt.xlabel(f'Predicted class_Fold={foldNum}')\n",
    "    #     plt.title(labels[0])\n",
    "    #\n",
    "    # plt.tight_layout()\n",
    "    #plt.savefig(pathSavingPlotsPerRunning +\"/\" + f\"ConfusionMatrix_Fold={foldNum}_Epochs{epochs}_flagSeed{flagSeed}.png\", dpi=300, format='png')\n",
    "    #plt.show()\n",
    "\n",
    "    datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"End running time Fold={foldNum}: {datestr} ,-------------------------- \\n\")\n",
    "\n",
    "\n",
    "dfPrReF1=pd.DataFrame([np.round(dfPrReF1[dfPrReF1.index=='precision'].mean(),2),np.round(dfPrReF1[dfPrReF1.index=='recall'].mean(),2),np.round(dfPrReF1[dfPrReF1.index=='f1-score'].mean(),2)],index=['precision','recall','f1-score'])\n",
    "\n",
    "print(f\"\\nclassification_report_AllFolds:\\n {dfPrReF1}\")\n",
    "\n",
    "\n",
    "datestr = time.strftime(\"%y%m%d_%H%M%S\")\n",
    "print(f\"End running time: {datestr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj6ODidnORD9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8nx2p1fORHu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcztbiFKORLa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vyr-bwMmMPU0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkoWUjPkw_w3"
   },
   "source": [
    "## **Test that GPU is availabel or not:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaFQbEX0xGA3"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N23z3hoKxNoy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llcLaaLUxRtq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hovsUijExUsh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iex2kV8XxY8i"
   },
   "source": [
    "# **Test Camacity of assigned RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvnEElW2xeYC"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELbwa_h-xeuh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TestAuged_NN_Rolling_CV.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
